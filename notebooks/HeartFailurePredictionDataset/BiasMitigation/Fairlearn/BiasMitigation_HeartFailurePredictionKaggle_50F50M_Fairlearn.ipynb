{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288d499e",
   "metadata": {},
   "source": [
    "## Bias Mitigation using Fairlearn - Heart Failure Prediction Dataset (Source: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e647717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>146.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>130.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>128.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>140.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
       "0   61    1              3      146.0        241.0          0           0   \n",
       "1   39    1              1      130.0        215.0          0           0   \n",
       "2   60    0              0      150.0        240.0          0           0   \n",
       "3   49    1              3      128.0        212.0          0           0   \n",
       "4   50    0              2      140.0        288.0          0           0   \n",
       "\n",
       "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n",
       "0  148.0               1      3.0         0             1  \n",
       "1  120.0               0      0.0         2             0  \n",
       "2  171.0               0      0.9         2             0  \n",
       "3   96.0               1      0.0         1             1  \n",
       "4  140.0               1      0.0         1             1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load preprocessed data \n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"./data_subsets/train_50_50.csv\")\n",
    "\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")\n",
    "\n",
    "#check out the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5330b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y_test is a Series (not a DataFrame with 1 column)\n",
    "y_test = y_test.squeeze(\"columns\")\n",
    "\n",
    "# Define target and sensitive column names\n",
    "TARGET = \"HeartDisease\"\n",
    "SENSITIVE = \"Sex\"\n",
    "\n",
    "# Split train into X/y\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "# Extract sensitive features separately\n",
    "A_train = X_train[SENSITIVE].astype(int)\n",
    "A_test  = X_test[SENSITIVE].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749ae738",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"HeartDisease\"\n",
    "SENSITIVE = \"Sex\"   # 1 = Male, 0 = Female\n",
    "\n",
    "categorical_cols = ['Sex','ChestPainType','FastingBS','RestingECG','ExerciseAngina','ST_Slope']\n",
    "continuous_cols  = ['Age','RestingBP','Cholesterol','MaxHR','Oldpeak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fe70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into X / y and keep sensitive feature for fairness evaluation\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d93327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric features only, fit on train, transform test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_train.index\n",
    ")\n",
    "X_test_num_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9f63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode categoricals; numeric are kept as is \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\", sparse_output=False)\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "X_train_cat = pd.DataFrame(\n",
    "    ohe.transform(X_train[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_cat = pd.DataFrame(\n",
    "    ohe.transform(X_test[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e79e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shapes: (600, 18) (184, 18)\n"
     ]
    }
   ],
   "source": [
    "# Assemble final matrices\n",
    "X_train_ready = pd.concat([X_train_cat, X_train_num_scaled], axis=1)\n",
    "X_test_ready  = pd.concat([X_test_cat,  X_test_num_scaled],  axis=1)\n",
    "\n",
    "print(\"Final feature shapes:\", X_train_ready.shape, X_test_ready.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade7ec1",
   "metadata": {},
   "source": [
    "### Traditional ML Models - Baseline: K-Nearest Neighbors (KNN) & Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf455d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "#define a function \n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"=== {model_name} Evaluation ===\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred, average='binary'))\n",
    "    print(\"Recall   :\", recall_score(y_true, y_pred, average='binary'))\n",
    "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='binary'))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f7163",
   "metadata": {},
   "source": [
    "### PCA-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7c94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned PCA+KNN, no mitigation) ===\n",
      "=== KNN (best params) Evaluation ===\n",
      "Accuracy : 0.875\n",
      "Precision: 0.9247311827956989\n",
      "Recall   : 0.8431372549019608\n",
      "F1 Score : 0.882051282051282\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.87        82\n",
      "           1       0.92      0.84      0.88       102\n",
      "\n",
      "    accuracy                           0.88       184\n",
      "   macro avg       0.87      0.88      0.87       184\n",
      "weighted avg       0.88      0.88      0.88       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[75  7]\n",
      " [16 86]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "#1) PCA + KNN pipeline (on one-hot encoded + scaled features)\n",
    "pca_knn = Pipeline([\n",
    "    ('pca', PCA(n_components=0.95, random_state=42)),  # keep 95% variance\n",
    "    ('knn', KNeighborsClassifier(\n",
    "        n_neighbors=15, metric='manhattan', weights='distance'\n",
    "    ))\n",
    "])\n",
    "\n",
    "pca_knn.fit(X_train_ready, y_train)\n",
    "\n",
    "# Inspect PCA details\n",
    "n_comp = pca_knn.named_steps['pca'].n_components_\n",
    "expl_var = pca_knn.named_steps['pca'].explained_variance_ratio_.sum()\n",
    "\n",
    "print(\"=== Baseline (tuned PCA+KNN, no mitigation) ===\")\n",
    "# 2) Evaluate \n",
    "y_pred_pca_knn = pca_knn.predict(X_test_ready)\n",
    "probs_pca_knn = pca_knn.predict_proba(X_test_ready)[:, 1]\n",
    "  \n",
    "evaluate_model(y_test, y_pred_pca_knn, \"KNN (best params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34404c3",
   "metadata": {},
   "source": [
    "### Post-Processing -  KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f8d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned PCA+KNN) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.854167  0.1000  0.854167       0.595890  0.869863\n",
      "Accuracy: 0.8750 | DP diff: 0.4380 | EO diff: 0.1875\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.854167  0.1000  0.854167       0.595890  0.869863\n",
      "Accuracy: 0.8750 | DP diff: 0.4380 | EO diff: 0.1875\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.854167  0.1000  0.854167       0.595890  0.869863\n",
      "Accuracy: 0.8750 | DP diff: 0.4380 | EO diff: 0.1875\n"
     ]
    }
   ],
   "source": [
    "# Demographic Parity post-processing for your tuned PCA+KNN\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function\n",
    "def eval_fairness(y_true, y_pred, A):\n",
    "    mf = MetricFrame(\n",
    "        metrics={\n",
    "            \"TPR\": true_positive_rate,\n",
    "            \"FPR\": false_positive_rate,\n",
    "            \"Recall\": recall_score, \n",
    "            \"SelectionRate\": selection_rate,\n",
    "            \"Accuracy\": accuracy_score,\n",
    "        },\n",
    "        y_true=y_true, y_pred=y_pred, sensitive_features=A\n",
    "    )\n",
    "    return {\n",
    "        \"by_group\": mf.by_group,\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"dp\": demographic_parity_difference(y_true, y_pred, sensitive_features=A),\n",
    "        \"eo\": equalized_odds_difference(y_true, y_pred, sensitive_features=A),\n",
    "    }\n",
    "\n",
    "# 1) Baseline metrics (no mitigation) \n",
    "pca_knn.fit(X_train_ready, y_train)\n",
    "y_base = pca_knn.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (tuned PCA+KNN) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing with DEMOGRAPHIC PARITY\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    prefit=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Post-processing with EQUALIZED ODDS\n",
    "post_eod = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    prefit=True,                                # makes randomized post-processing reproducible\n",
    ")\n",
    "post_eod.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_eod = post_eod.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eod = eval_fairness(y_test, y_eod, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eod[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod['acc']:.4f} | DP diff: {m_eod['dp']:.4f} | EO diff: {m_eod['eo']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d0918",
   "metadata": {},
   "source": [
    "### Bias Mitigation Results: PCA+KNN – Post-Processing  \n",
    "\n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Model                    | Accuracy | DP diff | EO diff | Notes                                                             |\n",
    "|--------------------------|:--------:|:-------:|:-------:|-------------------------------------------------------------------|\n",
    "| PCA+KNN Baseline (tuned) | 0.8750   | 0.4380  | 0.1875  | Large DP gap; sizable EO gap                                      |\n",
    "| PCA+KNN + PP (DP)        | 0.8750   | 0.4380  | 0.1875  | **Identical to baseline** → post-processing not applied/effective |\n",
    "| PCA+KNN + PP (EO)        | 0.8750   | 0.4380  | 0.1875  | **Identical to baseline** → post-processing not applied/effective |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- Baseline shows **strong outcome disparity** (SelRate: **0.158** F vs **0.596** M → **DP 0.438**) and **non-trivial error-rate gap** (**EO 0.1875**).\n",
    "- Both post-processing runs yield **no change**, indicating the mitigator likely didn’t use scores or its adjusted outputs weren’t used for evaluation.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb782e92",
   "metadata": {},
   "source": [
    "**CorrelationRemover** will be implemented to improve fairness after DP/EOD post-processing failed to change any predictions (0% flips), leaving metrics unchanged. By removing linear correlation between features and the sensitive attribute, we reduce leakage and make group score distributions more comparable, giving PCA+KNN and also any subsequent post-processing room to adjust selection rates and error rates—all while staying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f37790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessing: CorrelationRemover + PCA+KNN ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.822917  0.1000  0.822917       0.575342  0.849315\n",
      "Accuracy: 0.8587 | DP diff: 0.4174 | EO diff: 0.1562\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "from sklearn.metrics import recall_score  \n",
    "\n",
    "Xtr_df = X_train_ready.copy()\n",
    "Xte_df = X_test_ready.copy()\n",
    "Xtr_df[\"__A__\"] = A_train.values\n",
    "Xte_df[\"__A__\"] = A_test.values\n",
    "\n",
    "cr = CorrelationRemover(sensitive_feature_ids=[\"__A__\"])\n",
    "\n",
    "Xtr_fair_arr = cr.fit_transform(Xtr_df)   # shape: (n_samples, n_features - 1)\n",
    "Xte_fair_arr = cr.transform(Xte_df)\n",
    "\n",
    "# Rebuild DataFrames with columns that exclude the sensitive column\n",
    "cols_out = [c for c in Xtr_df.columns if c != \"__A__\"]\n",
    "Xtr_fair = pd.DataFrame(Xtr_fair_arr, index=Xtr_df.index, columns=cols_out)\n",
    "Xte_fair = pd.DataFrame(Xte_fair_arr, index=Xte_df.index, columns=cols_out)\n",
    "\n",
    "# Refit your PCA+KNN\n",
    "pca_knn.fit(Xtr_fair, y_train)\n",
    "y_cr = pca_knn.predict(Xte_fair)\n",
    "m_cr = eval_fairness(y_test, y_cr, A_test)\n",
    "\n",
    "print(\"\\n=== Preprocessing: CorrelationRemover + PCA+KNN ===\")\n",
    "print(m_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_cr['acc']:.4f} | DP diff: {m_cr['dp']:.4f} | EO diff: {m_cr['eo']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc7847",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "- **Accuracy:** **0.8587** (↓ **1.63** pts vs baseline **0.8750**).\n",
    "- **DP diff:** **0.4174** (↓ **0.0206**) — selection **Female 0.158** vs **Male 0.575** (~**3.64×** higher for males).\n",
    "- **EO diff:** **0.1562** (↓ **0.0313**):\n",
    "  - **TPR gap:** 0.667 vs 0.823 → **0.156** (improved from **0.188**).\n",
    "  - **FPR gap:** 0.0625 vs 0.1000 → **0.0375** (**unchanged**).\n",
    "- **Group accuracy:** **Female 0.895** (≈ same) vs **Male 0.849** (↓ ~2.0 pts) → **accuracy gap widens** (~0.025 → ~0.045).\n",
    "\n",
    "**Summary:** CorrelationRemover **modestly improves DP and EO** by **lowering male sensitivity/selection**, but at the cost of **overall and male-group accuracy**. If the goal is parity without degrading the better-performing group, consider **in-processing constraints** or **group-specific thresholds** that **raise female recall** instead of reducing male performance.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79e2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Post-CR (DP) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.822917  0.1000  0.822917       0.575342  0.849315\n",
      "Accuracy: 0.8587 | DP diff: 0.4174 | EO diff: 0.1562\n",
      "\n",
      "=== Post-CR (eOD) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.822917  0.1000  0.822917       0.575342  0.849315\n",
      "Accuracy: 0.8587 | DP diff: 0.4174 | EO diff: 0.1562\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Demographic Parity on top of the CorrelationRemover\n",
    "post_dp_cr = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    objective=\"accuracy_score\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    prefit=True\n",
    ")\n",
    "post_dp_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)  # ideally fit on a validation split\n",
    "y_dp_cr = post_dp_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_dp_cr = eval_fairness(y_test, y_dp_cr, A_test)\n",
    "\n",
    "# Equalized Odds on top of CorrelationRemover\n",
    "post_eod_cr = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    objective=\"accuracy_score\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    prefit=True\n",
    ")\n",
    "post_eod_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)  # ideally fit on a validation split\n",
    "y_eod_cr = post_eod_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_eod_cr = eval_fairness(y_test, y_eod_cr, A_test)\n",
    "\n",
    "\n",
    "print(\"\\n=== Post-CR (DP) ===\")\n",
    "print(m_dp_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp_cr['acc']:.4f} | DP diff: {m_dp_cr['dp']:.4f} | EO diff: {m_dp_cr['eo']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Post-CR (eOD) ===\")\n",
    "print(m_eod_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod_cr['acc']:.4f} | DP diff: {m_eod_cr['dp']:.4f} | EO diff: {m_eod_cr['eo']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0af743",
   "metadata": {},
   "source": [
    "**Interpretation:**  \n",
    "- **No effect:** Post-CR **DP** and **eOD** post-processing changed **0%** — predictions identical to the CR baseline.  \n",
    "- **Disparities persist:** **DP diff 0.4174** (Female sel. **0.158** vs Male **0.575**), **EO diff 0.1562** (TPR **0.667** vs **0.823**; FPR **0.0625** vs **0.1000**).  \n",
    "- **Accuracy:** **0.8587**, unchanged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074237d",
   "metadata": {},
   "source": [
    "### Bias mitigation comparison (PCA+KNN)  \n",
    "\n",
    "\n",
    "| Model variant                    | Accuracy | DP diff | EO diff | SelRate S=0 | SelRate S=1 | TPR S=0 | TPR S=1 | FPR S=0 | FPR S=1 | Notes                         |\n",
    "|----------------------------------|:--------:|:-------:|:-------:|:-----------:|:-----------:|:-------:|:-------:|:-------:|:-------:|--------------------------------|\n",
    "| Baseline (tuned PCA+KNN)         | 0.8750   | 0.4380  | 0.1875  | 0.1579      | 0.5959      | 0.6667  | 0.8542  | 0.0625  | 0.1000  | Reference                      |\n",
    "| Post-processing (DP constraint)  | 0.8750   | 0.4380  | 0.1875  | 0.1579      | 0.5959      | 0.6667  | 0.8542  | 0.0625  | 0.1000  | **Flips vs baseline: 0%**      |\n",
    "| Post-processing (EO constraint)  | 0.8750   | 0.4380  | 0.1875  | 0.1579      | 0.5959      | 0.6667  | 0.8542  | 0.0625  | 0.1000  | **Flips vs baseline: 0%**      |\n",
    "| CorrelationRemover + PCA+KNN     | 0.8587   | 0.4174  | 0.1562  | 0.1579      | 0.5753      | 0.6667  | 0.8229  | 0.0625  | 0.1000  | New baseline after CR          |\n",
    "| Post-CR (DP constraint)          | 0.8587   | 0.4174  | 0.1562  | 0.1579      | 0.5753      | 0.6667  | 0.8229  | 0.0625  | 0.1000  | **Flips vs CR baseline: 0%**   |\n",
    "| Post-CR (EO constraint)          | 0.8587   | 0.4174  | 0.1562  | 0.1579      | 0.5753      | 0.6667  | 0.8229  | 0.0625  | 0.1000  | **Flips vs CR baseline: 0%**   |\n",
    "\n",
    "**Takeaway:** Post-processing caused **no label changes** in either setting. **CorrelationRemover** modestly **reduced DP** (0.4380→0.4174) and **EO** (0.1875→0.1562) with a small **accuracy drop** (0.8750→0.8587). Male selection remains much higher than female (≈0.596→0.575 vs 0.158).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941cf3a",
   "metadata": {},
   "source": [
    "### Alternative Tuned & Pruned Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66f6420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage A — Best simple DT params: {'criterion': 'entropy', 'max_depth': 6, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Stage A — Best CV F1: 0.8800000000000001\n",
      "Stage B — Best ccp_alpha: 0.000000 | CV F1: 0.8800\n",
      "=== Alternative Tuned & Pruned Decision Tree Evaluation ===\n",
      "Accuracy : 0.8206521739130435\n",
      "Precision: 0.896551724137931\n",
      "Recall   : 0.7647058823529411\n",
      "F1 Score : 0.8253968253968254\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.89      0.82        82\n",
      "           1       0.90      0.76      0.83       102\n",
      "\n",
      "    accuracy                           0.82       184\n",
      "   macro avg       0.82      0.83      0.82       184\n",
      "weighted avg       0.83      0.82      0.82       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[73  9]\n",
      " [24 78]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative DT tuning: simpler trees + class balancing + cost-complexity pruning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# bias toward simpler trees with class_weight=\"balanced\" \n",
    "base_dt = DecisionTreeClassifier(random_state=42, class_weight=\"balanced\")\n",
    "\n",
    "param_grid_simple = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [3, 4, 5, 6, 7],\n",
    "    \"min_samples_split\": [5, 10, 20],\n",
    "    \"min_samples_leaf\": [2, 4, 6],\n",
    "    \"min_impurity_decrease\": [0.0, 1e-4, 1e-3],  # tiny regularization\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_simple = GridSearchCV(\n",
    "    estimator=base_dt,\n",
    "    param_grid=param_grid_simple,\n",
    "    cv=cv,\n",
    "    scoring=\"recall\",    # balanced focus\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True\n",
    ")\n",
    "grid_simple.fit(X_train_ready, y_train)\n",
    "\n",
    "print(\"Stage A — Best simple DT params:\", grid_simple.best_params_)\n",
    "print(\"Stage A — Best CV F1:\", grid_simple.best_score_)\n",
    "simple_dt = grid_simple.best_estimator_\n",
    "\n",
    "# cost-complexity pruning\n",
    "path = simple_dt.cost_complexity_pruning_path(X_train_ready, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "unique_alphas = np.unique(np.round(ccp_alphas, 6))\n",
    "candidate_alphas = np.linspace(unique_alphas.min(), unique_alphas.max(), num=min(20, len(unique_alphas)))\n",
    "candidate_alphas = np.unique(np.concatenate([candidate_alphas, [0.0]]))  \n",
    "\n",
    "cv_scores = []\n",
    "for alpha in candidate_alphas:\n",
    "    dt_alpha = DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\",\n",
    "        criterion=simple_dt.criterion,\n",
    "        max_depth=simple_dt.max_depth,\n",
    "        min_samples_split=simple_dt.min_samples_split,\n",
    "        min_samples_leaf=simple_dt.min_samples_leaf,\n",
    "        min_impurity_decrease=simple_dt.min_impurity_decrease,\n",
    "        ccp_alpha=alpha\n",
    "    )\n",
    "    f1_cv = cross_val_score(dt_alpha, X_train_ready, y_train, cv=cv, scoring=\"recall\", n_jobs=-1).mean()\n",
    "    cv_scores.append((alpha, f1_cv))\n",
    "\n",
    "best_alpha, best_cv_f1 = sorted(cv_scores, key=lambda x: x[1], reverse=True)[0]\n",
    "print(f\"Stage B — Best ccp_alpha: {best_alpha:.6f} | CV F1: {best_cv_f1:.4f}\")\n",
    "\n",
    "best_dt = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",\n",
    "    criterion=simple_dt.criterion,\n",
    "    max_depth=simple_dt.max_depth,\n",
    "    min_samples_split=simple_dt.min_samples_split,\n",
    "    min_samples_leaf=simple_dt.min_samples_leaf,\n",
    "    min_impurity_decrease=simple_dt.min_impurity_decrease,\n",
    "    ccp_alpha=best_alpha\n",
    ").fit(X_train_ready, y_train)\n",
    "\n",
    "# Evaluate on test set \n",
    "y_pred_dt = best_dt.predict(X_test_ready)\n",
    "y_prob_dt = best_dt.predict_proba(X_test_ready)[:, 1]  \n",
    "\n",
    "evaluate_model(y_test, y_pred_dt, \"Alternative Tuned & Pruned Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd1d09",
   "metadata": {},
   "source": [
    "### Bias Mitigation DT: Inprocessing - Exponentiated Gradient Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "680a1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Tuned DT) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.666667  0.125  0.666667       0.210526  0.842105\n",
      "1    0.770833  0.100  0.770833       0.541096  0.815068\n",
      "Accuracy: 0.8207 | DP diff: 0.3306 | EO diff: 0.1042\n",
      "\n",
      "=== In-processing: EG (Equalized Odds) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.500000  0.09375  0.500000       0.157895  0.842105\n",
      "1    0.822917  0.18000  0.822917       0.602740  0.821918\n",
      "Accuracy: 0.8261 | DP diff: 0.4448 | EO diff: 0.3229\n",
      "\n",
      "=== In-processing: EG (Demographic Parity) ===\n",
      "          TPR   FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                   \n",
      "0    0.833333  0.25  0.833333       0.342105  0.763158\n",
      "1    0.812500  0.26  0.812500       0.623288  0.787671\n",
      "Accuracy: 0.7826 | DP diff: 0.2812 | EO diff: 0.0208\n",
      "\n",
      "=== Decision Tree: Baseline vs In-processing (EG) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.3306</td>\n",
       "      <td>0.1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.8261</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.3229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.7826</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>0.0208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)    0.8207   0.3306   0.1042\n",
       "1         DT + EG (EO)    0.8261   0.4448   0.3229\n",
       "2         DT + EG (DP)    0.7826   0.2812   0.0208"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-processing mitigation for tuned Decision Tree\n",
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 0) Baseline: tuned DT without mitigation (for comparison)\n",
    "y_pred_dt_base = best_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_pred_dt_base, A_test)\n",
    "print(\"=== Baseline (Tuned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Exponentiated Gradient with Equalized Odds\n",
    "eg_eo = ExponentiatedGradient(\n",
    "    estimator=clone(best_dt),        \n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,                         \n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_eo = eg_eo.predict(X_test_ready)\n",
    "m_eo = eval_fairness(y_test, y_pred_eo, A_test)\n",
    "print(\"\\n=== In-processing: EG (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Exponentiated Gradient with Demographic Parity\n",
    "eg_dp = ExponentiatedGradient(\n",
    "    estimator=clone(best_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_dp = eg_dp.predict(X_test_ready)\n",
    "m_dp = eval_fairness(y_test, y_pred_dp, A_test)\n",
    "print(\"\\n=== In-processing: EG (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary table\n",
    "summary_dt = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (tuned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + EG (EO)\",        \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + EG (DP)\",        \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs In-processing (EG) ===\")\n",
    "summary_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0254766",
   "metadata": {},
   "source": [
    "### Bias Mitigation Results: Decision Tree – In-Processing  \n",
    "\n",
    "#### Metrics Overview  \n",
    "\n",
    "| Model                   | Accuracy | DP diff | EO diff | Notes                                                                 |\n",
    "|--------------------------|:--------:|:-------:|:-------:|-----------------------------------------------------------------------|\n",
    "| **DT Baseline (tuned)** | 0.8207   | 0.3306  | 0.1042  | Strong DP gap; moderate EO gap.                                       |\n",
    "| **DT + EG (EO)**        | 0.8261   | 0.4448  | 0.3229  | Higher accuracy, but **fairness worsened** (larger DP & EO gaps).     |\n",
    "| **DT + EG (DP)**        | 0.7826   | 0.2812  | 0.0208  | Accuracy dropped, but **fairness improved** (smaller DP & especially EO gaps). |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation  \n",
    "\n",
    "- **Baseline (tuned DT):**  \n",
    "  - Accuracy ≈ **82%**, but large **selection rate gap** (0.21 vs. 0.54, DP diff = 0.33).  \n",
    "  - Moderate EO gap (**0.10**) — indicating some imbalance in error rates across sexes.  \n",
    "\n",
    "- **Exponentiated Gradient (Equalized Odds):**  \n",
    "  - Slight **accuracy gain** (82.6%), but fairness degraded.  \n",
    "  - DP diff **increased** to 0.44, EO diff **tripled** to 0.32.  \n",
    "  - → Trade-off skewed toward accuracy at the expense of equity.  \n",
    "\n",
    "- **Exponentiated Gradient (Demographic Parity):**  \n",
    "  - Accuracy decreased (**78%**), but fairness improved.  \n",
    "  - DP diff reduced to **0.28** and EO diff nearly eliminated (**0.02**).  \n",
    "  - → Strong fairness gains, especially in equalizing true/false positive rates, though with some cost in predictive performance.  \n",
    "\n",
    "- **Conclusion:**  \n",
    "  - **EG (EO)** may not be suitable here, as it amplifies disparities.  \n",
    "  - **EG (DP)** shows a better fairness–accuracy trade-off, reducing both DP and EO gaps, albeit with a slight drop in accuracy.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87407025",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: In-processing: GridSearch Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c97e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing: GridSearch (Equalized Odds) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.666667  0.125  0.666667       0.210526  0.842105\n",
      "1    0.770833  0.100  0.770833       0.541096  0.815068\n",
      "Accuracy: 0.8207 | DP diff: 0.3306 | EO diff: 0.1042\n",
      "\n",
      "=== In-processing: GridSearch (Demographic Parity) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    1.000000  0.21875  1.000000       0.342105  0.815789\n",
      "1    0.802083  0.26000  0.802083       0.616438  0.780822\n",
      "Accuracy: 0.7880 | DP diff: 0.2743 | EO diff: 0.1979\n",
      "\n",
      "=== Decision Tree: Baseline vs EG vs GS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.3306</td>\n",
       "      <td>0.1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.8261</td>\n",
       "      <td>0.4448</td>\n",
       "      <td>0.3229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.7826</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>0.0208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT + GS (EO)</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.3306</td>\n",
       "      <td>0.1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT + GS (DP)</td>\n",
       "      <td>0.7880</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.1979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)    0.8207   0.3306   0.1042\n",
       "1         DT + EG (EO)    0.8261   0.4448   0.3229\n",
       "2         DT + EG (DP)    0.7826   0.2812   0.0208\n",
       "3         DT + GS (EO)    0.8207   0.3306   0.1042\n",
       "4         DT + GS (DP)    0.7880   0.2743   0.1979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds\n",
    "gs_eo = GridSearch(\n",
    "    estimator=clone(best_dt),              # unfitted clone of tuned DT\n",
    "    constraints=EqualizedOdds(),            # EO constraint\n",
    "    selection_rule=\"tradeoff_optimization\", \n",
    "    constraint_weight=0.5,                  \n",
    "    grid_size=15,                           \n",
    ")\n",
    "gs_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_eo = gs_eo.predict(X_test_ready)\n",
    "m_gs_eo = eval_fairness(y_test, y_pred_gs_eo, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo['acc']:.4f} | DP diff: {m_gs_eo['dp']:.4f} | EO diff: {m_gs_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity\n",
    "gs_dp = GridSearch(\n",
    "    estimator=clone(best_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15,\n",
    ")\n",
    "gs_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_dp = gs_dp.predict(X_test_ready)\n",
    "m_gs_dp = eval_fairness(y_test, y_pred_gs_dp, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp['acc']:.4f} | DP diff: {m_gs_dp['dp']:.4f} | EO diff: {m_gs_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with your existing runs\n",
    "summary_dt = pd.concat([\n",
    "    summary_dt,  \n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"DT + GS (EO)\", \"accuracy\":m_gs_eo[\"acc\"], \"dp_diff\":m_gs_eo[\"dp\"], \"eo_diff\":m_gs_eo[\"eo\"]},\n",
    "        {\"model\":\"DT + GS (DP)\", \"accuracy\":m_gs_dp[\"acc\"], \"dp_diff\":m_gs_dp[\"dp\"], \"eo_diff\":m_gs_dp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "print(\"\\n=== Decision Tree: Baseline vs EG vs GS ===\")\n",
    "summary_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86587b",
   "metadata": {},
   "source": [
    "### Decision Tree — In-Processing: EG vs. GridSearch (EO & DP)  \n",
    "\n",
    "#### Summary of results\n",
    "| Model                   | Accuracy | DP diff | EO diff | Notes |\n",
    "|--------------------------|:--------:|:-------:|:-------:|-------|\n",
    "| **DT Baseline (tuned)** | 0.8207   | 0.3306  | 0.1042  | Reference |\n",
    "| **DT + EG (EO)**        | 0.8261   | 0.4448  | 0.3229  | Higher accuracy, but fairness worsened |\n",
    "| **DT + EG (DP)**        | 0.7826   | 0.2812  | 0.0208  | Lower accuracy, but fairness improved |\n",
    "| **DT + GS (EO)**        | 0.8207   | 0.3306  | 0.1042  | **Identical to baseline** (no effect) |\n",
    "| **DT + GS (DP)**        | 0.7880   | 0.2743  | 0.1979  | Lower accuracy, DP gap smaller but EO gap larger |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline (tuned DT):**  \n",
    "  - Accuracy ≈ **82%**, with a substantial **DP gap (0.33)** and moderate **EO gap (0.10)**.  \n",
    "\n",
    "- **Exponentiated Gradient (EG):**  \n",
    "  - **EO constraint:** Increased accuracy slightly, but fairness deteriorated (both DP and EO gaps worsened).  \n",
    "  - **DP constraint:** Accuracy dropped, but fairness improved significantly — especially EO gap (down to 0.02).  \n",
    "\n",
    "- **GridSearch (GS):**  \n",
    "  - **EO constraint:** Produced results **identical to the baseline** (no impact).  \n",
    "  - **DP constraint:** Reduced DP gap somewhat, but EO gap worsened (≈ 0.20). Accuracy also dropped.  \n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "- **EG (DP)** gave the best fairness–accuracy trade-off, reducing disparities while accepting a modest performance loss.  \n",
    "- **EG (EO)** and **GS (DP)** highlight that not all fairness constraints move the model in the desired direction — they can worsen other gaps.  \n",
    "- **GS (EO)** had **no effect**, showing the constraint was not binding in this setup.  \n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15838f4",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: Post-processing: Threshold Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "873f1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned DT) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.666667  0.125  0.666667       0.210526  0.842105\n",
      "1    0.770833  0.100  0.770833       0.541096  0.815068\n",
      "Accuracy: 0.8207 | DP diff: 0.3306 | EO diff: 0.1042\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.1875  0.666667       0.263158  0.789474\n",
      "1    0.833333  0.1400  0.833333       0.595890  0.842466\n",
      "Accuracy: 0.8315 | DP diff: 0.3327 | EO diff: 0.1667\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.500000  0.09375  0.500000       0.157895  0.842105\n",
      "1    0.833333  0.14000  0.833333       0.595890  0.842466\n",
      "Accuracy: 0.8424 | DP diff: 0.4380 | EO diff: 0.3333\n",
      "\n",
      "=== Decision Tree: Baseline vs Post-processing ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.3306</td>\n",
       "      <td>0.1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + Post (EO)</td>\n",
       "      <td>0.8315</td>\n",
       "      <td>0.3327</td>\n",
       "      <td>0.1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + Post (DP)</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.4380</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)    0.8207   0.3306   0.1042\n",
       "1       DT + Post (EO)    0.8315   0.3327   0.1667\n",
       "2       DT + Post (DP)    0.8424   0.4380   0.3333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "#Baseline for mitigation: fixed tuned DT\n",
    "best_dt.fit(X_train_ready, y_train)\n",
    "y_base = best_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "print(\"=== Baseline (tuned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "#Post-processing: Equalized Odds\n",
    "post_eo = ThresholdOptimizer(\n",
    "    estimator=best_dt,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_eo = post_eo.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eo = eval_fairness(y_test, y_eo, A_test)\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# Post-processing: Demographic Parity\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=best_dt,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# create summary table \n",
    "summary = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (tuned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (EO)\",      \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (DP)\",      \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs Post-processing ===\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328aeb4",
   "metadata": {},
   "source": [
    "### Decision Tree — Post- vs In-Processing  \n",
    "\n",
    "#### Combined Results\n",
    "\n",
    "| Model / Method          | Accuracy | DP diff | EO diff | Notes (vs. baseline 0.8207 / 0.3306 / 0.1042) |\n",
    "|--------------------------|:--------:|:-------:|:-------:|-----------------------------------------------|\n",
    "| **Baseline (Tuned DT)** | 0.8207   | 0.3306  | 0.1042  | Reference                                      |\n",
    "| **Post (EO)**           | 0.8315   | 0.3327  | 0.1667  | Accuracy ↑ (+1.1 pts); DP ≈ baseline; **EO worsened** |\n",
    "| **Post (DP)**           | 0.8424   | 0.4380  | 0.3333  | Accuracy ↑ (+2.2 pts); **DP worsened**; **EO worsened significantly** |\n",
    "| **EG (EO)**             | 0.8261   | 0.4448  | 0.3229  | Accuracy ↑; **fairness worsened** (DP & EO higher) |\n",
    "| **EG (DP)**             | 0.7826   | 0.2812  | 0.0208  | Accuracy ↓; **fairness improved** (EO nearly eliminated, DP smaller) |\n",
    "| **GS (EO)**             | 0.8207   | 0.3306  | 0.1042  | **Identical to baseline** (no effect) |\n",
    "| **GS (DP)**             | 0.7880   | 0.2743  | 0.1979  | Accuracy ↓; DP improved slightly, EO worsened |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline (tuned DT):** Balanced accuracy (~82%) but a **large DP gap** (0.33) and moderate **EO gap** (0.10).  \n",
    "- **Post-processing (EO):** Accuracy improved slightly, but **EO disparity increased** (0.17). No DP relief.  \n",
    "- **Post-processing (DP):** Highest accuracy, but **fairness strongly worsened** — both DP and EO gaps grew.  \n",
    "- **Exponentiated Gradient (EG):**  \n",
    "  - **EO constraint:** Slight accuracy gain, but fairness deteriorated.  \n",
    "  - **DP constraint:** Lower accuracy, but **fairness gains**, especially EO (0.02).  \n",
    "- **GridSearch (GS):**  \n",
    "  - **EO constraint:** No effect (baseline results repeated).  \n",
    "  - **DP constraint:** Small DP improvement, but EO worsened and accuracy dropped.  \n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:**  \n",
    "- **Post-processing did not help fairness** in this setup — in fact, both EO and DP disparities worsened while accuracy increased.  \n",
    "- **EG with DP constraint** provided the **most meaningful fairness improvement** (especially EO gap reduction) at a modest accuracy cost.  \n",
    "- **GS (EO)** had no effect, while **GS (DP)** gave only marginal DP gains but worsened EO.  \n",
    "- Overall, **only EG (DP)** demonstrated a genuine fairness–accuracy trade-off; other methods either worsened disparities or had no effect.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbe2d4",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81ec0da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Evaluation ===\n",
      "Accuracy : 0.8315217391304348\n",
      "Precision: 0.8585858585858586\n",
      "Recall   : 0.8333333333333334\n",
      "F1 Score : 0.845771144278607\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.81        82\n",
      "           1       0.86      0.83      0.85       102\n",
      "\n",
      "    accuracy                           0.83       184\n",
      "   macro avg       0.83      0.83      0.83       184\n",
      "weighted avg       0.83      0.83      0.83       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[68 14]\n",
      " [17 85]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train_ready, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf.predict(X_test_ready)\n",
    "y_prob_rf = rf.predict_proba(X_test_ready)[:, 1]  \n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daad4ed",
   "metadata": {},
   "source": [
    "### Bias Mitgation RF: In-processing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1199f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.15625  0.833333       0.263158  0.842105\n",
      "1    0.833333  0.18000  0.833333       0.609589  0.828767\n",
      "Accuracy: 0.8315 | DP diff: 0.3464 | EO diff: 0.0237\n",
      "\n",
      "=== In-processing RF: EG (Equalized Odds) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.15625  0.833333       0.263158  0.842105\n",
      "1    0.833333  0.18000  0.833333       0.609589  0.828767\n",
      "Accuracy: 0.8315 | DP diff: 0.3464 | EO diff: 0.0237\n",
      "\n",
      "=== In-processing RF: EG (Demographic Parity) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.15625  0.833333       0.263158  0.842105\n",
      "1    0.833333  0.18000  0.833333       0.609589  0.828767\n",
      "Accuracy: 0.8315 | DP diff: 0.3464 | EO diff: 0.0237\n",
      "\n",
      "=== Random Forest: Baseline vs In-processing (EG) ===\n",
      "          model  accuracy  dp_diff  eo_diff\n",
      "0   RF Baseline    0.8315   0.3464   0.0237\n",
      "1  RF + EG (EO)    0.8315   0.3464   0.0237\n",
      "2  RF + EG (DP)    0.8315   0.3464   0.0237\n"
     ]
    }
   ],
   "source": [
    "# 0) Baseline Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_rf_base = rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_pred_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "#1) EG with Equalized Odds\n",
    "eg_eo_rf = ExponentiatedGradient(\n",
    "    estimator=clone(rf),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_eo = eg_eo_rf.predict(X_test_ready, random_state=42)\n",
    "m_rf_eo = eval_fairness(y_test, y_pred_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity \n",
    "eg_dp_rf = ExponentiatedGradient(\n",
    "    estimator=clone(rf),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_dp = eg_dp_rf.predict(X_test_ready, random_state=42)\n",
    "m_rf_dp = eval_fairness(y_test, y_pred_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table \n",
    "summary_rf = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",      \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (EO)\",     \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (DP)\",     \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b0260",
   "metadata": {},
   "source": [
    "### Random Forest Bias Mitigation Results  \n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model            | Accuracy | DP diff | EO diff | Interpretation                                  |\n",
    "|------------------|:--------:|:-------:|:-------:|-------------------------------------------------|\n",
    "| **RF Baseline**  | 0.8315   | 0.3464  | 0.0237  | Good accuracy; **moderate DP gap**; EO already low. |\n",
    "| **RF + EG (EO)** | 0.8315   | 0.3464  | 0.0237  | **No change** vs baseline → EO constraint had no effect. |\n",
    "| **RF + EG (DP)** | 0.8315   | 0.3464  | 0.0237  | **No change** vs baseline → DP constraint had no effect. |\n",
    "\n",
    "### Key Points\n",
    "- **Selection rates:** Female **0.263** vs Male **0.610** → **DP 0.346** (~**2.3×** higher for males).\n",
    "- **Error rates:** **TPR** equal (0.833 vs 0.833); **FPR** close (0.156 vs 0.180) → **EO 0.024** (very small).\n",
    "- **EG (EO/DP)** produced **0% movement** — constraints likely not binding or optimization stayed at the baseline frontier point.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c181c7",
   "metadata": {},
   "source": [
    "### Bias Mitigation: RF: In-processing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4e11367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method  weight       acc        dp      eo\n",
      "5  RF + GS (DP)    0.00  0.864130  0.451694  0.1875\n",
      "6  RF + GS (DP)    0.25  0.864130  0.451694  0.1875\n",
      "7  RF + GS (DP)    0.50  0.864130  0.451694  0.1875\n",
      "8  RF + GS (DP)    0.75  0.864130  0.451694  0.1875\n",
      "9  RF + GS (DP)    1.00  0.864130  0.451694  0.1875\n",
      "0  RF + GS (EO)    0.00  0.858696  0.425379  0.0975\n",
      "1  RF + GS (EO)    0.25  0.858696  0.425379  0.0975\n",
      "2  RF + GS (EO)    0.50  0.858696  0.425379  0.0975\n",
      "3  RF + GS (EO)    0.75  0.858696  0.425379  0.0975\n",
      "4  RF + GS (EO)    1.00  0.858696  0.425379  0.0975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0]   # 0.0 = accuracy-first, 1.0 = fairness-first\n",
    "grid = 50                               \n",
    "\n",
    "rows = []\n",
    "\n",
    "#Equalized Odds sweep\n",
    "for w in weights:\n",
    "    gs_eo_rf = GridSearch(\n",
    "        estimator=clone(rf),                 \n",
    "        constraints=EqualizedOdds(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    # Some versions accept random_state in predict; if yours doesn't, seed numpy before predicting\n",
    "    try:\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (EO)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "# Demographic Parity sweep\n",
    "for w in weights:\n",
    "    gs_dp_rf = GridSearch(\n",
    "        estimator=clone(rf),\n",
    "        constraints=DemographicParity(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    try:\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (DP)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "df_gs = pd.DataFrame(rows).sort_values([\"method\",\"weight\"])\n",
    "print(df_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfa38e",
   "metadata": {},
   "source": [
    "### Interpretation (RF + GridSearch)  \n",
    "\n",
    "- **No movement across weights:** For both **DP** and **EO** constraints, varying the weight **0 → 1** yields the **same metrics** each time.  \n",
    "- **Compared to baseline:**  \n",
    "\n",
    "| Method   | Acc    | ΔAcc  | DP     | ΔDP   | EO     | ΔEO   | Read |\n",
    "|----------|:------:|:-----:|:------:|:-----:|:------:|:-----:|------|\n",
    "| GS (DP)  | 0.8641 | −1.6  | 0.4517 | +0.044 | 0.1875 | +0.104 | Accuracy ↓; **DP & EO worsen** |\n",
    "| GS (EO)  | 0.8587 | −2.2  | 0.4254 | +0.017 | 0.0975 | +0.014 | Accuracy ↓; **EO slightly better than GS(DP)**, but still ↑ vs baseline |\n",
    "\n",
    "---\n",
    "\n",
    "### Takeaway\n",
    "GridSearch **locked onto single frontier points** regardless of weight.  \n",
    "- Both GS(DP) and GS(EO) **reduced accuracy** compared to the baseline RF.  \n",
    "- **Fairness worsened in all cases**: DP gaps ↑ and EO gaps ↑.  \n",
    "- **GS(EO)** is the less harmful option (smaller increases), but still offers **no improvement over the baseline**.  \n",
    "\n",
    "**Conclusion:** In this setup, RF + GridSearch does **not provide a fairness–utility gain**; it simply trades off accuracy for worse fairness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2117d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     i       acc        dp        eo\n",
      "0    0  0.836957  0.575342  0.812500\n",
      "1    1  0.820652  0.554795  0.781250\n",
      "2    2  0.847826  0.589041  0.833333\n",
      "3    3  0.815217  0.561644  0.781250\n",
      "4    4  0.695652  1.000000  1.000000\n",
      "5    5  0.831522  0.568493  0.802083\n",
      "6    6  0.847826  0.589041  0.833333\n",
      "7    7  0.836957  0.575342  0.812500\n",
      "8    8  0.684783  0.284066  0.800000\n",
      "9    9  0.695652  1.000000  1.000000\n",
      "10  10  0.695652  1.000000  1.000000\n",
      "11  11  0.826087  0.561644  0.791667\n",
      "12  12  0.820652  0.554795  0.781250\n",
      "13  13  0.842391  0.582192  0.822917\n",
      "14  14  0.706522  0.264600  0.808750\n",
      "15  15  0.701087  0.285148  0.828750\n",
      "16  16  0.684783  0.736842  0.812500\n",
      "17  17  0.690217  0.763158  0.843750\n",
      "18  18  0.690217  0.763158  0.843750\n",
      "19  19  0.858696  0.425379  0.097500\n",
      "20  20  0.875000  0.471161  0.140000\n",
      "21  21  0.826087  0.378515  0.086250\n",
      "22  22  0.711957  0.349315  0.800000\n",
      "23  23  0.711957  0.349315  0.800000\n",
      "24  24  0.706522  0.356164  0.800000\n",
      "25  25  0.690217  0.763158  0.843750\n",
      "26  26  0.695652  0.789474  0.875000\n",
      "27  27  0.706522  0.842105  0.937500\n",
      "28  28  0.706522  0.842105  0.937500\n",
      "29  29  0.853261  0.404831  0.077500\n",
      "30  30  0.831522  0.346431  0.023750\n",
      "31  31  0.869565  0.464311  0.140000\n",
      "32  32  0.701087  0.363014  0.800000\n",
      "33  33  0.701087  0.349315  0.780000\n",
      "34  34  0.706522  0.342466  0.780000\n",
      "35  35  0.701087  0.376712  0.820000\n",
      "36  36  0.701087  0.815789  0.906250\n",
      "37  37  0.701087  0.815789  0.906250\n",
      "38  38  0.706522  0.842105  0.937500\n",
      "39  39  0.842391  0.379596  0.055000\n",
      "40  40  0.858696  0.491709  0.200000\n",
      "41  41  0.858696  0.524874  0.200000\n",
      "42  42  0.684783  0.356164  0.760000\n",
      "43  43  0.701087  0.335616  0.760000\n",
      "44  44  0.701087  0.335616  0.760000\n",
      "45  45  0.293478  0.231435  0.706250\n",
      "46  46  0.298913  0.238284  0.757500\n",
      "47  47  0.456522  0.052632  0.333333\n",
      "48  48  0.456522  0.052632  0.333333\n",
      "49  49  0.456522  0.052632  0.333333\n",
      "     i       acc        dp        eo\n",
      "0    0  0.695652  1.000000  1.000000\n",
      "1    1  0.695652  1.000000  1.000000\n",
      "2    2  0.695652  1.000000  1.000000\n",
      "3    3  0.695652  1.000000  1.000000\n",
      "4    4  0.695652  1.000000  1.000000\n",
      "5    5  0.695652  1.000000  1.000000\n",
      "6    6  0.695652  1.000000  1.000000\n",
      "7    7  0.695652  1.000000  1.000000\n",
      "8    8  0.695652  1.000000  1.000000\n",
      "9    9  0.695652  1.000000  1.000000\n",
      "10  10  0.695652  1.000000  1.000000\n",
      "11  11  0.695652  1.000000  1.000000\n",
      "12  12  0.695652  1.000000  1.000000\n",
      "13  13  0.864130  0.451694  0.187500\n",
      "14  14  0.875000  0.399063  0.031250\n",
      "15  15  0.858696  0.425379  0.097500\n",
      "16  16  0.864130  0.471161  0.177083\n",
      "17  17  0.875000  0.504326  0.187500\n",
      "18  18  0.885870  0.531723  0.218750\n",
      "19  19  0.858696  0.478010  0.180000\n",
      "20  20  0.891304  0.478010  0.120000\n",
      "21  21  0.864130  0.412761  0.066250\n",
      "22  22  0.836957  0.392213  0.086250\n",
      "23  23  0.831522  0.399063  0.106250\n",
      "24  24  0.842391  0.379596  0.055000\n",
      "25  25  0.831522  0.346431  0.023750\n",
      "26  26  0.853261  0.399063  0.166667\n",
      "27  27  0.836957  0.411680  0.117500\n",
      "28  28  0.836957  0.425379  0.137500\n",
      "29  29  0.864130  0.432228  0.097500\n",
      "30  30  0.847826  0.444845  0.148750\n",
      "31  31  0.864130  0.465393  0.148750\n",
      "32  32  0.842391  0.443764  0.160000\n",
      "33  33  0.858696  0.497477  0.166667\n",
      "34  34  0.864130  0.518025  0.187500\n",
      "35  35  0.858696  0.524874  0.200000\n",
      "36  36  0.869565  0.497477  0.177083\n",
      "37  37  0.869565  0.497477  0.177083\n",
      "38  38  0.304348  1.000000  1.000000\n",
      "39  39  0.304348  1.000000  1.000000\n",
      "40  40  0.304348  1.000000  1.000000\n",
      "41  41  0.304348  1.000000  1.000000\n",
      "42  42  0.304348  1.000000  1.000000\n",
      "43  43  0.304348  1.000000  1.000000\n",
      "44  44  0.304348  1.000000  1.000000\n",
      "45  45  0.304348  1.000000  1.000000\n",
      "46  46  0.304348  1.000000  1.000000\n",
      "47  47  0.304348  1.000000  1.000000\n",
      "48  48  0.304348  1.000000  1.000000\n",
      "49  49  0.304348  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Inspect how many distinct models GridSearch actually produced\n",
    "len(gs_eo_rf.predictors_), len(gs_dp_rf.predictors_)\n",
    "\n",
    "# See the spread across the frontier (test metrics for each predictor)\n",
    "def eval_frontier(gs, X, y, A):\n",
    "    rows=[]\n",
    "    for i, clf in enumerate(gs.predictors_):\n",
    "        yhat = clf.predict(X)\n",
    "        m = eval_fairness(y, yhat, A)\n",
    "        rows.append({\"i\": i, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(eval_frontier(gs_eo_rf, X_test_ready, y_test, A_test))\n",
    "print(eval_frontier(gs_dp_rf, X_test_ready, y_test, A_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee0bbb",
   "metadata": {},
   "source": [
    "### RF GridSearch frontiers  \n",
    "**Baseline:** Acc **0.8804**, DP **0.4081**, EO **0.0833**\n",
    "\n",
    "**What the tables show:** Each index `i` is a GridSearch candidate. Many points are **degenerate** (e.g., `i=0–17` in table 1 and `i=0–12` in table 2: DP/EO = 1.0 or very low accuracy).  \n",
    "Useful frontier candidates emerge around **`i=25/30`** (EO ≈ **0.0238**, DP ≈ **0.3464**, Acc ≈ **0.8315**) and other mid–high accuracy models.\n",
    "\n",
    "---\n",
    "\n",
    "#### Notable candidates\n",
    "| Candidate | Acc    | DP      | EO      | How it compares to baseline |\n",
    "|-----------|:------:|:-------:|:-------:|------------------------------|\n",
    "| **i=25/30** (tbl1/tbl2) | 0.8315 | 0.3464 | **0.0238** | **Baseline-like**: EO-minimal, slightly lower Acc |\n",
    "| **i=24/39** (tbl1/tbl2) | 0.8424 | 0.3796 | 0.0550 | Acc ↑ (+1.1 pp), EO ↑ (but still small), DP ↑ |\n",
    "| **i=29** (table 1)      | 0.8533 | 0.4048 | 0.0775 | Acc ↑ (+2.2 pp), DP & EO ↑ (accuracy-leaning) |\n",
    "| **i=14** (table 2)      | 0.8750 | 0.3991 | 0.0313 | **Highest accuracy** (+4.3 pp), EO small, DP ↑ |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **No Pareto-better solution:** No candidate simultaneously improves **DP**, **EO**, and **accuracy** vs the baseline.  \n",
    "- **Baseline-like (i=25/30):** Best for **minimizing EO** (≈0.024), but with lower accuracy than the true baseline.  \n",
    "- **Accuracy-focused (i=14, i=29):** Both increase accuracy but at the cost of higher DP and EO.  \n",
    "- **Balanced step (i=24/39):** Slight accuracy gain with only modest fairness deterioration.  \n",
    "- Candidates with **lower DP** than baseline exist only at **poor accuracy levels** (not clinically acceptable).  \n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "The RF frontier primarily trades **accuracy gains for fairness losses**.  \n",
    "- If the clinical goal is **error-rate parity (low EO)** → stay near **i=25/30**.  \n",
    "- If **accuracy** is prioritized, **i=14** is strongest (Acc ≈ 0.875, EO small but ↑, DP worse).  \n",
    "- If you want a **mild trade-off**, **i=24/39** offers slightly higher accuracy with EO/DP still moderate.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5477483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RF + GS (EO): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (EO)] i=25\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.15625  0.666667       0.236842  0.815789\n",
      "1    1.000000  1.00000  1.000000       1.000000  0.657534\n",
      "Accuracy: 0.6902 | DP diff: 0.7632 | EO diff: 0.8438\n",
      "\n",
      "[RF + GS (EO)] i=14\n",
      "       TPR      FPR  Recall  SelectionRate  Accuracy\n",
      "Sex                                                 \n",
      "0    0.500  0.96875   0.500       0.894737  0.105263\n",
      "1    0.875  0.16000   0.875       0.630137  0.863014\n",
      "Accuracy: 0.7065 | DP diff: 0.2646 | EO diff: 0.8087\n",
      "\n",
      "--- Summary (RF + GS (EO)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "1  14    0.7065   0.2646   0.8088\n",
      "0  25    0.6902   0.7632   0.8438\n",
      "\n",
      "=== RF + GS (DP): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (DP)] i=25\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.15625  0.833333       0.263158  0.842105\n",
      "1    0.833333  0.18000  0.833333       0.609589  0.828767\n",
      "Accuracy: 0.8315 | DP diff: 0.3464 | EO diff: 0.0237\n",
      "\n",
      "[RF + GS (DP)] i=14\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.09375  0.833333       0.210526  0.894737\n",
      "1    0.864583  0.12000  0.864583       0.609589  0.869863\n",
      "Accuracy: 0.8750 | DP diff: 0.3991 | EO diff: 0.0312\n",
      "\n",
      "--- Summary (RF + GS (DP)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "1  14    0.8750   0.3991   0.0312\n",
      "0  25    0.8315   0.3464   0.0237\n"
     ]
    }
   ],
   "source": [
    "# Show results for the specific frontier models\n",
    "# for both RF GridSearch runs (EO- and DP-constrained).\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "indices = [25,14]\n",
    "\n",
    "def eval_selected(gs, label):\n",
    "    rows = []\n",
    "    n = len(gs.predictors_)\n",
    "    print(f\"\\n=== {label}: {n} frontier candidates ===\")\n",
    "    for i in indices:\n",
    "        if i >= n:\n",
    "            print(f\"[{label}] Skipping i={i} (only {n} candidates).\")\n",
    "            continue\n",
    "        clf = gs.predictors_[i]\n",
    "        y_hat = clf.predict(X_test_ready)\n",
    "        m = eval_fairness(y_test, y_hat, A_test)\n",
    "        rows.append({\"i\": i, \"accuracy\": m[\"acc\"], \"dp_diff\": m[\"dp\"], \"eo_diff\": m[\"eo\"]})\n",
    "\n",
    "        # Per-group breakdown for this model\n",
    "        print(f\"\\n[{label}] i={i}\")\n",
    "        print(m[\"by_group\"])\n",
    "        print(f\"Accuracy: {m['acc']:.4f} | DP diff: {m['dp']:.4f} | EO diff: {m['eo']:.4f}\")\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows).sort_values(\"i\").round(4)\n",
    "        print(f\"\\n--- Summary ({label}) ---\")\n",
    "        print(df)\n",
    "\n",
    "# Evaluate selected indices for both EO and DP GridSearch objects\n",
    "eval_selected(gs_eo_rf, \"RF + GS (EO)\")\n",
    "eval_selected(gs_dp_rf, \"RF + GS (DP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e2f04",
   "metadata": {},
   "source": [
    "### RF + GridSearch — Interpretation  \n",
    "\n",
    "#### GS (Equalized Odds constraint)  \n",
    "| Candidate | Accuracy | DP diff | EO diff | Quick read |\n",
    "|-----------|:--------:|:-------:|:-------:|------------|\n",
    "| **i=25**  | 0.6902   | 0.7632  | 0.8438  | Reject: very poor accuracy, extreme DP/EO gaps |\n",
    "| i=14      | 0.7065   | 0.2646  | 0.8088  | Reject: slightly better DP, but EO catastrophic (F FPR ≈ 0.969) |\n",
    "\n",
    "**Summary:** Under EO constraint, **no viable candidate** emerges. Both i=14 and i=25 are dominated by **huge error-rate disparities** and **low accuracy**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### GS (Demographic Parity constraint)  \n",
    "| Candidate | Accuracy | DP diff | EO diff | Quick read |\n",
    "|-----------|:--------:|:-------:|:-------:|------------|\n",
    "| **i=14**  | **0.8750** | 0.3991 | **0.0312** | Best trade-off: **highest accuracy**, strong EO improvement |\n",
    "| i=25      | 0.8315   | **0.3464** | 0.0237 | Better DP & EO, but accuracy lower |\n",
    "\n",
    "**Why i=14 (DP) stands out:**  \n",
    "- **Accuracy ↑** to 0.8750 (best across candidates).  \n",
    "- **EO ↓** to 0.0312 (near parity, driven by balanced TPR/FPR: 0.833 vs 0.865, 0.094 vs 0.120).  \n",
    "- **DP gap** remains large (0.211 vs 0.610 → DP ≈ 0.399), only slightly worse than baseline.  \n",
    "\n",
    "---\n",
    "\n",
    "### Takeaways  \n",
    "- **EO constraint (GS-EO):** No usable points — EO exploded and accuracy collapsed.  \n",
    "- **DP constraint (GS-DP):**  \n",
    "  - **i=14** offers the **best balance**: highest accuracy + very low EO, though DP gap persists.  \n",
    "  - **i=25** gives slightly lower DP (0.346) and lowest EO (0.024), but at reduced accuracy.  \n",
    "\n",
    "**Conclusion:** If the goal is **high accuracy with minimized error-rate disparity**, **GS(DP) i=14** is the strongest candidate.  \n",
    "If **slightly better DP** is prioritized and accuracy loss is acceptable, **GS(DP) i=25** is preferable.  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001876d",
   "metadata": {},
   "source": [
    "### Bias Mitigation RD: Post-processing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8238f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.15625  0.833333       0.263158  0.842105\n",
      "1    0.833333  0.18000  0.833333       0.609589  0.828767\n",
      "Accuracy: 0.8315 | DP diff: 0.3464 | EO diff: 0.0237\n",
      "\n",
      "=== RF + Post-processing (Equalized Odds) ===\n",
      "          TPR  FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                  \n",
      "0    0.833333  0.0  0.833333       0.131579  0.973684\n",
      "1    0.833333  0.2  0.833333       0.616438  0.821918\n",
      "Accuracy: 0.8533 | DP diff: 0.4849 | EO diff: 0.2000\n",
      "\n",
      "=== RF + Post-processing (Demographic Parity) ===\n",
      "          TPR  FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                  \n",
      "0    0.833333  0.0  0.833333       0.131579  0.973684\n",
      "1    0.833333  0.2  0.833333       0.616438  0.821918\n",
      "Accuracy: 0.8533 | DP diff: 0.4849 | EO diff: 0.2000\n",
      "\n",
      "=== Random Forest: Baseline vs Post-processing ===\n",
      "            model  accuracy  dp_diff  eo_diff\n",
      "0     RF Baseline    0.8315   0.3464   0.0237\n",
      "1  RF + Post (EO)    0.8533   0.4849   0.2000\n",
      "2  RF + Post (DP)    0.8533   0.4849   0.2000\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# 0) Baseline RF \n",
    "rf.fit(X_train_ready, y_train)\n",
    "y_rf_base = rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds \n",
    "post_rf_eo = ThresholdOptimizer(\n",
    "    estimator=rf,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_eo = post_rf_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_rf_eo = eval_fairness(y_test, y_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity \n",
    "post_rf_dp = ThresholdOptimizer(\n",
    "    estimator=rf,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_dp = post_rf_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_rf_dp = eval_fairness(y_test, y_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "#3) Summary Table\n",
    "summary_rf_post = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",       \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (EO)\",    \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (DP)\",    \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs Post-processing ===\")\n",
    "print(summary_rf_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde82b4a",
   "metadata": {},
   "source": [
    "# Random Forest Bias Mitigation (Post-processing)  \n",
    "\n",
    "## Summary\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Interpretation                                      |\n",
    "|---------------------|:--------:|:-------:|:-------:|-----------------------------------------------------|\n",
    "| **RF Baseline**     | 0.8315   | 0.3464  | 0.0237  | Solid accuracy; moderate DP gap; EO very low.       |\n",
    "| **RF + Post (EO)**  | 0.8533   | 0.4849  | 0.2000  | **Accuracy ↑**, but **DP worsens sharply**; EO ↑.   |\n",
    "| **RF + Post (DP)**  | 0.8533   | 0.4849  | 0.2000  | Identical to EO → no fairness gain, disparities ↑.  |\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation\n",
    "- **Baseline RF** already shows a **large DP disparity (0.35)** but **minimal EO (0.02)**.  \n",
    "- **Post-processing (EO/DP)** increased accuracy slightly, but **both fairness gaps worsened**:  \n",
    "  - **DP gap** rose from 0.346 → 0.485 due to much lower female selection (**0.13 vs 0.62** for males).  \n",
    "  - **EO gap** widened from 0.024 → 0.200, with error-rate differences (**FPR 0.00 vs 0.20**).  \n",
    "- Both **EO and DP constraints converged to the same adjusted predictions**, showing **no effective bias mitigation** here.  \n",
    "\n",
    "**Takeaway:** For RF, **ThresholdOptimizer harms fairness** under both EO and DP settings—accuracy gains come at the cost of **much larger disparities**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2c505",
   "metadata": {},
   "source": [
    "### Deep Learning - Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b0d5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required library \n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4cbac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best MLP params: {'learning_rate_init': 0.001, 'hidden_layer_sizes': (64, 32), 'batch_size': 16, 'alpha': 0.001, 'activation': 'relu'}\n",
      "=== Best MLP Evaluation ===\n",
      "Accuracy : 0.8532608695652174\n",
      "Precision: 0.9120879120879121\n",
      "Recall   : 0.8137254901960784\n",
      "F1 Score : 0.8601036269430051\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.85        82\n",
      "           1       0.91      0.81      0.86       102\n",
      "\n",
      "    accuracy                           0.85       184\n",
      "   macro avg       0.85      0.86      0.85       184\n",
      "weighted avg       0.86      0.85      0.85       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[74  8]\n",
      " [19 83]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Improved MLP pipeline: recall-first tuning  \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    f1_score, recall_score, fbeta_score, make_scorer\n",
    ")\n",
    "\n",
    "\n",
    "# 1) Recall-first search (Adam + early_stopping)\n",
    "base_mlp = MLPClassifier(\n",
    "    solver=\"adam\",\n",
    "    early_stopping=True,          # uses internal 15% validation\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=20,\n",
    "    max_iter=2000,                # allow convergence\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"hidden_layer_sizes\": [(64,), (128,), (64, 32), (128, 64)],\n",
    "    \"activation\": [\"relu\", \"tanh\"],\n",
    "    \"alpha\": [1e-5, 1e-4, 3e-4, 1e-3],\n",
    "    \"learning_rate_init\": [1e-3, 5e-4, 3e-4, 1e-4],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# multi-metric scoring; refit on recall-oriented F-beta\n",
    "scoring = {\n",
    "    \"f1\": make_scorer(f1_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"fbeta2\": make_scorer(fbeta_score, beta=2)  # emphasize recall\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=base_mlp,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=scoring,\n",
    "    refit=\"fbeta2\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rs.fit(X_train_ready, y_train)\n",
    "recallfirst_best_mlp = rs.best_estimator_\n",
    "print(\"Best MLP params:\", rs.best_params_)\n",
    "\n",
    "# 2) Evaluation\n",
    "y_prob = recallfirst_best_mlp.predict_proba(X_test_ready)[:, 1]\n",
    "y_pred_best_mlp = recallfirst_best_mlp.predict(X_test_ready)\n",
    "evaluate_model(y_test, y_pred_best_mlp, model_name=f\"Best MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775454c",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "857cf027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.822917  0.1200  0.822917       0.582192  0.842466\n",
      "Accuracy: 0.8533 | DP diff: 0.4243 | EO diff: 0.1562\n",
      "\n",
      "=== In-processing MLP: EG (Equalized Odds) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.770833  0.1800  0.770833       0.568493  0.787671\n",
      "Accuracy: 0.8098 | DP diff: 0.4106 | EO diff: 0.1175\n",
      "\n",
      "=== In-processing MLP: EG (Demographic Parity) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.791667  0.1600  0.791667       0.575342  0.808219\n",
      "Accuracy: 0.8261 | DP diff: 0.4174 | EO diff: 0.1250\n",
      "\n",
      "=== MLP: Baseline vs In-processing (EG) ===\n",
      "           model  accuracy  dp_diff  eo_diff\n",
      "0   MLP Baseline    0.8533   0.4243   0.1562\n",
      "1  MLP + EG (EO)    0.8098   0.4106   0.1175\n",
      "2  MLP + EG (DP)    0.8261   0.4174   0.1250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "mlp = clone(recallfirst_best_mlp)\n",
    "\n",
    "# 0) Baseline MLP (seeded for reproducibility)\n",
    "mlp.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_mlp_base = mlp.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_pred_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) EG with Equalized Odds\n",
    "eg_eo_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(mlp),   # unfitted clone of your tuned MLP\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_eo = eval_fairness(y_test, y_pred_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP: EG (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity\n",
    "eg_dp_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(mlp),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_dp = eval_fairness(y_test, y_pred_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP: EG (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",  \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (EO)\", \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (DP)\", \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943e26",
   "metadata": {},
   "source": [
    "### MLP — In-Processing: Exponentiated Gradient\n",
    "\n",
    "#### Comparative summary\n",
    "| Model          | Accuracy | ΔAcc (pp) | DP diff |  ΔDP   | EO diff |  ΔEO   | Notes |\n",
    "|----------------|:--------:|:---------:|:-------:|:------:|:-------:|:------:|-------|\n",
    "| **Baseline**   | 0.8533   |     –     | 0.4243  |   –    | 0.1562  |   –    | Reference |\n",
    "| **EG (EO)**    | 0.8098   | **−4.4**  | 0.4106  | −0.0137| **0.1175**| **−0.0387** | Best EO improvement; small DP ↓; accuracy drops most |\n",
    "| **EG (DP)**    | 0.8261   | −2.7      | 0.4174  | −0.0069| 0.1250  | −0.0312| Slight EO & DP gains; smaller accuracy hit |\n",
    "\n",
    "#### Readout\n",
    "- **Baseline:** Selection rates **S=0: 0.158** vs **S=1: 0.582** → **DP = 0.424**; TPR gap ≈ **0.156** dominates **EO = 0.156**.\n",
    "- **EG (EO):** Narrows both **TPR** and **FPR** gaps → **EO ↓ to 0.118** and **DP ↓ slightly**, at a **−4.4 pp** accuracy cost.\n",
    "- **EG (DP):** Also reduces **EO** (to **0.125**) and **DP** (to **0.417**), with a **smaller accuracy drop** (−2.7 pp).\n",
    "\n",
    "**Takeaway:** Both EG variants improve fairness (EO ↓, DP ↓) but **trade off accuracy**.  \n",
    "- If minimizing **error-rate disparity (EO)** is primary → **EG (EO)**.  \n",
    "- If you want **milder accuracy loss** with still-better EO and DP → **EG (DP)**.  \n",
    "DP remains sizeable in all cases; further tuning may be needed to close the selection-rate gap.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11de87",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1b9ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing MLP: GridSearch (Equalized Odds) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.822917  0.1200  0.822917       0.582192  0.842466\n",
      "Accuracy: 0.8533 | DP diff: 0.4243 | EO diff: 0.1562\n",
      "\n",
      "=== In-processing MLP: GridSearch (Demographic Parity) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.854167  0.2200  0.854167       0.636986  0.828767\n",
      "Accuracy: 0.8424 | DP diff: 0.4791 | EO diff: 0.1875\n",
      "\n",
      "=== MLP: Baseline vs EG vs GS ===\n",
      "           model  accuracy  dp_diff  eo_diff\n",
      "0   MLP Baseline    0.8533   0.4243   0.1562\n",
      "1  MLP + EG (EO)    0.8098   0.4106   0.1175\n",
      "2  MLP + EG (DP)    0.8261   0.4174   0.1250\n",
      "3  MLP + GS (EO)    0.8533   0.4243   0.1562\n",
      "4  MLP + GS (DP)    0.8424   0.4791   0.1875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "mlp = clone(recallfirst_best_mlp)\n",
    "\n",
    "# 1) GridSearch with Equalized Odds (MLP)\n",
    "gs_eo_mlp = GridSearch(\n",
    "    estimator=clone(mlp),                 # unfitted clone of your MLP (inherits random_state=42)\n",
    "    constraints=EqualizedOdds(),\n",
    "    selection_rule=\"tradeoff_optimization\",  \n",
    "    constraint_weight=0.5,                   \n",
    "    grid_size=15                             \n",
    ")\n",
    "gs_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_eo_mlp = eval_fairness(y_test, y_pred_gs_eo_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo_mlp['acc']:.4f} | DP diff: {m_gs_eo_mlp['dp']:.4f} | EO diff: {m_gs_eo_mlp['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity (MLP)\n",
    "gs_dp_mlp = GridSearch(\n",
    "    estimator=clone(mlp),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15\n",
    ")\n",
    "gs_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_dp_mlp = eval_fairness(y_test, y_pred_gs_dp_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp_mlp['acc']:.4f} | DP diff: {m_gs_dp_mlp['dp']:.4f} | EO diff: {m_gs_dp_mlp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with existing MLP runs (baseline + EG)\n",
    "summary_mlp = pd.concat([\n",
    "    summary_mlp,\n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"MLP + GS (EO)\", \"accuracy\":m_gs_eo_mlp[\"acc\"], \"dp_diff\":m_gs_eo_mlp[\"dp\"], \"eo_diff\":m_gs_eo_mlp[\"eo\"]},\n",
    "        {\"model\":\"MLP + GS (DP)\", \"accuracy\":m_gs_dp_mlp[\"acc\"], \"dp_diff\":m_gs_dp_mlp[\"dp\"], \"eo_diff\":m_gs_dp_mlp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs EG vs GS ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4cf0b",
   "metadata": {},
   "source": [
    "### MLP — In-Processing: Exponentiated Gradient vs. GridSearch\n",
    "\n",
    "#### Comparative Summary\n",
    "| Model          | Accuracy | ΔAcc (pp) | DP diff |  ΔDP   | EO diff |  ΔEO   | Notes |\n",
    "|----------------|:--------:|:---------:|:-------:|:------:|:-------:|:------:|-------|\n",
    "| **Baseline**   | 0.8533   |    –      | 0.4243  |   –    | 0.1562  |   –    | Reference |\n",
    "| **EG (EO)**    | 0.8098   | −4.4      | 0.4106  | −0.0137| **0.1175** | **−0.0387** | Best EO gain; small DP ↓; accuracy drops most |\n",
    "| **EG (DP)**    | 0.8261   | −2.7      | 0.4174  | −0.0069| 0.1250  | −0.0312| Balanced: EO ↓, DP ↓ slightly; moderate accuracy hit |\n",
    "| **GS (EO)**    | 0.8533   |   0.0     | 0.4243  |  0.0000| 0.1562  |  0.0000| Identical to baseline; constraint ineffective |\n",
    "| **GS (DP)**    | 0.8424   | −1.1      | 0.4791  | +0.0548| 0.1875  | +0.0313| Accuracy ↓; DP and EO both **worsen** |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline:** Already shows **large DP disparity (0.424)** and a moderate **EO gap (0.156)** due to different selection rates (Females 0.158 vs. Males 0.582).\n",
    "- **Exponentiated Gradient (EG):**\n",
    "  - **EG (EO):** Strongest fairness improvement → **EO ↓ to 0.118**, with a small DP reduction. However, comes with the **largest accuracy drop (−4.4 pp)**.\n",
    "  - **EG (DP):** Produces a **milder trade-off**: both DP and EO improve slightly, accuracy loss smaller (−2.7 pp).\n",
    "- **GridSearch (GS):**\n",
    "  - **GS (EO):** No effect → converged to baseline solution.\n",
    "  - **GS (DP):** **Counterproductive** → worsens both DP and EO, with reduced accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "#### Takeaway\n",
    "- **EG methods are effective** at improving fairness, especially **EG (EO)** for minimizing error-rate disparity, though at a cost to accuracy.\n",
    "- **GS methods under current settings are ineffective or harmful**: GS (EO) had no effect, GS (DP) worsened disparities.\n",
    "- For CVD bias mitigation, **EG (EO)** is best if **minimizing EO** is the priority; **EG (DP)** offers a more balanced option with smaller accuracy loss.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d29d4a",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Postprocessing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4591c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.822917  0.1200  0.822917       0.582192  0.842466\n",
      "Accuracy: 0.8533 | DP diff: 0.4243 | EO diff: 0.1562\n",
      "\n",
      "=== MLP + Post-processing (Equalized Odds) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.09375  0.666667       0.184211  0.868421\n",
      "1    0.822917  0.12000  0.822917       0.582192  0.842466\n",
      "Accuracy: 0.8478 | DP diff: 0.3980 | EO diff: 0.1562\n",
      "\n",
      "=== MLP + Post-processing (Demographic Parity) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    0.875000  0.2200  0.875000       0.650685  0.842466\n",
      "Accuracy: 0.8533 | DP diff: 0.4928 | EO diff: 0.2083\n",
      "\n",
      "=== MLP: Baseline vs Post-processing ===\n",
      "             model  accuracy  dp_diff  eo_diff\n",
      "0     MLP Baseline    0.8533   0.4243   0.1562\n",
      "1  MLP + Post (EO)    0.8478   0.3980   0.1562\n",
      "2  MLP + Post (DP)    0.8533   0.4928   0.2083\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "import pandas as pd\n",
    "\n",
    "mlp = clone(recallfirst_best_mlp)\n",
    "\n",
    "# 0) Baseline MLP\n",
    "mlp.fit(X_train_ready, y_train)\n",
    "y_mlp_base = mlp.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds\n",
    "post_mlp_eo = ThresholdOptimizer(\n",
    "    estimator=mlp,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_eo = post_mlp_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_eo = eval_fairness(y_test, y_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity\n",
    "post_mlp_dp = ThresholdOptimizer(\n",
    "    estimator=mlp,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_dp = post_mlp_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_dp = eval_fairness(y_test, y_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp_post = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",       \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (EO)\",    \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (DP)\",    \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs Post-processing ===\")\n",
    "print(summary_mlp_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d9e15",
   "metadata": {},
   "source": [
    "# Gender Bias Mitigation in CVD Prediction — Overall Interpretation\n",
    "\n",
    "**Fairness metrics:**  \n",
    "- **DP diff** (Demographic Parity): selection-rate gap across sexes (lower = more equal triage/alerts).  \n",
    "- **EO diff** (Equalized Odds): error-rate gap (combined TPR/FPR gap) across sexes (lower = more equal misses/false alarms).\n",
    "\n",
    "---\n",
    "\n",
    "## One-glance summary (best observed per family)\n",
    "\n",
    "| Model family | Best config in your runs | Accuracy | DP diff | EO diff | Why it’s “best” |\n",
    "|---|---:|---:|---:|---:|---|\n",
    "| **PCA+KNN** | *(none effective)* | — | — | — | Post-processing made **0% flips**; CR modestly ↓DP/EO but at an accuracy cost. |\n",
    "| **Decision Tree** | **EG (DP)** | 0.7826 | **0.2812** | **0.0208** | Strong EO gain, DP reduced, with modest acc cost. |\n",
    "|  | Post (EO) | 0.8315 | 0.3327 | 0.1667 | Accuracy ↑, but fairness worsened. |\n",
    "|  | Post (DP) | 0.8424 | 0.4380 | 0.3333 | Accuracy ↑, but both DP & EO worsened. |\n",
    "| **Random Forest** | **GS (DP, i=14)** | **0.8750** | 0.3991 | **0.0312** | Highest accuracy among RF; EO strongly improved; DP still large. |\n",
    "|  | GS (DP, i=25) | 0.8315 | **0.3464** | **0.0237** | Matches baseline acc; lowest DP/EO combo but no gains. |\n",
    "| **MLP** | **EG (EO)** | 0.8098 | 0.4106 | **0.1175** | Best EO reduction (−0.039); acc −4.4 pp. |\n",
    "|  | **EG (DP)** | 0.8261 | 0.4174 | 0.1250 | Balanced EO/DP ↓; acc −2.7 pp. |\n",
    "|  | *(Post / GS)* | — | — | — | Post-processing and GS were **harmful or ineffective**. |\n",
    "\n",
    "> Baselines to remember: **DT** (acc 0.8207, DP 0.3306, EO 0.1042) • **RF** (acc 0.8315, DP 0.346, EO 0.024) • **MLP** (acc 0.8533, DP 0.424, EO 0.156) • **PCA+KNN** (acc 0.875, DP 0.438, EO 0.188)\n",
    "\n",
    "---\n",
    "\n",
    "## What worked \n",
    "\n",
    "### When the clinical priority is **error-rate parity (EO)**  \n",
    "\n",
    "- **Decision Tree + EG (DP):** EO **0.104 → 0.021**, DP ↓, accuracy −3.8 pp.  \n",
    "- **Random Forest + GS (DP, i=14):** EO **0.024 → 0.031** with **highest acc (0.875)**.  \n",
    "- **MLP + EG (EO):** EO **0.156 → 0.118**; strongest EO improvement for NN, but accuracy −4.4 pp.  \n",
    "\n",
    "**Pick:**  \n",
    "- For **near-zero EO** → **DT + EG (DP)**.  \n",
    "- For **best EO in an ensemble** → **RF + GS (DP, i=14)**.  \n",
    "- For **NN fairness** → **MLP + EG (EO)**, if accuracy loss acceptable.  \n",
    "\n",
    "---\n",
    "\n",
    "### When the priority is **outcome parity (DP)**  \n",
    "\n",
    "- **Decision Tree + EG (DP):** DP **0.331 → 0.281** (improvement), EO nearly eliminated, modest acc drop.  \n",
    "- **Random Forest + GS (DP, i=25):** Best DP (0.346) with EO 0.024, but no acc gain.  \n",
    "- **MLP + EG (DP):** DP **0.424 → 0.417**, EO ↓, with moderate acc drop.  \n",
    "\n",
    "**Pick:**  \n",
    "- For **DT** → **EG (DP)** (best DP/EO combo).  \n",
    "- For **RF** → **GS (DP, i=25)** (best DP/EO at baseline acc).  \n",
    "- For **NN** → **EG (DP)** (slight DP/EO ↓ with moderate acc hit).  \n",
    "\n",
    "---\n",
    "\n",
    "### Approaches that were **ineffective or harmful**\n",
    "\n",
    "- **PCA+KNN Post-processing:** **0% label flips**; CR improved fairness slightly but reduced acc.  \n",
    "- **RF EG (EO/DP):** **No effect**.  \n",
    "- **RF Post (EO/DP):** Acc ↑ but **DP and EO worsened sharply**.  \n",
    "- **MLP GS & Post-processing:** Mostly worsened fairness or accuracy.  \n",
    "- **DT Post (EO/DP):** Accuracy ↑ but **both DP and EO gaps worsened**.  \n",
    "- **DT GS (EO):** No change (baseline repeated).  \n",
    "- **DT GS (DP):** Minor DP ↓, but EO ↑ and accuracy ↓.  \n",
    "- **RF GS (EO):** Generated **degenerate solutions** (low acc, EO explosion).  \n",
    "\n",
    "---\n",
    "\n",
    "### Practical guidance for CVD deployment\n",
    "\n",
    "1. **Decide your fairness target:**  \n",
    "   - **EO focus (clinical safety):** minimize error-rate disparity → aim EO ≤ 0.05.  \n",
    "   - **DP focus (access equity):** equalize selection rates → aim DP ≤ 0.10–0.15.  \n",
    "\n",
    "2. **Pick mitigation accordingly:**  \n",
    "   - **EO-driven:**  \n",
    "     - **DT + EG (DP):** EO nearly eliminated (0.021).  \n",
    "     - **RF + GS (DP, i=14):** EO 0.031, high acc (0.875).  \n",
    "     - **MLP + EG (EO):** EO 0.118, tolerable acc loss.  \n",
    "   - **DP-driven:**  \n",
    "     - **DT + EG (DP):** DP 0.281, EO 0.021.  \n",
    "     - **RF + GS (DP, i=25):** lowest DP (0.346) with EO 0.024.  \n",
    "     - **MLP + EG (DP):** mild DP/EO ↓ with moderate acc loss.  \n",
    "\n",
    "3. **Lock criteria before final selection:**  \n",
    "   - Use thresholds (e.g., **EO ≤ 0.05, DP ≤ 0.30, acc ≥ baseline −0.5 pp**).  \n",
    "   - Select frontier candidates that meet all thresholds.  \n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **No universal fix**: each model family offers different trade-offs.  \n",
    "- **EO parity** (error-rate balance) is best addressed by:  \n",
    "  - **DT + EG (DP)** (EO ↓ to 0.021),  \n",
    "  - **RF + GS (DP, i=14)** (EO 0.031 with top accuracy),  \n",
    "  - **MLP + EG (EO)** (EO ↓ with accuracy hit).  \n",
    "- **DP parity** (equal outcomes) is harder: only **DT + EG (DP)** achieved a meaningful DP ↓ (0.281).  \n",
    "- **Avoid** PCA+KNN post, RF EG/Post, DT Post, DT GS(EO), and MLP GS/Post — they failed to improve fairness.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160be2a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
