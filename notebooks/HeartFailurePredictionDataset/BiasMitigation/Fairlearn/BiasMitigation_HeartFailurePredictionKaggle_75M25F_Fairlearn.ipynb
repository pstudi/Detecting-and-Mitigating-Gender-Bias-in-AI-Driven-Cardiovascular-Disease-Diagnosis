{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288d499e",
   "metadata": {},
   "source": [
    "## Bias Mitigation using Fairlearn - Heart Failure Prediction Dataset (Source: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e647717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>146.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>150.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>128.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>120.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
       "0   61    1              3      146.0        241.0          0           0   \n",
       "1   52    1              1      120.0        284.0          0           0   \n",
       "2   48    0              3      150.0        227.0          0           0   \n",
       "3   49    1              3      128.0        212.0          0           0   \n",
       "4   56    1              3      120.0        236.0          0           1   \n",
       "\n",
       "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n",
       "0  148.0               1      3.0         0             1  \n",
       "1  118.0               0      0.0         2             0  \n",
       "2  130.0               1      1.0         1             0  \n",
       "3   96.0               1      0.0         1             1  \n",
       "4  148.0               0      0.0         1             1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load preprocessed data \n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"./data_subsets/train_75M_25F.csv\")\n",
    "\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")\n",
    "\n",
    "#check out the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5330b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y_test is a Series (not a DataFrame with 1 column)\n",
    "y_test = y_test.squeeze(\"columns\")\n",
    "\n",
    "# Define target and sensitive column names\n",
    "TARGET = \"HeartDisease\"\n",
    "SENSITIVE = \"Sex\"\n",
    "\n",
    "# Split train into X/y\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "# Extract sensitive features separately\n",
    "A_train = X_train[SENSITIVE].astype(int)\n",
    "A_test  = X_test[SENSITIVE].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749ae738",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"HeartDisease\"\n",
    "SENSITIVE = \"Sex\"   # 1 = Male, 0 = Female\n",
    "\n",
    "categorical_cols = ['Sex','ChestPainType','FastingBS','RestingECG','ExerciseAngina','ST_Slope']\n",
    "continuous_cols  = ['Age','RestingBP','Cholesterol','MaxHR','Oldpeak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fe70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into X / y and keep sensitive feature for fairness evaluation\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d93327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric features only, fit on train, transform test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_train.index\n",
    ")\n",
    "X_test_num_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9f63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode categoricals; numeric are kept as is \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\", sparse_output=False)\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "X_train_cat = pd.DataFrame(\n",
    "    ohe.transform(X_train[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_cat = pd.DataFrame(\n",
    "    ohe.transform(X_test[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e79e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shapes: (600, 18) (184, 18)\n"
     ]
    }
   ],
   "source": [
    "# Assemble final matrices\n",
    "X_train_ready = pd.concat([X_train_cat, X_train_num_scaled], axis=1)\n",
    "X_test_ready  = pd.concat([X_test_cat,  X_test_num_scaled],  axis=1)\n",
    "\n",
    "print(\"Final feature shapes:\", X_train_ready.shape, X_test_ready.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade7ec1",
   "metadata": {},
   "source": [
    "### Traditional ML Models - Baseline: K-Nearest Neighbors (KNN) & Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf455d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "#define a function \n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"=== {model_name} Evaluation ===\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred, average='binary'))\n",
    "    print(\"Recall   :\", recall_score(y_true, y_pred, average='binary'))\n",
    "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='binary'))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f7163",
   "metadata": {},
   "source": [
    "### PCA-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7c94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned PCA+KNN, no mitigation) ===\n",
      "=== KNN (best params) Evaluation ===\n",
      "Accuracy : 0.8858695652173914\n",
      "Precision: 0.9090909090909091\n",
      "Recall   : 0.8823529411764706\n",
      "F1 Score : 0.8955223880597015\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87        82\n",
      "           1       0.91      0.88      0.90       102\n",
      "\n",
      "    accuracy                           0.89       184\n",
      "   macro avg       0.88      0.89      0.88       184\n",
      "weighted avg       0.89      0.89      0.89       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[73  9]\n",
      " [12 90]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "#1) PCA + KNN pipeline (on one-hot encoded + scaled features)\n",
    "pca_knn = Pipeline([\n",
    "    ('pca', PCA(n_components=0.95, random_state=42)),  # keep 95% variance\n",
    "    ('knn', KNeighborsClassifier(\n",
    "        n_neighbors=15, metric='manhattan', weights='distance'\n",
    "    ))\n",
    "])\n",
    "\n",
    "pca_knn.fit(X_train_ready, y_train)\n",
    "\n",
    "# Inspect PCA details\n",
    "n_comp = pca_knn.named_steps['pca'].n_components_\n",
    "expl_var = pca_knn.named_steps['pca'].explained_variance_ratio_.sum()\n",
    "\n",
    "print(\"=== Baseline (tuned PCA+KNN, no mitigation) ===\")\n",
    "# 2) Evaluate \n",
    "y_pred_pca_knn = pca_knn.predict(X_test_ready)\n",
    "probs_pca_knn = pca_knn.predict_proba(X_test_ready)[:, 1]\n",
    "  \n",
    "evaluate_model(y_test, y_pred_pca_knn, \"KNN (best params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34404c3",
   "metadata": {},
   "source": [
    "### Post-Processing -  KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f8d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned PCA+KNN) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.833333  0.125  0.833333       0.236842  0.868421\n",
      "1    0.885417  0.100  0.885417       0.616438  0.890411\n",
      "Accuracy: 0.8859 | DP diff: 0.3796 | EO diff: 0.0521\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.833333  0.125  0.833333       0.236842  0.868421\n",
      "1    0.885417  0.100  0.885417       0.616438  0.890411\n",
      "Accuracy: 0.8859 | DP diff: 0.3796 | EO diff: 0.0521\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.833333  0.125  0.833333       0.236842  0.868421\n",
      "1    0.885417  0.100  0.885417       0.616438  0.890411\n",
      "Accuracy: 0.8859 | DP diff: 0.3796 | EO diff: 0.0521\n"
     ]
    }
   ],
   "source": [
    "# Demographic Parity post-processing for your tuned PCA+KNN\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function\n",
    "def eval_fairness(y_true, y_pred, A):\n",
    "    mf = MetricFrame(\n",
    "        metrics={\n",
    "            \"TPR\": true_positive_rate,\n",
    "            \"FPR\": false_positive_rate,\n",
    "            \"Recall\": recall_score, \n",
    "            \"SelectionRate\": selection_rate,\n",
    "            \"Accuracy\": accuracy_score,\n",
    "        },\n",
    "        y_true=y_true, y_pred=y_pred, sensitive_features=A\n",
    "    )\n",
    "    return {\n",
    "        \"by_group\": mf.by_group,\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"dp\": demographic_parity_difference(y_true, y_pred, sensitive_features=A),\n",
    "        \"eo\": equalized_odds_difference(y_true, y_pred, sensitive_features=A),\n",
    "    }\n",
    "\n",
    "# 1) Baseline metrics (no mitigation) \n",
    "pca_knn.fit(X_train_ready, y_train)\n",
    "y_base = pca_knn.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (tuned PCA+KNN) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing with DEMOGRAPHIC PARITY\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Post-processing with EQUALIZED ODDS\n",
    "post_eod = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    flip=True,                                # makes randomized post-processing reproducible\n",
    ")\n",
    "post_eod.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_eod = post_eod.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eod = eval_fairness(y_test, y_eod, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eod[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod['acc']:.4f} | DP diff: {m_eod['dp']:.4f} | EO diff: {m_eod['eo']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a334cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Post-Processing (Demographic Parity & Equalized Odds) — PCA+KNN\n",
    "\n",
    "#### Combined Results\n",
    "\n",
    "| Model                                            | Accuracy | DP diff | EO diff | Notes                                                |\n",
    "|--------------------------------------------------|----------|---------|---------|------------------------------------------------------|\n",
    "| Baseline (tuned PCA+KNN)                         | 0.8859   | 0.3796  | 0.0521  | High DP disparity, low EO gap                        |\n",
    "| Post-processing (Demographic Parity constraint)  | 0.8859   | 0.3796  | 0.0521  | Identical to baseline — no fairness change           |\n",
    "| Post-processing (Equalized Odds constraint)      | 0.8859   | 0.3796  | 0.0521  | Identical to baseline — no fairness change           |\n",
    "\n",
    "#### Interpretation\n",
    "- Metrics are unchanged across baseline, DP, and EO post-processing: accuracy ≈ **88.6%**, **DP diff = 0.38 (large)**, **EO diff = 0.05 (small)**.\n",
    "- With KNN, `predict_proba` is **coarse (steps of 1/k)**, limiting randomized thresholding; `ThresholdOptimizer` defaulted to the **baseline mapping**.\n",
    "- **In-processing mitigation via Fairlearn reductions is not available with KNN** (no `sample_weight`), so if DP parity is required, a reductions-compatible estimator or (within KNN) try larger **k** and a higher `grid_size` in `ThresholdOptimizer` can be considered.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb782e92",
   "metadata": {},
   "source": [
    "**CorrelationRemover** will be implemented to improve fairness after DP/EOD post-processing failed to change any predictions (0% flips), leaving metrics unchanged. By removing linear correlation between features and the sensitive attribute, we reduce leakage and make group score distributions more comparable, giving PCA+KNN and also any subsequent post-processing room to adjust selection rates and error rates—all while staying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f37790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessing: CorrelationRemover + PCA+KNN ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    1.000000  0.09375  1.000000       0.236842  0.921053\n",
      "1    0.854167  0.10000  0.854167       0.595890  0.869863\n",
      "Accuracy: 0.8804 | DP diff: 0.3590 | EO diff: 0.1458\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "from sklearn.metrics import recall_score  \n",
    "\n",
    "# X_* are DataFrames; A_* are Series\n",
    "Xtr_df = X_train_ready.copy()\n",
    "Xte_df = X_test_ready.copy()\n",
    "Xtr_df[\"__A__\"] = A_train.values\n",
    "Xte_df[\"__A__\"] = A_test.values\n",
    "\n",
    "cr = CorrelationRemover(sensitive_feature_ids=[\"__A__\"])\n",
    "\n",
    "Xtr_fair_arr = cr.fit_transform(Xtr_df)   # shape: (n_samples, n_features - 1)\n",
    "Xte_fair_arr = cr.transform(Xte_df)\n",
    "\n",
    "# Rebuild DataFrames with columns that exclude the sensitive column\n",
    "cols_out = [c for c in Xtr_df.columns if c != \"__A__\"]\n",
    "Xtr_fair = pd.DataFrame(Xtr_fair_arr, index=Xtr_df.index, columns=cols_out)\n",
    "Xte_fair = pd.DataFrame(Xte_fair_arr, index=Xte_df.index, columns=cols_out)\n",
    "\n",
    "# Refit your PCA+KNN\n",
    "pca_knn.fit(Xtr_fair, y_train)\n",
    "y_cr = pca_knn.predict(Xte_fair)\n",
    "m_cr = eval_fairness(y_test, y_cr, A_test)\n",
    "\n",
    "print(\"\\n=== Preprocessing: CorrelationRemover + PCA+KNN ===\")\n",
    "print(m_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_cr['acc']:.4f} | DP diff: {m_cr['dp']:.4f} | EO diff: {m_cr['eo']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc7847",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "- Accuracy is **0.8804**, a slight decrease from the baseline (0.8859).\n",
    "- **DP diff = 0.3590** shows a modest improvement vs. baseline (0.3796), but selection rates remain far apart (Sex=0: 0.237 vs. Sex=1: 0.596 ≈ 2.5× higher).\n",
    "- **EO diff = 0.1458** worsened vs. baseline (0.0521), driven by a larger **TPR gap** (Sex=0: 1.000 vs. Sex=1: 0.854); **FPRs** are similar (0.094 vs. 0.100).\n",
    "- Net effect: CorrelationRemover slightly improved **DP** but **hurt EO**, indicating reduced proxy correlation did not equalize error rates and in fact amplified the TPR disparity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79e2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed vs CR baseline (DP):  0.000%\n",
      "Changed vs CR baseline (eOD): 0.000%\n",
      "\n",
      "=== Post-CR (DP) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    1.000000  0.09375  1.000000       0.236842  0.921053\n",
      "1    0.854167  0.10000  0.854167       0.595890  0.869863\n",
      "Accuracy: 0.8804 | DP diff: 0.3590 | EO diff: 0.1458\n",
      "\n",
      "=== Post-CR (eOD) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    1.000000  0.09375  1.000000       0.236842  0.921053\n",
      "1    0.854167  0.10000  0.854167       0.595890  0.869863\n",
      "Accuracy: 0.8804 | DP diff: 0.3590 | EO diff: 0.1458\n"
     ]
    }
   ],
   "source": [
    "# Demographic Parity on top of the CorrelationRemover\n",
    "post_dp_cr = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    flip=True\n",
    ")\n",
    "post_dp_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)\n",
    "y_dp_cr = post_dp_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_dp_cr = eval_fairness(y_test, y_dp_cr, A_test)\n",
    "\n",
    "# Equalized Odds on top of  CorrelationRemover\n",
    "post_eod_cr = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    flip=True\n",
    ")\n",
    "post_eod_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)\n",
    "y_eod_cr = post_eod_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_eod_cr = eval_fairness(y_test, y_eod_cr, A_test)\n",
    "\n",
    "# check out the results\n",
    "import numpy as np\n",
    "print(f\"Changed vs CR baseline (DP):  {np.mean(y_dp_cr  != y_cr):.3%}\")\n",
    "print(f\"Changed vs CR baseline (eOD): {np.mean(y_eod_cr != y_cr):.3%}\")\n",
    "\n",
    "print(\"\\n=== Post-CR (DP) ===\")\n",
    "print(m_dp_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp_cr['acc']:.4f} | DP diff: {m_dp_cr['dp']:.4f} | EO diff: {m_dp_cr['eo']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Post-CR (eOD) ===\")\n",
    "print(m_eod_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod_cr['acc']:.4f} | DP diff: {m_eod_cr['dp']:.4f} | EO diff: {m_eod_cr['eo']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0af743",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "- **0% flips for both DP and eOD** means post-processing after CorrelationRemover made **no label changes**, so metrics are **identical** to the CR+KNN baseline.\n",
    "- **Metrics (unchanged):** Accuracy = **0.8804**; **DP diff = 0.3590** (large gap: selection rates 0.237 vs 0.596); **EO diff = 0.1458**, driven by a **TPR gap** (1.000 vs 0.854) while FPRs are similar (0.094 vs 0.100).\n",
    "- **Interpretation:** Even with `grid_size=1000` and `flip=True`, the optimizer found no feasible re-thresholding better than CR+KNN—consistent with **KNN’s coarse probability grid** and the accuracy–fairness trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074237d",
   "metadata": {},
   "source": [
    "### Bias mitigation comparison (PCA+KNN)\n",
    "\n",
    "| Model variant                      | Accuracy | DP diff | EO diff | SelRate S=0 | SelRate S=1 | TPR S=0 | TPR S=1 | FPR S=0 | FPR S=1 | Notes                          |\n",
    "|-----------------------------------|:--------:|:-------:|:-------:|:-----------:|:-----------:|:-------:|:-------:|:-------:|:-------:|--------------------------------|\n",
    "| Baseline (tuned PCA+KNN)          | 0.8859   | 0.3796  | 0.0521  | 0.2368      | 0.6164      | 0.8333  | 0.8854  | 0.1250  | 0.1000  | Reference                      |\n",
    "| Post-processing (DP constraint)   | 0.8859   | 0.3796  | 0.0521  | 0.2368      | 0.6164      | 0.8333  | 0.8854  | 0.1250  | 0.1000  | **Flips vs baseline: 0%**      |\n",
    "| Post-processing (EO constraint)   | 0.8859   | 0.3796  | 0.0521  | 0.2368      | 0.6164      | 0.8333  | 0.8854  | 0.1250  | 0.1000  | **Flips vs baseline: 0%**      |\n",
    "| CorrelationRemover + PCA+KNN      | 0.8804   | 0.3590  | 0.1458  | 0.2368      | 0.5959      | 1.0000  | 0.8542  | 0.0938  | 0.1000  | New baseline after CR          |\n",
    "| Post-CR (DP constraint)           | 0.8804   | 0.3590  | 0.1458  | 0.2368      | 0.5959      | 1.0000  | 0.8542  | 0.0938  | 0.1000  | **Flips vs CR baseline: 0%**   |\n",
    "| Post-CR (EO constraint)           | 0.8804   | 0.3590  | 0.1458  | 0.2368      | 0.5959      | 1.0000  | 0.8542  | 0.0938  | 0.1000  | **Flips vs CR baseline: 0%**   |\n",
    "\n",
    "**Takeaway:** Post-processing produced no label changes (0% flips) and thus no metric changes; CorrelationRemover slightly reduced DP disparity but increased EO disparity and reduced accuracy. In the CVD prediction setting, the persistent DP gap (≈0.36–0.38) indicates substantially higher positive rates for Males than females risking unequal alerting while decorrelation increased the TPR gap (EO) so that males experiences more missed positives than females, meaning gender bias remains and may be exacerbated by the CR step.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941cf3a",
   "metadata": {},
   "source": [
    "### Tuned Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a66f6420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree params: {'criterion': 'entropy', 'max_depth': 9, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best CV F1: 0.8593494246061409\n",
      "=== Tuned Decision Tree (best params) Evaluation ===\n",
      "Accuracy : 0.8097826086956522\n",
      "Precision: 0.819047619047619\n",
      "Recall   : 0.8431372549019608\n",
      "F1 Score : 0.8309178743961353\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78        82\n",
      "           1       0.82      0.84      0.83       102\n",
      "\n",
      "    accuracy                           0.81       184\n",
      "   macro avg       0.81      0.81      0.81       184\n",
      "weighted avg       0.81      0.81      0.81       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[63 19]\n",
      " [16 86]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# 1) Base model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 2) Hyperparameter grid \n",
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [3, 5, 7, 9, None],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6, 10],\n",
    "}\n",
    "\n",
    "# 3) Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 4) Grid search \n",
    "grid_dt = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"f1\",      \n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_dt.fit(X_train_ready, y_train)\n",
    "\n",
    "print(\"Best Decision Tree params:\", grid_dt.best_params_)\n",
    "print(\"Best CV F1:\", grid_dt.best_score_)\n",
    "\n",
    "# 5) Train & evaluate best DT\n",
    "tuned_dt = grid_dt.best_estimator_\n",
    "y_pred_dt_best = tuned_dt.predict(X_test_ready)\n",
    "y_prob_dt_best = tuned_dt.predict_proba(X_test_ready)[:, 1] \n",
    "\n",
    "evaluate_model(y_test, y_pred_dt_best, \"Tuned Decision Tree (best params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd1d09",
   "metadata": {},
   "source": [
    "### Bias Mitigation DT: Inprocessing - Exponentiated Gradient Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "680a1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Tuned DT) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.28125  0.833333       0.368421  0.736842\n",
      "1    0.843750  0.20000  0.843750       0.623288  0.828767\n",
      "Accuracy: 0.8098 | DP diff: 0.2549 | EO diff: 0.0812\n",
      "\n",
      "=== In-processing: EG (Equalized Odds) ===\n",
      "          TPR   FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                   \n",
      "0    0.666667  0.25  0.666667       0.315789  0.736842\n",
      "1    0.854167  0.22  0.854167       0.636986  0.828767\n",
      "Accuracy: 0.8098 | DP diff: 0.3212 | EO diff: 0.1875\n",
      "\n",
      "=== In-processing: EG (Demographic Parity) ===\n",
      "          TPR   FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                   \n",
      "0    0.833333  0.25  0.833333       0.342105  0.763158\n",
      "1    0.843750  0.22  0.843750       0.630137  0.821918\n",
      "Accuracy: 0.8098 | DP diff: 0.2880 | EO diff: 0.0300\n",
      "\n",
      "=== Decision Tree: Baseline vs In-processing (EG) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.2549</td>\n",
       "      <td>0.0812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.3212</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.0300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)    0.8098   0.2549   0.0812\n",
       "1         DT + EG (EO)    0.8098   0.3212   0.1875\n",
       "2         DT + EG (DP)    0.8098   0.2880   0.0300"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-processing mitigation for tuned Decision Tree\n",
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 0) Baseline: tuned DT without mitigation (for comparison)\n",
    "y_pred_dt_base = tuned_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_pred_dt_base, A_test)\n",
    "print(\"=== Baseline (Tuned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Exponentiated Gradient with Equalized Odds\n",
    "eg_eo = ExponentiatedGradient(\n",
    "    estimator=clone(tuned_dt),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_eo = eg_eo.predict(X_test_ready, random_state=42)\n",
    "m_eo = eval_fairness(y_test, y_pred_eo, A_test)\n",
    "print(\"\\n=== In-processing: EG (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Exponentiated Gradient with Demographic Parity\n",
    "eg_dp = ExponentiatedGradient(\n",
    "    estimator=clone(tuned_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_dp = eg_dp.predict(X_test_ready, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_pred_dp, A_test)\n",
    "print(\"\\n=== In-processing: EG (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary table\n",
    "summary_dt = pd.DataFrame([\n",
    "    {\"model\": \"DT Baseline (tuned)\", \"accuracy\": m_base[\"acc\"], \"dp_diff\": m_base[\"dp\"], \"eo_diff\": m_base[\"eo\"]},\n",
    "    {\"model\": \"DT + EG (EO)\",        \"accuracy\": m_eo[\"acc\"],   \"dp_diff\": m_eo[\"dp\"],   \"eo_diff\": m_eo[\"eo\"]},\n",
    "    {\"model\": \"DT + EG (DP)\",        \"accuracy\": m_dp[\"acc\"],   \"dp_diff\": m_dp[\"dp\"],   \"eo_diff\": m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs In-processing (EG) ===\")\n",
    "summary_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0254766",
   "metadata": {},
   "source": [
    "### Bias Mitigation Results: Decision Tree – In-Processing\n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Model                 | Accuracy | DP diff | EO diff | Notes                                                                 |\n",
    "|-----------------------|:--------:|:-------:|:-------:|------------------------------------------------------------------------|\n",
    "| **DT Baseline (tuned)** | 0.8098   | 0.2549  | 0.0812  | Moderate DP gap; small-to-moderate EO gap                              |\n",
    "| **DT + EG (EO)**      | 0.8098   | 0.3212  | 0.1875  | Accuracy unchanged; **DP worsens** (↑), **EO worsens** sharply         |\n",
    "| **DT + EG (DP)**      | 0.8098   | 0.2880  | 0.0300  | Accuracy unchanged; **EO improves strongly**, DP still elevated        |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- The **baseline DT** already has a **substantial DP disparity** (≈0.25) and a **smaller EO gap** (≈0.08).  \n",
    "- **EG (Equalized Odds)** fails here: it does **not improve accuracy** and makes fairness worse — both **DP** and **EO** increase.  \n",
    "- **EG (Demographic Parity)** is more promising: it drives **EO down to 0.03**, greatly aligning error rates, but **DP remains high** (≈0.29), meaning outcome disparities persist.  \n",
    "\n",
    "**Conclusion:** For this DT, **EG with a DP constraint** provides the most useful improvement (minimizing error-rate disparity without hurting accuracy). However, **DP remains unresolved**, suggesting that further tuning (e.g., smaller `eps`) or alternative mitigation strategies may be needed if demographic parity is a strict requirement.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87407025",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: In-processing: GridSearch Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c97e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing: GridSearch (Equalized Odds) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.28125  0.833333       0.368421  0.736842\n",
      "1    0.843750  0.20000  0.843750       0.623288  0.828767\n",
      "Accuracy: 0.8098 | DP diff: 0.2549 | EO diff: 0.0812\n",
      "\n",
      "=== In-processing: GridSearch (Demographic Parity) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.500000  0.1875  0.500000       0.236842  0.763158\n",
      "1    0.802083  0.2400  0.802083       0.609589  0.787671\n",
      "Accuracy: 0.7826 | DP diff: 0.3727 | EO diff: 0.3021\n",
      "\n",
      "=== Decision Tree: Baseline vs EG vs GS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.2549</td>\n",
       "      <td>0.0812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.3212</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.0300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT + GS (EO)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.2549</td>\n",
       "      <td>0.0812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT + GS (DP)</td>\n",
       "      <td>0.7826</td>\n",
       "      <td>0.3727</td>\n",
       "      <td>0.3021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)    0.8098   0.2549   0.0812\n",
       "1         DT + EG (EO)    0.8098   0.3212   0.1875\n",
       "2         DT + EG (DP)    0.8098   0.2880   0.0300\n",
       "3         DT + GS (EO)    0.8098   0.2549   0.0812\n",
       "4         DT + GS (DP)    0.7826   0.3727   0.3021"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds\n",
    "gs_eo = GridSearch(\n",
    "    estimator=clone(tuned_dt),              # unfitted clone of tuned DT\n",
    "    constraints=EqualizedOdds(),            # EO constraint\n",
    "    selection_rule=\"tradeoff_optimization\", # or: \"best_classifier\", \"minimum_violation\"\n",
    "    constraint_weight=0.5,                  # trade-off weight (0..1); tune this\n",
    "    grid_size=15                          # more points -> finer Pareto front                      \n",
    ")\n",
    "gs_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_eo = gs_eo.predict(X_test_ready)\n",
    "m_gs_eo = eval_fairness(y_test, y_pred_gs_eo, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo['acc']:.4f} | DP diff: {m_gs_eo['dp']:.4f} | EO diff: {m_gs_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity\n",
    "gs_dp = GridSearch(\n",
    "    estimator=clone(tuned_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15\n",
    ")\n",
    "gs_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_dp = gs_dp.predict(X_test_ready)\n",
    "m_gs_dp = eval_fairness(y_test, y_pred_gs_dp, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp['acc']:.4f} | DP diff: {m_gs_dp['dp']:.4f} | EO diff: {m_gs_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with your existing runs\n",
    "summary_dt = pd.concat([\n",
    "    summary_dt,  \n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"DT + GS (EO)\", \"accuracy\":m_gs_eo[\"acc\"], \"dp_diff\":m_gs_eo[\"dp\"], \"eo_diff\":m_gs_eo[\"eo\"]},\n",
    "        {\"model\":\"DT + GS (DP)\", \"accuracy\":m_gs_dp[\"acc\"], \"dp_diff\":m_gs_dp[\"dp\"], \"eo_diff\":m_gs_dp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "print(\"\\n=== Decision Tree: Baseline vs EG vs GS ===\")\n",
    "summary_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86587b",
   "metadata": {},
   "source": [
    "### Decision Tree — In-Processing: EG vs. GridSearch (EO & DP)\n",
    "\n",
    "#### Summary of results\n",
    "| Model                | Accuracy | DP diff | EO diff | Interpretation |\n",
    "|----------------------|:--------:|:-------:|:-------:|----------------|\n",
    "| **DT Baseline (tuned)** | 0.8098   | 0.2549  | 0.0812  | Moderate disparities in DP (≈0.25) and EO (≈0.08). |\n",
    "| **DT + EG (EO)**       | 0.8098   | 0.3212  | 0.1875  | Accuracy unchanged; **DP worsens**; **EO worsens** markedly. |\n",
    "| **DT + EG (DP)**       | 0.8098   | 0.2880  | 0.0300  | Accuracy unchanged; **EO improves strongly**; **DP slightly worse**. |\n",
    "| **DT + GS (EO)**       | 0.8098   | 0.2549  | 0.0812  | **Identical to baseline** — no improvement from GridSearch. |\n",
    "| **DT + GS (DP)**       | 0.7826   | 0.3727  | 0.3021  | Accuracy ↓ (−2.7 pp); **DP and EO both worsen** significantly. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- The **baseline DT** exhibits a **substantial DP disparity** (≈0.25) and a smaller EO gap (≈0.08).  \n",
    "- **EG (EO)** is counterproductive: fairness worsens (DP ↑, EO ↑) with **no accuracy gain**.  \n",
    "- **EG (DP)** is the **most effective option**: EO improves dramatically (0.08 → 0.03) while accuracy is preserved, though DP disparity remains high.  \n",
    "- **GS (EO)** provides **no change**, converging to the baseline point.  \n",
    "- **GS (DP)** performs poorly, reducing accuracy and worsening both fairness metrics — an undesirable outcome.  \n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:**  \n",
    "For this Decision Tree, the best fairness–utility trade-off comes from **Exponentiated Gradient with a Demographic Parity constraint**: it keeps accuracy stable while substantially reducing EO disparity. However, **DP remains unresolved**, and GridSearch methods under current settings provide no benefit or even worsen outcomes.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15838f4",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: Post-processing: Threshold Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "873f1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned DT) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.28125  0.833333       0.368421  0.736842\n",
      "1    0.843750  0.20000  0.843750       0.623288  0.828767\n",
      "Accuracy: 0.8098 | DP diff: 0.2549 | EO diff: 0.0812\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "          TPR   FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                   \n",
      "0    0.666667  0.25  0.666667       0.315789  0.736842\n",
      "1    0.843750  0.20  0.843750       0.623288  0.828767\n",
      "Accuracy: 0.8098 | DP diff: 0.3075 | EO diff: 0.1771\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.833333  0.3125  0.833333       0.394737  0.710526\n",
      "1    0.864583  0.2000  0.864583       0.636986  0.842466\n",
      "Accuracy: 0.8152 | DP diff: 0.2422 | EO diff: 0.1125\n",
      "\n",
      "=== Decision Tree: Baseline vs Post-processing ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.2549</td>\n",
       "      <td>0.0812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + Post (EO)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.3075</td>\n",
       "      <td>0.1771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + Post (DP)</td>\n",
       "      <td>0.8152</td>\n",
       "      <td>0.2422</td>\n",
       "      <td>0.1125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)    0.8098   0.2549   0.0812\n",
       "1       DT + Post (EO)    0.8098   0.3075   0.1771\n",
       "2       DT + Post (DP)    0.8152   0.2422   0.1125"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Baseline for mitigation: fixed tuned DT\n",
    "tuned_dt.fit(X_train_ready, y_train)\n",
    "y_base = tuned_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "print(\"=== Baseline (tuned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# Post-processing: Equalized Odds\n",
    "post_eo = ThresholdOptimizer(\n",
    "    estimator=tuned_dt,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_eo = post_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_eo = eval_fairness(y_test, y_eo, A_test)\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# Post-processing: Demographic Parity\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=tuned_dt,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True,\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# Create summary table \n",
    "summary = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (tuned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (EO)\",      \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (DP)\",      \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs Post-processing ===\")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e209fe",
   "metadata": {},
   "source": [
    "### Decision Tree: Post- vs In-Processing\n",
    "\n",
    "### Combined Results\n",
    "\n",
    "| Model / Method          | Accuracy | DP diff | EO diff | Notes |\n",
    "|-------------------------|:--------:|:-------:|:-------:|-------|\n",
    "| **Baseline (Tuned DT)** | 0.8098   | 0.2549  | 0.0812  | Reference point |\n",
    "| **Post (EO)**           | 0.8098   | 0.3075  | 0.1771  | Accuracy = baseline; **EO worsens sharply**; DP ↑ |\n",
    "| **Post (DP)**           | 0.8152   | **0.2422** | 0.1125 | Slight accuracy gain; **best DP** among post-proc; EO ↑ |\n",
    "| **EG (EO)**             | 0.8098   | 0.3212  | 0.1875  | Accuracy = baseline; both **DP and EO worsen** |\n",
    "| **EG (DP)**             | 0.8098   | 0.2880  | **0.0300** | Accuracy = baseline; **EO improves strongly**; DP ↑ |\n",
    "| **GS (EO)**             | 0.8098   | 0.2549  | 0.0812  | Identical to baseline — no change |\n",
    "| **GS (DP)**             | 0.7826   | 0.3727  | 0.3021  | Accuracy ↓ (−2.7 pp); **worst DP & EO** |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "- The **baseline DT** has a **large DP gap (~0.25)** and a **moderate EO gap (~0.08)**.  \n",
    "- **Post (EO)** worsens fairness overall — EO nearly doubles, DP rises, with no accuracy benefit.  \n",
    "- **Post (DP)** slightly improves DP (best among post-proc, 0.2422) and increases accuracy a bit, but EO worsens notably.  \n",
    "- **EG (EO)** is counterproductive: DP and EO both worsen.  \n",
    "- **EG (DP)** is the only method that **substantially improves EO** (0.03) while keeping accuracy steady, though DP disparity remains high.  \n",
    "- **GS (EO)** returns the baseline point (no improvement).  \n",
    "- **GS (DP)** harms both fairness metrics and lowers accuracy — clearly undesirable.  \n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:**  \n",
    "- If the priority is **error-rate parity (Equalized Odds)** → **EG (DP)** is the best option (EO ↓ to 0.03).  \n",
    "- If the focus is **outcome-rate parity (Demographic Parity)** → **Post (DP)** gives the lowest DP (0.2422) with a slight accuracy boost, but worsens EO.  \n",
    "- **Post (EO), EG (EO), and GS (DP)** should be avoided as they worsen disparities.  \n",
    "- **GridSearch (EO)** adds no value under current settings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbe2d4",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81ec0da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Evaluation ===\n",
      "Accuracy : 0.8804347826086957\n",
      "Precision: 0.8703703703703703\n",
      "Recall   : 0.9215686274509803\n",
      "F1 Score : 0.8952380952380953\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86        82\n",
      "           1       0.87      0.92      0.90       102\n",
      "\n",
      "    accuracy                           0.88       184\n",
      "   macro avg       0.88      0.88      0.88       184\n",
      "weighted avg       0.88      0.88      0.88       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[68 14]\n",
      " [ 8 94]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train_ready, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf.predict(X_test_ready)\n",
    "y_prob_rf = rf.predict_proba(X_test_ready)[:, 1]  \n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daad4ed",
   "metadata": {},
   "source": [
    "### Bias Mitgation RF: In-processing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1199f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    1.000000  0.125  1.000000       0.263158  0.894737\n",
      "1    0.916667  0.200  0.916667       0.671233  0.876712\n",
      "Accuracy: 0.8804 | DP diff: 0.4081 | EO diff: 0.0833\n",
      "\n",
      "=== In-processing RF: EG (Equalized Odds) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    1.000000  0.125  1.000000       0.263158  0.894737\n",
      "1    0.916667  0.200  0.916667       0.671233  0.876712\n",
      "Accuracy: 0.8804 | DP diff: 0.4081 | EO diff: 0.0833\n",
      "\n",
      "=== In-processing RF: EG (Demographic Parity) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    1.000000  0.125  1.000000       0.263158  0.894737\n",
      "1    0.916667  0.200  0.916667       0.671233  0.876712\n",
      "Accuracy: 0.8804 | DP diff: 0.4081 | EO diff: 0.0833\n",
      "\n",
      "=== Random Forest: Baseline vs In-processing (EG) ===\n",
      "          model  accuracy  dp_diff  eo_diff\n",
      "0   RF Baseline    0.8804   0.4081   0.0833\n",
      "1  RF + EG (EO)    0.8804   0.4081   0.0833\n",
      "2  RF + EG (DP)    0.8804   0.4081   0.0833\n"
     ]
    }
   ],
   "source": [
    "# 0) Baseline Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_rf_base = rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_pred_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "# 1) EG with Equalized Odds\n",
    "eg_eo_rf = ExponentiatedGradient(\n",
    "    estimator=clone(rf),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50,\n",
    ")\n",
    "eg_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_eo = eg_eo_rf.predict(X_test_ready)\n",
    "m_rf_eo = eval_fairness(y_test, y_pred_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity \n",
    "eg_dp_rf = ExponentiatedGradient(\n",
    "    estimator=clone(rf),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_dp = eg_dp_rf.predict(X_test_ready)\n",
    "m_rf_dp = eval_fairness(y_test, y_pred_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table \n",
    "summary_rf = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",      \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (EO)\",     \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (DP)\",     \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b0260",
   "metadata": {},
   "source": [
    "## Random Forest Bias Mitigation Results\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model            | Accuracy | DP Diff | EO Diff | Interpretation                                |\n",
    "|------------------|:--------:|:-------:|:-------:|-----------------------------------------------|\n",
    "| **RF Baseline**  | 0.8804   | 0.4081  | 0.0833  | Strong accuracy; **large DP gap** remains (≈0.41); EO moderate. |\n",
    "| **RF + EG (EO)** | 0.8804   | 0.4081  | 0.0833  | **Identical to baseline** — EO constraint had no impact. |\n",
    "| **RF + EG (DP)** | 0.8804   | 0.4081  | 0.0833  | **Identical to baseline** — DP constraint had no impact. |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points\n",
    "- **Random Forest baseline** achieves high accuracy but suffers from a **large disparity in selection rates (DP ≈ 0.41)**, with EO at a moderate level (~0.08).  \n",
    "- **ExponentiatedGradient** with either **Equalized Odds** or **Demographic Parity** produced **no change**.  \n",
    "- This typically occurs when:\n",
    "  - The model is **insensitive to sample reweighting** (as tree ensembles often are).  \n",
    "  - The fairness tolerance (`eps=0.01`) is too strict, so EG returns the original classifier.  \n",
    "- In contrast to Decision Trees, RF here is **locked at its baseline frontier point**, offering no fairness–utility trade-off under EG.  \n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:**  \n",
    "For this RF setup, **EG provides no benefit**. Addressing the strong DP disparity likely requires alternative mitigation strategies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c181c7",
   "metadata": {},
   "source": [
    "### Bias Mitigation: RF: In-processing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4e11367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method  weight       acc        dp        eo\n",
      "5  RF + GS (DP)    0.00  0.880435  0.474405  0.126250\n",
      "6  RF + GS (DP)    0.25  0.880435  0.474405  0.126250\n",
      "7  RF + GS (DP)    0.50  0.880435  0.474405  0.126250\n",
      "8  RF + GS (DP)    0.75  0.880435  0.474405  0.126250\n",
      "9  RF + GS (DP)    1.00  0.880435  0.474405  0.126250\n",
      "0  RF + GS (EO)    0.00  0.880435  0.493872  0.260417\n",
      "1  RF + GS (EO)    0.25  0.880435  0.493872  0.260417\n",
      "2  RF + GS (EO)    0.50  0.880435  0.493872  0.260417\n",
      "3  RF + GS (EO)    0.75  0.880435  0.493872  0.260417\n",
      "4  RF + GS (EO)    1.00  0.880435  0.493872  0.260417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0]   # 0.0 = accuracy-first, 1.0 = fairness-first\n",
    "grid = 50                               \n",
    "\n",
    "rows = []\n",
    "\n",
    "#Equalized Odds sweep\n",
    "for w in weights:\n",
    "    gs_eo_rf = GridSearch(\n",
    "        estimator=clone(rf),                 \n",
    "        constraints=EqualizedOdds(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    # Some versions accept random_state in predict; if yours doesn't, seed numpy before predicting\n",
    "    try:\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (EO)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "# Demographic Parity sweep\n",
    "for w in weights:\n",
    "    gs_dp_rf = GridSearch(\n",
    "        estimator=clone(rf),\n",
    "        constraints=DemographicParity(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    try:\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (DP)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "df_gs = pd.DataFrame(rows).sort_values([\"method\",\"weight\"])\n",
    "print(df_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfa38e",
   "metadata": {},
   "source": [
    "### Interpretation — RF + GridSearch (weight sweep)\n",
    "\n",
    "- **DP-constrained GridSearch:** All weights (0.00 → 1.00) converge to the **same solution**:  \n",
    "  - Accuracy = **0.8804**  \n",
    "  - DP diff = **0.4744** (very high)  \n",
    "  - EO diff = **0.1263**  \n",
    "- **EO-constrained GridSearch:** Similarly, all weights (0.00 → 1.00) produce **identical results**:  \n",
    "  - Accuracy = **0.8804**  \n",
    "  - DP diff = **0.4939** (very high)  \n",
    "  - EO diff = **0.2604** (extremely high)  \n",
    "\n",
    "- The invariance across weights shows that **GridSearch is stuck on a single frontier point** for both constraints.  \n",
    "- These frontier points are **worse than the baseline RF** on fairness:  \n",
    "  - DP disparities are nearly **0.47–0.49** (vs. ~0.41 baseline).  \n",
    "  - EO worsens under the EO constraint (0.26 vs. ~0.08 baseline).  \n",
    "  - Accuracy remains unchanged.  \n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:**  \n",
    "Under current settings, **RF + GridSearch does not provide any useful fairness–utility trade-off**. Both DP- and EO-constrained runs simply reproduce **fairness-worse frontier points** with unchanged accuracy, suggesting the method is ineffective here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2117d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     i       acc        dp        eo\n",
      "0    0  0.869565  0.684932  0.927083\n",
      "1    1  0.864130  0.678082  0.916667\n",
      "2    2  0.875000  0.664384  0.916667\n",
      "3    3  0.869565  0.671233  0.916667\n",
      "4    4  0.875000  0.650685  0.906250\n",
      "5    5  0.864130  0.664384  0.906250\n",
      "6    6  0.869565  0.671233  0.916667\n",
      "7    7  0.875000  0.664384  0.916667\n",
      "8    8  0.706522  0.196107  0.708750\n",
      "9    9  0.880435  0.657534  0.916667\n",
      "10  10  0.880435  0.657534  0.916667\n",
      "11  11  0.875000  0.650685  0.906250\n",
      "12  12  0.875000  0.664384  0.916667\n",
      "13  13  0.875000  0.664384  0.916667\n",
      "14  14  0.722826  0.183490  0.717500\n",
      "15  15  0.728261  0.190339  0.737500\n",
      "16  16  0.701087  0.763158  0.875000\n",
      "17  17  0.880435  0.493872  0.260417\n",
      "18  18  0.885870  0.453857  0.086250\n",
      "19  19  0.891304  0.493872  0.137500\n",
      "20  20  0.885870  0.467556  0.106250\n",
      "21  21  0.869565  0.394376  0.072917\n",
      "22  22  0.717391  0.301370  0.740000\n",
      "23  23  0.717391  0.301370  0.740000\n",
      "24  24  0.706522  0.287671  0.700000\n",
      "25  25  0.701087  0.763158  0.875000\n",
      "26  26  0.701087  0.763158  0.875000\n",
      "27  27  0.885870  0.520187  0.188750\n",
      "28  28  0.891304  0.513338  0.168750\n",
      "29  29  0.875000  0.453857  0.106250\n",
      "30  30  0.880435  0.408075  0.083333\n",
      "31  31  0.885870  0.434391  0.093750\n",
      "32  32  0.717391  0.287671  0.720000\n",
      "33  33  0.706522  0.287671  0.700000\n",
      "34  34  0.722826  0.294521  0.740000\n",
      "35  35  0.711957  0.294521  0.720000\n",
      "36  36  0.695652  0.736842  0.843750\n",
      "37  37  0.891304  0.507570  0.157500\n",
      "38  38  0.880435  0.513338  0.188750\n",
      "39  39  0.880435  0.447008  0.086250\n",
      "40  40  0.864130  0.448089  0.115000\n",
      "41  41  0.875000  0.448089  0.095000\n",
      "42  42  0.728261  0.301370  0.760000\n",
      "43  43  0.711957  0.294521  0.720000\n",
      "44  44  0.706522  0.287671  0.700000\n",
      "45  45  0.885870  0.520187  0.188750\n",
      "46  46  0.880435  0.427541  0.106250\n",
      "47  47  0.885870  0.453857  0.086250\n",
      "48  48  0.891304  0.474405  0.157500\n",
      "49  49  0.891304  0.527037  0.188750\n",
      "     i       acc        dp        eo\n",
      "0    0  0.885870  0.664384  0.927083\n",
      "1    1  0.869565  0.671233  0.916667\n",
      "2    2  0.875000  0.664384  0.916667\n",
      "3    3  0.885870  0.664384  0.927083\n",
      "4    4  0.880435  0.671233  0.927083\n",
      "5    5  0.880435  0.657534  0.916667\n",
      "6    6  0.869565  0.671233  0.916667\n",
      "7    7  0.880435  0.657534  0.916667\n",
      "8    8  0.875000  0.664384  0.916667\n",
      "9    9  0.885870  0.664384  0.927083\n",
      "10  10  0.875000  0.664384  0.916667\n",
      "11  11  0.875000  0.664384  0.916667\n",
      "12  12  0.880435  0.657534  0.916667\n",
      "13  13  0.880435  0.474405  0.126250\n",
      "14  14  0.869565  0.441240  0.095000\n",
      "15  15  0.891304  0.493872  0.137500\n",
      "16  16  0.885870  0.434391  0.093750\n",
      "17  17  0.864130  0.414924  0.083333\n",
      "18  18  0.875000  0.500721  0.177500\n",
      "19  19  0.875000  0.414924  0.093750\n",
      "20  20  0.896739  0.414924  0.062500\n",
      "21  21  0.880435  0.447008  0.086250\n",
      "22  22  0.875000  0.434391  0.083333\n",
      "23  23  0.869565  0.441240  0.095000\n",
      "24  24  0.869565  0.408075  0.083333\n",
      "25  25  0.880435  0.408075  0.083333\n",
      "26  26  0.875000  0.381759  0.083333\n",
      "27  27  0.869565  0.408075  0.083333\n",
      "28  28  0.858696  0.408075  0.072917\n",
      "29  29  0.875000  0.381759  0.093750\n",
      "30  30  0.880435  0.388609  0.072917\n",
      "31  31  0.880435  0.441240  0.093750\n",
      "32  32  0.875000  0.453857  0.106250\n",
      "33  33  0.875000  0.414924  0.093750\n",
      "34  34  0.891304  0.493872  0.137500\n",
      "35  35  0.875000  0.448089  0.095000\n",
      "36  36  0.891304  0.527037  0.188750\n",
      "37  37  0.891304  0.493872  0.137500\n",
      "38  38  0.717391  0.273973  0.700000\n",
      "39  39  0.711957  0.294521  0.720000\n",
      "40  40  0.717391  0.301370  0.740000\n",
      "41  41  0.728261  0.287671  0.740000\n",
      "42  42  0.717391  0.287671  0.720000\n",
      "43  43  0.728261  0.301370  0.760000\n",
      "44  44  0.711957  0.294521  0.720000\n",
      "45  45  0.717391  0.301370  0.740000\n",
      "46  46  0.717391  0.301370  0.740000\n",
      "47  47  0.717391  0.287671  0.720000\n",
      "48  48  0.728261  0.315068  0.780000\n",
      "49  49  0.733696  0.294521  0.760000\n"
     ]
    }
   ],
   "source": [
    "# Inspect how many distinct models GridSearch actually produced\n",
    "len(gs_eo_rf.predictors_), len(gs_dp_rf.predictors_)\n",
    "\n",
    "# See the spread across the frontier (test metrics for each predictor)\n",
    "def eval_frontier(gs, X, y, A):\n",
    "    rows=[]\n",
    "    for i, clf in enumerate(gs.predictors_):\n",
    "        yhat = clf.predict(X)\n",
    "        m = eval_fairness(y, yhat, A)\n",
    "        rows.append({\"i\": i, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(eval_frontier(gs_eo_rf, X_test_ready, y_test, A_test))\n",
    "print(eval_frontier(gs_dp_rf, X_test_ready, y_test, A_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee0bbb",
   "metadata": {},
   "source": [
    "### RF GridSearch frontiers — concise read (CVD gender bias)\n",
    "\n",
    "**What the tables show:** Each index `i` corresponds to one Random Forest model on the fairness–accuracy frontier. The results span a wide range, with some **degenerate points** and a few **strong candidates**.\n",
    "\n",
    "---\n",
    "\n",
    "**Avoid (dominated / degenerate points):**  \n",
    "- Many candidates (`i ∈ {0–7, 9–15, 16, 18}` in the first table and `i ∈ {0–12, 38–49}` in the second) show **very large DP (≈0.65–0.76)** and **very high EO (≈0.90+)** despite only **moderate accuracy (≤0.88)**.  \n",
    "- These are **clinically unacceptable** and not worth considering.\n",
    "\n",
    "---\n",
    "\n",
    "**Good candidates (vs. baseline RF: Acc 0.8804, DP 0.4081, EO 0.0833):**\n",
    "\n",
    "- **Best overall balanced improvement:**  \n",
    "  - `i=30` → **Acc 0.8804**, **DP 0.3886**, **EO 0.0729**.  \n",
    "  - *Both fairness metrics improve vs. baseline at identical accuracy.*  \n",
    "\n",
    "- **Best EO & highest accuracy:**  \n",
    "  - `i=20` → **Acc 0.8967**, **DP 0.4149**, **EO 0.0625**.  \n",
    "  - *Stronger error-rate parity and higher accuracy, with only a small DP increase.*  \n",
    "\n",
    "- **Lowest DP with small accuracy cost:**  \n",
    "  - `i=26` or `i=29` → **Acc ≈0.8750**, **DP 0.3818**, **EO ≈0.0833**.  \n",
    "  - *Lower outcome disparity (DP) than baseline; EO ≈ baseline; accuracy slightly reduced.*  \n",
    "\n",
    "---\n",
    "\n",
    "**CVD context takeaway:**  \n",
    "- To **minimize sex-based error-rate disparity** → pick **`i=20`** (lowest EO, highest accuracy).  \n",
    "- To **improve both fairness metrics simultaneously without losing accuracy** → pick **`i=30`**.  \n",
    "- To **reduce outcome disparity (DP)** specifically → **`i=26/29`** are best, trading off a small accuracy drop.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5477483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RF + GS (EO): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (EO)] i=30\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    1.000000  0.125  1.000000       0.263158  0.894737\n",
      "1    0.916667  0.200  0.916667       0.671233  0.876712\n",
      "Accuracy: 0.8804 | DP diff: 0.4081 | EO diff: 0.0833\n",
      "\n",
      "[RF + GS (EO)] i=20\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.09375  0.833333       0.210526  0.894737\n",
      "1    0.927083  0.20000  0.927083       0.678082  0.883562\n",
      "Accuracy: 0.8859 | DP diff: 0.4676 | EO diff: 0.1063\n",
      "\n",
      "[RF + GS (EO)] i=26\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.833333  0.125  0.833333       0.236842  0.868421\n",
      "1    1.000000  1.000  1.000000       1.000000  0.657534\n",
      "Accuracy: 0.7011 | DP diff: 0.7632 | EO diff: 0.8750\n",
      "\n",
      "--- Summary (RF + GS (EO)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "1  20    0.8859   0.4676   0.1062\n",
      "2  26    0.7011   0.7632   0.8750\n",
      "0  30    0.8804   0.4081   0.0833\n",
      "\n",
      "=== RF + GS (DP): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (DP)] i=30\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    1.000000  0.15625  1.000000       0.289474  0.868421\n",
      "1    0.927083  0.20000  0.927083       0.678082  0.883562\n",
      "Accuracy: 0.8804 | DP diff: 0.3886 | EO diff: 0.0729\n",
      "\n",
      "[RF + GS (DP)] i=20\n",
      "        TPR    FPR  Recall  SelectionRate  Accuracy\n",
      "Sex                                                \n",
      "0    1.0000  0.125  1.0000       0.263158  0.894737\n",
      "1    0.9375  0.180  0.9375       0.678082  0.897260\n",
      "Accuracy: 0.8967 | DP diff: 0.4149 | EO diff: 0.0625\n",
      "\n",
      "[RF + GS (DP)] i=26\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    1.000000  0.15625  1.000000       0.289474  0.868421\n",
      "1    0.916667  0.20000  0.916667       0.671233  0.876712\n",
      "Accuracy: 0.8750 | DP diff: 0.3818 | EO diff: 0.0833\n",
      "\n",
      "--- Summary (RF + GS (DP)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "1  20    0.8967   0.4149   0.0625\n",
      "2  26    0.8750   0.3818   0.0833\n",
      "0  30    0.8804   0.3886   0.0729\n"
     ]
    }
   ],
   "source": [
    "# Show results for the specific frontier models \n",
    "# for both RF GridSearch runs (EO- and DP-constrained).\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "indices = [30,20,26]\n",
    "\n",
    "def eval_selected(gs, label):\n",
    "    rows = []\n",
    "    n = len(gs.predictors_)\n",
    "    print(f\"\\n=== {label}: {n} frontier candidates ===\")\n",
    "    for i in indices:\n",
    "        if i >= n:\n",
    "            print(f\"[{label}] Skipping i={i} (only {n} candidates).\")\n",
    "            continue\n",
    "        clf = gs.predictors_[i]\n",
    "        y_hat = clf.predict(X_test_ready)\n",
    "        m = eval_fairness(y_test, y_hat, A_test)\n",
    "        rows.append({\"i\": i, \"accuracy\": m[\"acc\"], \"dp_diff\": m[\"dp\"], \"eo_diff\": m[\"eo\"]})\n",
    "\n",
    "        # Per-group breakdown for this model\n",
    "        print(f\"\\n[{label}] i={i}\")\n",
    "        print(m[\"by_group\"])\n",
    "        print(f\"Accuracy: {m['acc']:.4f} | DP diff: {m['dp']:.4f} | EO diff: {m['eo']:.4f}\")\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows).sort_values(\"i\").round(4)\n",
    "        print(f\"\\n--- Summary ({label}) ---\")\n",
    "        print(df)\n",
    "\n",
    "# Evaluate selected indices for both EO and DP GridSearch objects\n",
    "eval_selected(gs_eo_rf, \"RF + GS (EO)\")\n",
    "eval_selected(gs_dp_rf, \"RF + GS (DP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb98b6d",
   "metadata": {},
   "source": [
    "## Random Forest — In-Processing: EG vs. GridSearch\n",
    "\n",
    "### Summary of results\n",
    "\n",
    "| Method        | i   | Accuracy | DP diff | EO diff | Interpretation |\n",
    "|---------------|-----|:--------:|:-------:|:-------:|----------------|\n",
    "| **Baseline**  | –   | 0.8804   | 0.4081  | 0.0833  | High accuracy; **large DP gap (~0.41)**; EO moderate (~0.08). |\n",
    "| **EG (EO)**   | –   | 0.8804   | 0.4081  | 0.0833  | **No change** vs. baseline — EO constraint ineffective. |\n",
    "| **EG (DP)**   | –   | 0.8804   | 0.4081  | 0.0833  | **No change** vs. baseline — DP constraint ineffective. |\n",
    "| **GS (EO)**   | 30  | 0.8804   | 0.4081  | 0.0833  | Baseline-equivalent; no gain. |\n",
    "| **GS (EO)**   | 20  | 0.8859   | 0.4676  | 0.1063  | Slight accuracy ↑; fairness worsens (DP & EO ↑). |\n",
    "| **GS (EO)**   | 26  | 0.7011   | 0.7632  | 0.8750  | **Degenerate**: poor accuracy & fairness. |\n",
    "| **GS (DP)**   | 30  | 0.8804   | 0.3886  | 0.0729  | **Strict improvement**: better DP & EO at baseline accuracy. |\n",
    "| **GS (DP)**   | 20  | 0.8967   | 0.4149  | 0.0625  | **Best accuracy** with EO improvement; DP slightly worse. |\n",
    "| **GS (DP)**   | 26  | 0.8750   | 0.3818  | 0.0833  | **Lowest DP**, EO ≈ baseline; small accuracy cost. |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "- **Exponentiated Gradient (EO & DP):** Ineffective — RF predictions are unchanged.  \n",
    "- **GridSearch (EO):** Mostly unhelpful; either baseline-equivalent, worse fairness, or degenerate.  \n",
    "- **GridSearch (DP):** Provides **real trade-offs**:  \n",
    "  - **i=30:** best balanced improvement (Acc same, DP ↓, EO ↓).  \n",
    "  - **i=20:** highest accuracy + EO gain, but DP worsens.  \n",
    "  - **i=26:** lowest DP, small accuracy loss, EO ≈ baseline.  \n",
    "\n",
    "**Takeaway:** For RF, **EG fails**, but **GridSearch with DP constraint** yields useful frontier candidates depending on whether the goal is **overall balance (i=30)**, **accuracy + EO parity (i=20)**, or **minimizing DP (i=26)**.\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001876d",
   "metadata": {},
   "source": [
    "### Bias Mitigation RF: Post-processing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8238f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    1.000000  0.125  1.000000       0.263158  0.894737\n",
      "1    0.916667  0.200  0.916667       0.671233  0.876712\n",
      "Accuracy: 0.8804 | DP diff: 0.4081 | EO diff: 0.0833\n",
      "\n",
      "=== RF + Post-processing (Equalized Odds) ===\n",
      "         TPR    FPR   Recall  SelectionRate  Accuracy\n",
      "Sex                                                  \n",
      "0    1.00000  0.125  1.00000       0.263158  0.894737\n",
      "1    0.90625  0.200  0.90625       0.664384  0.869863\n",
      "Accuracy: 0.8750 | DP diff: 0.4012 | EO diff: 0.0938\n",
      "\n",
      "=== RF + Post-processing (Demographic Parity) ===\n",
      "         TPR    FPR   Recall  SelectionRate  Accuracy\n",
      "Sex                                                  \n",
      "0    1.00000  0.125  1.00000       0.263158  0.894737\n",
      "1    0.90625  0.200  0.90625       0.664384  0.869863\n",
      "Accuracy: 0.8750 | DP diff: 0.4012 | EO diff: 0.0938\n",
      "\n",
      "=== Random Forest: Baseline vs Post-processing ===\n",
      "            model  accuracy  dp_diff  eo_diff\n",
      "0     RF Baseline    0.8804   0.4081   0.0833\n",
      "1  RF + Post (EO)    0.8750   0.4012   0.0938\n",
      "2  RF + Post (DP)    0.8750   0.4012   0.0938\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# 0) Baseline RF \n",
    "rf.fit(X_train_ready, y_train)\n",
    "y_rf_base = rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds \n",
    "post_rf_eo = ThresholdOptimizer(\n",
    "    estimator=rf,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_eo = post_rf_eo.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_rf_eo = eval_fairness(y_test, y_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity \n",
    "post_rf_dp = ThresholdOptimizer(\n",
    "    estimator=rf,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_dp = post_rf_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_rf_dp = eval_fairness(y_test, y_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_rf_post = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",       \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (EO)\",    \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (DP)\",    \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs Post-processing ===\")\n",
    "print(summary_rf_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde82b4a",
   "metadata": {},
   "source": [
    "# Random Forest Bias Mitigation (Post-processing)\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Model              | Accuracy | DP Diff | EO Diff | Interpretation                                   |\n",
    "|--------------------|:--------:|:-------:|:-------:|--------------------------------------------------|\n",
    "| **RF Baseline**    | 0.8804   | 0.4081  | 0.0833  | High accuracy; **large DP disparity (~0.41)**; EO moderate. |\n",
    "| **RF + Post (EO)** | 0.8750   | 0.4012  | 0.0938  | Accuracy ↓ slightly; DP gap ≈ baseline; **EO worsens**. |\n",
    "| **RF + Post (DP)** | 0.8750   | 0.4012  | 0.0938  | Same outcome as Post (EO) → **no meaningful fairness gain**. |\n",
    "\n",
    "## Key Facts\n",
    "- The **RF baseline** already delivers high accuracy but exhibits a **substantial DP disparity** (≈0.41).  \n",
    "- **ThresholdOptimizer** (both EO and DP) converged to **nearly identical solutions**, leaving DP almost unchanged and EO slightly worse.  \n",
    "- Accuracy dropped marginally (−0.5 pp), making these post-processing methods ineffective in this setup.  \n",
    "- Compared to DT, the **RF model is less responsive to post-processing interventions**, confirming that ensemble stability can limit fairness adjustments.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2c505",
   "metadata": {},
   "source": [
    "### Deep Learning - Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b0d5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required library \n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4cbac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== (Adam + EarlyStopping) Evaluation ===\n",
      "Accuracy : 0.8586956521739131\n",
      "Precision: 0.8877551020408163\n",
      "Recall   : 0.8529411764705882\n",
      "F1 Score : 0.87\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85        82\n",
      "           1       0.89      0.85      0.87       102\n",
      "\n",
      "    accuracy                           0.86       184\n",
      "   macro avg       0.86      0.86      0.86       184\n",
      "weighted avg       0.86      0.86      0.86       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[71 11]\n",
      " [15 87]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adam + Early Stopping \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "adammlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),   # slightly smaller/deeper can help\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=1e-3,       # smaller step can stabilize\n",
    "    alpha=1e-3,                    # L2 regularization to reduce overfitting\n",
    "    batch_size=32,\n",
    "    max_iter=1000,                 # increased max_iter\n",
    "    early_stopping=True,           # use a validation split internally\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=25,          \n",
    "    tol=1e-4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "adammlp.fit(X_train_ready, y_train)  \n",
    "y_pred_mlp = adammlp.predict(X_test_ready)                     \n",
    "y_prob_mlp = adammlp.predict_proba(X_test_ready)[:, 1]         \n",
    "\n",
    "evaluate_model(y_test, y_pred_mlp, \"(Adam + EarlyStopping)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775454c",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "857cf027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "         TPR    FPR   Recall  SelectionRate  Accuracy\n",
      "Sex                                                  \n",
      "0    1.00000  0.125  1.00000       0.263158  0.894737\n",
      "1    0.84375  0.140  0.84375       0.602740  0.849315\n",
      "Accuracy: 0.8587 | DP diff: 0.3396 | EO diff: 0.1562\n",
      "\n",
      "=== In-processing MLP: EG (Equalized Odds) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    1.000000  0.125  1.000000       0.263158  0.894737\n",
      "1    0.833333  0.160  0.833333       0.602740  0.835616\n",
      "Accuracy: 0.8478 | DP diff: 0.3396 | EO diff: 0.1667\n",
      "\n",
      "=== In-processing MLP: EG (Demographic Parity) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    1.000000  0.125  1.000000       0.263158  0.894737\n",
      "1    0.833333  0.160  0.833333       0.602740  0.835616\n",
      "Accuracy: 0.8478 | DP diff: 0.3396 | EO diff: 0.1667\n",
      "\n",
      "=== MLP: Baseline vs In-processing (EG) ===\n",
      "           model  accuracy  dp_diff  eo_diff\n",
      "0   MLP Baseline    0.8587   0.3396   0.1562\n",
      "1  MLP + EG (EO)    0.8478   0.3396   0.1667\n",
      "2  MLP + EG (DP)    0.8478   0.3396   0.1667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Baseline MLP (seeded for reproducibility)\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=1e-3,\n",
    "    alpha=1e-3,\n",
    "    batch_size=32,\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=25,\n",
    "    tol=1e-4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_mlp_base = mlp.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_pred_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) EG with Equalized Odds\n",
    "eg_eo_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(mlp),   # inherits random_state=42\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "# Prefer predict(..., random_state=42) if supported; otherwise fall back without global seeds\n",
    "try:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_eo = eval_fairness(y_test, y_pred_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP: EG (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity\n",
    "eg_dp_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(mlp),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_dp = eval_fairness(y_test, y_pred_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP: EG (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",  \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (EO)\", \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (DP)\", \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943e26",
   "metadata": {},
   "source": [
    "# MLP In-Processing Bias Mitigation Results\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Model             | Accuracy | DP Diff | EO Diff | Interpretation                                                        |\n",
    "|-------------------|:--------:|:-------:|:-------:|-----------------------------------------------------------------------|\n",
    "| **MLP Baseline**  | 0.8587   | 0.3396  | 0.1562  | Decent accuracy but **substantial DP disparity** (~0.34) and **EO gap** (~0.16). |\n",
    "| **MLP + EG (EO)** | 0.8478   | 0.3396  | 0.1667  | Accuracy ↓; **EO worsens**; DP unchanged.                             |\n",
    "| **MLP + EG (DP)** | 0.8478   | 0.3396  | 0.1667  | Same as EG(EO): accuracy ↓; **EO worsens**; DP unchanged.             |\n",
    "\n",
    "## Key Points\n",
    "- **Exponentiated Gradient (EG)** failed to improve fairness:  \n",
    "  - **DP disparity** stayed high (~0.34).  \n",
    "  - **EO disparity** worsened (0.156 → 0.167).  \n",
    "  - **Accuracy dropped** slightly (0.859 → 0.848).  \n",
    "- Identical outcomes for EG(EO) and EG(DP) suggest the algorithm converged to the **same frontier solution**, showing **low sensitivity** of MLP to these fairness constraints.  \n",
    "- In the **CVD gender bias** context, this means disparities remain unresolved and mitigation attempts were ineffective.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11de87",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1b9ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing MLP: GridSearch (Equalized Odds) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    1.000000  0.09375  1.000000       0.236842  0.921053\n",
      "1    0.833333  0.18000  0.833333       0.609589  0.828767\n",
      "Accuracy: 0.8478 | DP diff: 0.3727 | EO diff: 0.1667\n",
      "\n",
      "=== In-processing MLP: GridSearch (Demographic Parity) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    1.000000  0.125  1.000000       0.263158  0.894737\n",
      "1    0.822917  0.160  0.822917       0.595890  0.828767\n",
      "Accuracy: 0.8424 | DP diff: 0.3327 | EO diff: 0.1771\n",
      "\n",
      "=== MLP: Baseline vs EG vs GS ===\n",
      "           model  accuracy  dp_diff  eo_diff\n",
      "0   MLP Baseline    0.8587   0.3396   0.1562\n",
      "1  MLP + EG (EO)    0.8478   0.3396   0.1667\n",
      "2  MLP + EG (DP)    0.8478   0.3396   0.1667\n",
      "3  MLP + GS (EO)    0.8478   0.3727   0.1667\n",
      "4  MLP + GS (DP)    0.8424   0.3327   0.1771\n",
      "5  MLP + GS (EO)    0.8478   0.3727   0.1667\n",
      "6  MLP + GS (DP)    0.8424   0.3327   0.1771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds (MLP)\n",
    "gs_eo_mlp = GridSearch(\n",
    "    estimator=clone(adammlp),                 \n",
    "    constraints=EqualizedOdds(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,                   \n",
    "    grid_size=15                             \n",
    ")\n",
    "gs_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "# Some Fairlearn versions support random_state in predict; fall back if not\n",
    "try:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_eo_mlp = eval_fairness(y_test, y_pred_gs_eo_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo_mlp['acc']:.4f} | DP diff: {m_gs_eo_mlp['dp']:.4f} | EO diff: {m_gs_eo_mlp['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity (MLP)\n",
    "gs_dp_mlp = GridSearch(\n",
    "    estimator=clone(adammlp),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15\n",
    ")\n",
    "gs_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_dp_mlp = eval_fairness(y_test, y_pred_gs_dp_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp_mlp['acc']:.4f} | DP diff: {m_gs_dp_mlp['dp']:.4f} | EO diff: {m_gs_dp_mlp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with existing MLP runs (baseline + EG)\n",
    "summary_mlp = pd.concat([\n",
    "    summary_mlp,\n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"MLP + GS (EO)\", \"accuracy\":m_gs_eo_mlp[\"acc\"], \"dp_diff\":m_gs_eo_mlp[\"dp\"], \"eo_diff\":m_gs_eo_mlp[\"eo\"]},\n",
    "        {\"model\":\"MLP + GS (DP)\", \"accuracy\":m_gs_dp_mlp[\"acc\"], \"dp_diff\":m_gs_dp_mlp[\"dp\"], \"eo_diff\":m_gs_dp_mlp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs EG vs GS ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4cf0b",
   "metadata": {},
   "source": [
    "### MLP — In-Processing (GridSearch vs EG)\n",
    "\n",
    "### Comparative Results\n",
    "\n",
    "| Model              | Accuracy | DP Diff | EO Diff | Notes |\n",
    "|--------------------|:--------:|:-------:|:-------:|-------|\n",
    "| **MLP Baseline**   | 0.8587   | 0.3396  | 0.1562  | Reference: good accuracy but clear DP & EO disparities |\n",
    "| **EG (EO)**        | 0.8478   | 0.3396  | 0.1667  | Accuracy ↓; DP unchanged; EO worsens |\n",
    "| **EG (DP)**        | 0.8478   | 0.3396  | 0.1667  | Same as EG(EO): no fairness gain |\n",
    "| **GS (EO)**        | 0.8478   | 0.3727  | 0.1667  | Accuracy ↓; DP ↑; EO worsens (driven by TPR/FPR gaps) |\n",
    "| **GS (DP)**        | 0.8424   | 0.3327  | 0.1771  | Accuracy ↓; tiny DP improvement; EO worsens |\n",
    "\n",
    "### Interpretation\n",
    "- **Baseline (MLP):** Accuracy **0.8587** with **moderate DP (0.34)** and **EO (0.16)** disparities.  \n",
    "- **EG methods:** Both EO and DP constraints produced **identical models** — no DP improvement, EO worsened slightly, and accuracy dropped.  \n",
    "- **GS methods:**  \n",
    "  - **GS (EO)** worsened both disparities (DP ↑, EO ↑) while lowering accuracy.  \n",
    "  - **GS (DP)** gave a **tiny DP reduction**, but EO worsened and accuracy dropped more.  \n",
    "\n",
    "### Takeaway\n",
    "Neither **Exponentiated Gradient** nor **GridSearch** improved fairness for MLP.  \n",
    "- **EO consistently worsened** in all constrained models.  \n",
    "- **DP improved only trivially** under GS(DP).  \n",
    "- All fairness constraints **reduced accuracy**, making the **baseline MLP the best option** in this setup.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d29d4a",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Postprocessing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4591c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "         TPR    FPR   Recall  SelectionRate  Accuracy\n",
      "Sex                                                  \n",
      "0    1.00000  0.125  1.00000       0.263158  0.894737\n",
      "1    0.84375  0.140  0.84375       0.602740  0.849315\n",
      "Accuracy: 0.8587 | DP diff: 0.3396 | EO diff: 0.1562\n",
      "\n",
      "=== MLP + Post-processing (Equalized Odds) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    1.000000  0.15625  1.000000       0.289474  0.868421\n",
      "1    0.947917  0.30000  0.947917       0.726027  0.863014\n",
      "Accuracy: 0.8641 | DP diff: 0.4366 | EO diff: 0.1437\n",
      "\n",
      "=== MLP + Post-processing (Demographic Parity) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.833333  0.125  0.833333       0.236842  0.868421\n",
      "1    0.843750  0.140  0.843750       0.602740  0.849315\n",
      "Accuracy: 0.8533 | DP diff: 0.3659 | EO diff: 0.0150\n",
      "\n",
      "=== MLP: Baseline vs Post-processing ===\n",
      "             model  accuracy  dp_diff  eo_diff\n",
      "0     MLP Baseline    0.8587   0.3396   0.1562\n",
      "1  MLP + Post (EO)    0.8641   0.4366   0.1438\n",
      "2  MLP + Post (DP)    0.8533   0.3659   0.0150\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Baseline MLP\n",
    "adammlp.fit(X_train_ready, y_train)\n",
    "y_mlp_base = adammlp.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds\n",
    "post_mlp_eo = ThresholdOptimizer(\n",
    "    estimator=adammlp,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_eo = post_mlp_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_eo = eval_fairness(y_test, y_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity\n",
    "post_mlp_dp = ThresholdOptimizer(\n",
    "    estimator=adammlp,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_dp = post_mlp_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_dp = eval_fairness(y_test, y_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp_post = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",       \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (EO)\",    \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (DP)\",    \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs Post-processing ===\")\n",
    "print(summary_mlp_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d9e15",
   "metadata": {},
   "source": [
    "### MLP — Post-Processing: Threshold Optimizer \n",
    "\n",
    "#### Summary\n",
    "\n",
    "| Model             | Accuracy | DP diff | EO diff | Notes                                                                 |\n",
    "|-------------------|:--------:|:-------:|:-------:|------------------------------------------------------------------------|\n",
    "| **Baseline**      | 0.8587   | 0.3396  | 0.1562  | Solid accuracy; large outcome disparity (DP ≈ 0.34), moderate EO gap.  |\n",
    "| **Post (EO)**     | 0.8641   | 0.4366  | 0.1437  | Accuracy ↑ (+0.0054); **EO improves slightly** (−0.0125); **DP worsens sharply** (+0.097). |\n",
    "| **Post (DP)**     | 0.8533   | 0.3659  | 0.0150  | Accuracy ↓ (−0.0054); **EO improves strongly** (−0.1412); **DP worsens slightly** (+0.0263). |\n",
    "\n",
    "#### Interpretation\n",
    "- **Equalized Odds (Post EO):**  \n",
    "  - Brings a **tiny EO gain** (0.156 → 0.144), improves accuracy,  \n",
    "  - But **DP disparity rises substantially** (0.340 → 0.437), meaning sex groups get flagged at very different rates.  \n",
    "\n",
    "- **Demographic Parity (Post DP):**  \n",
    "  - Produces a **dramatic EO improvement** (0.156 → 0.015), nearly equalizing error rates,  \n",
    "  - But comes with a small accuracy drop and a **slight DP increase**.  \n",
    "\n",
    "**Takeaway:**  \n",
    "- If the priority is **balanced error rates** (minimizing sex-based gaps in TPR/FPR), **Post (DP)** is preferable despite modest DP worsening.  \n",
    "- If **accuracy** is prioritized and a slight EO gain is enough, **Post (EO)** is the better choice, though it increases outcome disparities.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d93504",
   "metadata": {},
   "source": [
    "## Overall Comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688fc3af",
   "metadata": {},
   "source": [
    "# Gender Bias Mitigation in CVD Prediction — Overall Interpretation\n",
    "\n",
    "**Fairness metrics:**  \n",
    "- **DP diff** (Demographic Parity): selection-rate gap across sexes (lower = more equal triage/alerts).  \n",
    "- **EO diff** (Equalized Odds): error-rate gap (combined TPR/FPR gap) across sexes (lower = more equal misses/false alarms).\n",
    "\n",
    "---\n",
    "\n",
    "## One-glance summary (best observed per family)\n",
    "\n",
    "| Model family | Best config in your runs | Accuracy | DP diff | EO diff | Why it’s “best” |\n",
    "|---|---:|---:|---:|---:|---|\n",
    "| **PCA+KNN** | *(none effective)* | — | — | — | Post-processing made **0% flips**; CR slightly ↓DP but ↑EO and ↓acc. |\n",
    "| **Decision Tree** | **EG (DP)** | 0.8098 | 0.2880 | **0.0300** | **Strongest EO reduction** with **no accuracy loss**; DP still high. |\n",
    "|  | **Post (DP)** | 0.8152 | **0.2422** | 0.1125 | **Best DP** for DT with slight **acc ↑**; EO worsens. |\n",
    "| **Random Forest** | **GS (DP), i=30** | 0.8804 | **0.3886** | **0.0729** | **Balanced improvement** vs baseline: same acc, **DP↓**, **EO↓**. |\n",
    "|  | GS (DP), i=20 | **0.8967** | 0.4149 | **0.0625** | **Highest acc** + **EO↓**; DP slightly worse than baseline. |\n",
    "|  | GS (DP), i=26 | 0.8750 | **0.3818** | 0.0833 | **Lowest DP** among RF with small acc cost; EO ≈ baseline. |\n",
    "| **MLP** | **Post (DP)** | 0.8533 | 0.3659 | **0.0150** | **Best EO overall** (near-equal error rates); DP ↑ and acc ↓ slightly. |\n",
    "|  | *(EG / GS)* | — | — | — | EG and GS **did not help**; some runs worsened fairness and/or accuracy. |\n",
    "\n",
    "> Baselines to remember: **DT** (acc 0.8098, DP 0.2549, EO 0.0812) • **RF** (acc 0.8804, DP 0.4081, EO 0.0833) • **MLP** (acc 0.8587, DP 0.3396, EO 0.1562) • **PCA+KNN** (acc 0.8859, DP 0.3796, EO 0.0521)\n",
    "\n",
    "---\n",
    "\n",
    "## What worked \n",
    "\n",
    "### When the clinical priority is **error-rate parity (EO)**  \n",
    "\n",
    "- **Decision Tree + EG (DP)**: EO **0.0812 → 0.0300** with **no acc loss**; DP rises to 0.2880.  \n",
    "- **MLP + Post (DP)**: EO **0.1562 → 0.0150** (best EO overall) with **small acc drop** and **DP ↑**.  \n",
    "- **Random Forest + GS (DP, i=20)**: EO **0.0833 → 0.0625** with **highest acc** (0.8967), DP slightly worse.\n",
    "\n",
    "**Pick:**  \n",
    "- Want **strongest EO** and can tolerate some DP/accuracy trade-off → **MLP + Post(DP)**.  \n",
    "- Want **EO ↓ with no acc loss** in a simple model → **DT + EG(DP)**.  \n",
    "- Want **EO ↓ with best acc** in an ensemble → **RF + GS(DP, i=20)**.\n",
    "\n",
    "---\n",
    "\n",
    "### When the priority is **outcome parity (DP)**  \n",
    "\n",
    "- **Decision Tree + Post (DP)**: DP **0.2549 → 0.2422**, **acc ↑** slightly; EO worsens.  \n",
    "- **Random Forest + GS (DP, i=26)**: DP **0.4081 → 0.3818** (lowest DP for RF) with small acc cost; EO ≈ baseline.  \n",
    "- **Random Forest + GS (DP, i=30)**: **Joint DP↓ & EO↓** at **same acc** — best *balanced* RF improvement.\n",
    "\n",
    "**Pick:**  \n",
    "- Need **best DP** on DT with minimal disruption → **DT + Post(DP)**.  \n",
    "- Need **RF with DP focus** → **GS(DP, i=26)** (lowest DP) or **i=30** for **joint DP↓ & EO↓** without acc loss.\n",
    "\n",
    "---\n",
    "\n",
    "### Approaches that were **ineffective or harmful** in this study\n",
    "\n",
    "- **PCA+KNN Post-processing (DP/EO):** **0% label flips** → no effect. **CR**: ↓acc, ↓DP, but **↑EO**.  \n",
    "- **Random Forest EG (EO/DP):** **No change** from baseline.  \n",
    "- **Random Forest Post (EO/DP):** **Acc ↓** and **EO worsened**; DP ≈ baseline.  \n",
    "- **MLP EG / GS:** Generally **↓acc** and **EO ↑**; only **Post(DP)** helped EO (with DP/acc trade-offs).  \n",
    "- **GridSearch (EO)** for DT and RF: repeatedly **baseline-equivalent** or **worse**; some EO runs **degenerate**.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical guidance for CVD deployment\n",
    "\n",
    "1. **First decide the fairness target**  \n",
    "   - **Clinical safety (minimize sex-based error-rate gaps)** → **EO target** (e.g., EO ≤ 0.05).  \n",
    "   - **Access equity (equalize intervention rates)** → **DP target** (e.g., DP ≤ 0.10–0.15 given your baselines).\n",
    "\n",
    "2. **Pick the model/mitigation to match that target**  \n",
    "   - **EO-driven:**  \n",
    "     - **DT + EG(DP)** (stable acc, EO ~0.03), or  \n",
    "     - **RF + GS(DP, i=20)** (best acc, EO ~0.063), or  \n",
    "     - **MLP + Post(DP)** (EO ~0.015, accept DP↑ and small acc↓).  \n",
    "   - **DP-driven:**  \n",
    "     - **DT + Post(DP)** (lowest DP for DT; EO trade-off),  \n",
    "     - **RF + GS(DP, i=26)** (lowest DP in RF), or **i=30** for **joint DP↓ & EO↓** without acc loss.\n",
    "\n",
    "3. **Lock selection rules before finalizing**  \n",
    "   - Use a **held-out selection set** with pre-declared thresholds (e.g., **EO ≤ 0.05 & DP ≤ 0.30 & Acc ≥ baseline − 0.5 pp**).  \n",
    "   - Prefer **frontier points** that meet all thresholds rather than optimizing a single metric in isolation.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Summary:\n",
    "\n",
    "- **There is no one-size-fits-all fix**: each model category offers a different **fairness–utility** profile.  \n",
    "- For **EO parity** (clinical risk symmetry), your strongest choices are **DT + EG(DP)** (no acc loss) and **RF + GS(DP, i=20)** (best acc), with **MLP + Post(DP)** delivering the **lowest EO** if you accept a DP/accuracy trade-off.  \n",
    "- For **DP parity** (equal access), **DT + Post(DP)** and **RF + GS(DP, i=26/30)** are the most appropriate.  \n",
    "- **Avoid** PCA+KNN post-processing (no effect), **RF EG** (no effect), and any configurations that **inflate EO** without clear benefit.  \n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
