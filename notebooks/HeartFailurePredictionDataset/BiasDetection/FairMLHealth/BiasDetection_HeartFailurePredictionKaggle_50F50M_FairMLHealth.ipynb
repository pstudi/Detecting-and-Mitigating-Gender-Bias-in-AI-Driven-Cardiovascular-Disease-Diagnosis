{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba0b2b9",
   "metadata": {},
   "source": [
    "### Bias Detection and Fairness Evaluation on Heart Failure Prediction Dataset (Kaggle) using FairMLhealth\n",
    "(Source: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c29a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load X_test set\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57b21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup successful\n"
     ]
    }
   ],
   "source": [
    "import fairmlhealth\n",
    "import aif360\n",
    "print(\"Environment setup successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f126c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fairmlhealth\n",
      "Version: 1.0.2\n",
      "Summary: Health-centered variation analysis\n",
      "Home-page: https://github.com/KenSciResearch/fairMLHealth\n",
      "Author: Christine Allen\n",
      "Author-email: ca.magallen@gmail.com\n",
      "License: \n",
      "Location: c:\\users\\patri\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: aif360, ipython, jupyter, numpy, pandas, requests, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#have a look at the details of fairmlhealth - especially the version\n",
    "!pip show fairmlhealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76186324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "#have a look at the modules that are within fairmlhealth\n",
    "\n",
    "print(dir(fairmlhealth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44657ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "#load necessary modules \n",
    "\n",
    "#import module measure to use measure.summary for bias detection\n",
    "from fairmlhealth import measure\n",
    "\n",
    "#import module for investigation of individual cohorts \n",
    "from fairmlhealth.__utils import iterate_cohorts\n",
    "\n",
    "#import FairRanges to flag high values\n",
    "from fairmlhealth.__utils import FairRanges\n",
    "\n",
    "# Wrap the fairness summary function for cohort-wise analysis\n",
    "@iterate_cohorts\n",
    "def cohort_summary(**kwargs):\n",
    "    return measure.summary(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10ccd4",
   "metadata": {},
   "source": [
    "During the execution of FairMLHealth and AIF360, several runtime warnings were raised (e.g., “AdversarialDebiasing will be unavailable” due to the absence of TensorFlow, and deprecation warnings from the inFairness package regarding PyTorch’s functorch.vmap). These warnings do not affect the fairness metrics or results presented in this study, as the unavailable components were not used. To maintain clarity of output, the warnings were silenced programmatically, and the analysis was conducted without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb6c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"inFairness\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"AdversarialDebiasing will be unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a099295",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models - KNN & DT\n",
    "\n",
    "#### K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d817251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true    y_prob  y_pred\n",
      "0    1       1  0.935632       1\n",
      "1    1       1  0.718133       1\n",
      "2    1       1  0.947130       1\n",
      "3    1       1  0.379473       0\n",
      "4    0       0  0.238673       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "knn_df = pd.read_csv(\"HeartFailureData_50_50_PCA_KNN_predictions.csv\")\n",
    "\n",
    "print(knn_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871aa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_knn = knn_df[\"y_true\"].values\n",
    "y_prob_knn = knn_df[\"y_prob\"].values\n",
    "y_pred_knn = knn_df[\"y_pred\"].values\n",
    "gender_knn = knn_df[\"Sex\"].values\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_knn = gender_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad78836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0350\n",
      "               Balanced Accuracy Difference           -0.0750\n",
      "               Balanced Accuracy Ratio                 0.9145\n",
      "               Disparate Impact Ratio                  0.2650\n",
      "               Equal Odds Difference                  -0.1875\n",
      "               Equal Odds Ratio                        0.6250\n",
      "               Positive Predictive Parity Difference  -0.2759\n",
      "               Positive Predictive Parity Ratio        0.7073\n",
      "               Statistical Parity Difference          -0.4380\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "knn_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_true_knn,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn,\n",
    "    prtc_attr=protected_attr_knn,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,   # skip inconsistency metrics that cause NearestNeighbors error\n",
    "    skip_performance=True\n",
    ")\n",
    "\n",
    "print(knn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ba3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Custom scenario oriented bounds\n",
    "\n",
    "custom_ranges = {\n",
    "    \"tpr diff\": (-0.03, 0.03),\n",
    "    \"fpr diff\": (-0.03, 0.03),\n",
    "    \"equal odds difference\": (-0.04, 0.04),\n",
    "    \"statistical parity difference\": (-0.05, 0.05),\n",
    "    \"disparate impact ratio\": (0.9, 1.1),\n",
    "    \"selection ratio\": (0.9, 1.1),\n",
    "    \"auc difference\": (-0.02, 0.02),\n",
    "    \"balanced accuracy difference\": (-0.02, 0.02),\n",
    "}\n",
    "\n",
    "bounds = FairRanges().load_fair_ranges(custom_ranges=custom_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14d2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for highlighting metrics outside of the thresholds\n",
    "#  restore Styler.set_precision to adjust the highlighting color in the styled table\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "Styler = type(pd.DataFrame({\"_\":[0]}).style)  \n",
    "\n",
    "if not hasattr(Styler, \"set_precision\"):\n",
    "    def _set_precision(self, precision=4):\n",
    "        try:\n",
    "            return self.format(precision=precision)\n",
    "        except TypeError:\n",
    "            return self.format(formatter=lambda x:\n",
    "                f\"{x:.{precision}g}\" if isinstance(x, (int, float, np.floating)) else x\n",
    "            )\n",
    "    setattr(Styler, \"set_precision\", _set_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90435472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_401e8_row0_col0, #T_401e8_row1_col0, #T_401e8_row3_col0, #T_401e8_row4_col0, #T_401e8_row5_col0, #T_401e8_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_401e8\">\n",
       "  <caption>KNN Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_401e8_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_401e8_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_401e8_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_401e8_row0_col0\" class=\"data row0 col0\" >0.0350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_401e8_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_401e8_row1_col0\" class=\"data row1 col0\" >-0.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_401e8_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_401e8_row2_col0\" class=\"data row2 col0\" >0.9145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_401e8_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_401e8_row3_col0\" class=\"data row3 col0\" >0.2650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_401e8_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_401e8_row4_col0\" class=\"data row4 col0\" >-0.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_401e8_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_401e8_row5_col0\" class=\"data row5 col0\" >0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_401e8_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_401e8_row6_col0\" class=\"data row6 col0\" >-0.2759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_401e8_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_401e8_row7_col0\" class=\"data row7 col0\" >0.7073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_401e8_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_401e8_row8_col0\" class=\"data row8 col0\" >-0.4380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_401e8_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_401e8_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_401e8_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x190b2069840>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "from fairmlhealth.__utils import Flagger\n",
    "\n",
    "class MyFlagger(Flagger):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.flag_color = \"#491ee6\"   \n",
    "        self.flag_type = \"background-color\"\n",
    "\n",
    "styled_knn = MyFlagger().apply_flag(\n",
    "    df=knn_bias,\n",
    "    caption=\"KNN Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d3278",
   "metadata": {},
   "source": [
    "## Fairness Analysis – KNN Model (by Gender)\n",
    "\n",
    "This table summarizes **fairness metrics** for the KNN model, comparing performance between the unprivileged group (female) and the privileged group (male).  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. AUC & Balanced Accuracy\n",
    "- **AUC Difference (0.0350):** Small disparity in ranking quality across genders, but males slightly benefit from better ranking performance.  \n",
    "- **Balanced Accuracy Difference (−0.0750), Ratio (0.9145):** Females experience noticeably **lower balanced accuracy**, showing reduced fairness in how the model handles positive and negative cases.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Group Fairness Metrics\n",
    "- **Disparate Impact Ratio (0.2650):** Well below the fairness threshold (0.80–1.25). This means the selection rate for females is **much lower**, signaling strong under-representation.  \n",
    "- **Equal Odds Difference (−0.1875), Ratio (0.6250):** Significant imbalance in error rates (TPR and FPR) across genders, disadvantaging females.  \n",
    "- **Statistical Parity Difference (−0.4380):** Negative value confirms females are **selected far less often** than males.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Predictive Parity\n",
    "- **Positive Predictive Parity Difference (−0.2759), Ratio (0.7073):** Females have **lower precision** than males, meaning predictions for females are less reliable.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "- The KNN model shows **systematic gender bias**, particularly disadvantaging the **female (unprivileged) group**.  \n",
    "- Females face:\n",
    "  - Lower balanced accuracy.  \n",
    "  - Much lower selection rates.  \n",
    "  - Weaker precision and reliability of predictions.  \n",
    "- Males (privileged group) benefit from consistently more favorable outcomes across fairness measures.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The fairness metrics reveal that KNN is **not gender-fair**. It disproportionately disadvantages females through **reduced selection, lower precision, and weaker balanced accuracy**, while males are treated more favorably. These disparities highlight a **serious fairness concern**, making bias mitigation necessary before practical use.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0eeae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - KNN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.0935</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>1.600</td>\n",
       "      <td>0.2759</td>\n",
       "      <td>1.4138</td>\n",
       "      <td>0.438</td>\n",
       "      <td>3.774</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>1.2812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>-0.0375</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.2759</td>\n",
       "      <td>0.7073</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.7805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                         0.075   \n",
       "1          Sex             1                        -0.075   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0935    0.0375      1.600    0.2759     1.4138   \n",
       "1                   0.9145   -0.0375      0.625   -0.2759     0.7073   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0           0.438            3.774    0.1875     1.2812  \n",
       "1          -0.438            0.265   -0.1875     0.7805  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - KNN\")\n",
    "measure.bias(X_test, y_test, y_pred_knn, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05764be",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – KNN by Gender\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Balanced Accuracy Difference = **+0.0750**, Ratio = **1.0935**  \n",
    "- **Males (1):** Balanced Accuracy Difference = **−0.0750**, Ratio = **0.9145**  \n",
    "- ➝ The model achieves **higher balanced accuracy for females**, suggesting better handling of true positives and true negatives for the unprivileged group.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** FPR Difference = **+0.0375**, Ratio = **1.6000**  \n",
    "- **Males (1):** FPR Difference = **−0.0375**, Ratio = **0.6250**  \n",
    "- ➝ Females face a **higher false positive rate**, meaning they are more often incorrectly flagged as positive compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** PPV Difference = **+0.2759**, Ratio = **1.4138**  \n",
    "- **Males (1):** PPV Difference = **−0.2759**, Ratio = **0.7073**  \n",
    "- ➝ Predictions for females are **more reliable (higher precision)**, while for males, predictions are less trustworthy.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Selection Difference = **+0.4380**, Ratio = **3.7740**  \n",
    "- **Males (1):** Selection Difference = **−0.4380**, Ratio = **0.2650**  \n",
    "- ➝ The model selects **females at a much higher rate** than males, indicating strong preferential treatment toward the unprivileged group.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** TPR Difference = **+0.1875**, Ratio = **1.2812**  \n",
    "- **Males (1):** TPR Difference = **−0.1875**, Ratio = **0.7805**  \n",
    "- ➝ Females benefit from a **higher sensitivity**, meaning they are more likely to be correctly identified when positive, while males face more missed detections.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- The KNN model demonstrates a **reverse bias** in this setup, **favoring females (unprivileged group)** over males (privileged group).  \n",
    "- Females experience **higher balanced accuracy, higher precision, greater selection rates, and stronger sensitivity**.  \n",
    "- However, this comes at the cost of a **higher false positive rate for females**, meaning they are more likely to be incorrectly flagged.  \n",
    "- Males, on the other hand, face **under-selection and reduced predictive reliability**, highlighting a clear imbalance.\n",
    "\n",
    "In short, the model’s fairness issue here is not traditional male privilege but rather a **systematic skew toward females**, creating inequity in the opposite direction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fdeff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.8821</td>\n",
       "      <td>0.0854</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9247</td>\n",
       "      <td>0.9348</td>\n",
       "      <td>0.8431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.5959</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.8962</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9425</td>\n",
       "      <td>0.9181</td>\n",
       "      <td>0.8542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.5054    0.8750   \n",
       "1           Sex             0   38.0       0.1579           0.1579    0.8947   \n",
       "2           Sex             1  146.0       0.6575           0.5959    0.8699   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8821  0.0854      —     0.9247   0.9348  0.8431  \n",
       "1    0.6667  0.0625      —     0.6667   0.9531  0.6667  \n",
       "2    0.8962  0.1000      —     0.9425   0.9181  0.8542  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fairmlhealth import measure\n",
    "import pandas as pd\n",
    "from IPython.display import display  \n",
    "\n",
    "# convert gender into DataFrame with a clear column name to get a nice table as output\n",
    "gender_df = pd.DataFrame({\"Sex\": X_test[\"Sex\"].astype(int)})\n",
    "\n",
    "\n",
    "# Get the stratified table\n",
    "perf_table_knn = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_knn = perf_table_knn.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e845f8a",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – KNN by Gender\n",
    "\n",
    "This table shows the **stratified performance metrics** of the KNN model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.8750)** and **F1-score (0.8821)** indicate strong overall performance.  \n",
    "- **ROC AUC (0.9348)** demonstrates excellent discriminatory ability.  \n",
    "- **Precision (0.9247)** shows predictions are highly reliable.  \n",
    "- **TPR (0.8431)** reflects solid sensitivity, though subgroup analysis reveals disparities.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8947     | 0.8699   | Accuracy is slightly higher for females. |\n",
    "| **F1-Score**  | 0.6667     | 0.8962   | The model is much more effective for males. |\n",
    "| **FPR**       | 0.0625     | 0.1000   | Females face fewer false positives than males. |\n",
    "| **Precision** | 0.6667     | 0.9425   | Predictions are far more reliable for males. |\n",
    "| **ROC AUC**   | 0.9531     | 0.9181   | Females benefit from higher ranking performance. |\n",
    "| **TPR**       | 0.6667     | 0.8542   | Males are more likely to be correctly identified when positive. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged):**  \n",
    "  - Show slightly higher accuracy and ROC AUC.  \n",
    "  - However, they suffer from **lower F1-score (0.6667)** and **much lower TPR (66.7%)**, meaning many true cases are missed.  \n",
    "  - Precision is also weaker (0.6667), so positive predictions for females are less trustworthy.  \n",
    "\n",
    "- **Males (privileged):**  \n",
    "  - Achieve stronger F1, precision, and sensitivity, indicating the model is **more effective overall** for this group.  \n",
    "  - Face a somewhat higher false positive rate (10%), but this trade-off comes with much higher predictive reliability and sensitivity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The KNN model shows **clear bias in favor of males**.  \n",
    "- Females benefit slightly in terms of ROC AUC and accuracy, but these gains are overshadowed by **much weaker recall, F1-score, and precision**, making the model less useful for them.  \n",
    "- Males enjoy **more consistent and reliable predictions**, highlighting a systematic gender disparity in performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5db1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6667\n",
      "  False Positive Rate (FPR): 0.0625\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.8542\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#group specific error analysis\n",
    "\n",
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_knn == 0)  # unprivileged group (female)\n",
    "male_mask   = (protected_attr_knn == 1)  # privileged group (male)\n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba4143",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – KNN Model\n",
    "\n",
    "To examine fairness at the subgroup level, we compare the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.\n",
    "\n",
    "---\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.6667  | 0.0625  |\n",
    "| Privileged (Male)      | 0.8542  | 0.1000  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR):**  \n",
    "  - Females have a considerably lower TPR (66.67%) than males (85.42%).  \n",
    "  - This means the model **misses more true positive cases** among females, suggesting lower sensitivity for this group.  \n",
    "\n",
    "- **False Positive Rate (FPR):**  \n",
    "  - Females experience a lower FPR (6.25%) compared to males (10.00%).  \n",
    "  - This indicates that females are **less often incorrectly classified as positive**, whereas males face more false alarms.  \n",
    "\n",
    "- **Trade-off observed:**  \n",
    "  - The model is **more sensitive for males**, detecting more true cases, but at the cost of higher false positives.  \n",
    "  - Conversely, females face **reduced detection of true cases** but benefit from fewer false positives.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "\n",
    "The KNN model exhibits **unequal error distributions across genders**:  \n",
    "- Females are disadvantaged in terms of **recall (TPR)**, meaning they are more likely to have their condition overlooked.  \n",
    "- Males benefit from higher sensitivity but also face more false alarms.  \n",
    "\n",
    "This imbalance contributes to fairness concerns, as the model **systematically favors the privileged group (males)** in identifying true cases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb317ca",
   "metadata": {},
   "source": [
    "### Decision Tree - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adbaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true    y_prob  y_pred\n",
      "0    1       1  1.000000       1\n",
      "1    1       1  0.000000       0\n",
      "2    1       1  1.000000       1\n",
      "3    1       1  0.272727       0\n",
      "4    0       0  1.000000       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "dt_df = pd.read_csv(\"HeartFailureData_50_50_AltTunedDT_predictions.csv\")\n",
    "\n",
    "print(dt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06530601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract common columns\n",
    "y_true_dt = dt_df[\"y_true\"].values\n",
    "y_prob_dt = dt_df[\"y_prob\"].values\n",
    "y_pred_dt = dt_df[\"y_pred\"].values\n",
    "gender_dt = dt_df[\"Sex\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_dt = gender_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b83e2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Decision Tree Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.0635\n",
      "               Balanced Accuracy Difference           -0.0646\n",
      "               Balanced Accuracy Ratio                 0.9227\n",
      "               Disparate Impact Ratio                  0.3891\n",
      "               Equal Odds Difference                  -0.1042\n",
      "               Equal Odds Ratio                        1.2500\n",
      "               Positive Predictive Parity Difference  -0.4367\n",
      "               Positive Predictive Parity Ratio        0.5338\n",
      "               Statistical Parity Difference          -0.3306\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Gender Bias Report\n",
    "print(\"\\n--- Decision Tree Gender Bias Report ---\")\n",
    "\n",
    "dt_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt,\n",
    "    prtc_attr=protected_attr_dt,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,  \n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(dt_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc2d88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e9ce8_row0_col0, #T_e9ce8_row1_col0, #T_e9ce8_row3_col0, #T_e9ce8_row4_col0, #T_e9ce8_row5_col0, #T_e9ce8_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e9ce8\">\n",
       "  <caption>Decision Tree Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e9ce8_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e9ce8_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_e9ce8_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_e9ce8_row0_col0\" class=\"data row0 col0\" >-0.0635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9ce8_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_e9ce8_row1_col0\" class=\"data row1 col0\" >-0.0646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9ce8_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_e9ce8_row2_col0\" class=\"data row2 col0\" >0.9227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9ce8_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_e9ce8_row3_col0\" class=\"data row3 col0\" >0.3891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9ce8_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_e9ce8_row4_col0\" class=\"data row4 col0\" >-0.1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9ce8_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_e9ce8_row5_col0\" class=\"data row5 col0\" >1.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9ce8_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_e9ce8_row6_col0\" class=\"data row6 col0\" >-0.4367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9ce8_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_e9ce8_row7_col0\" class=\"data row7 col0\" >0.5338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9ce8_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_e9ce8_row8_col0\" class=\"data row8 col0\" >-0.3306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9ce8_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_e9ce8_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_e9ce8_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x190b206af50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "styled_dt = MyFlagger().apply_flag(\n",
    "    df=dt_bias,\n",
    "    caption=\"Decision Tree Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1c7f5",
   "metadata": {},
   "source": [
    "## Gender Bias Detection Results – Decision Tree Model\n",
    "\n",
    "This table summarizes group fairness metrics for the **Decision Tree (DT)** model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (−0.0635):**  \n",
    "  The ROC AUC is lower for females, indicating weaker ranking performance compared to males.\n",
    "\n",
    "- **Balanced Accuracy Difference (−0.0646)** and **Ratio (0.9227):**  \n",
    "  Females experience **notably worse balanced accuracy**, showing that the model is less effective in correctly identifying both positives and negatives for this group.\n",
    "\n",
    "- **Disparate Impact Ratio (0.3891):**  \n",
    "  Well below the fairness threshold (0.80–1.25), suggesting **substantially unequal selection rates**, with females selected much less frequently than males.\n",
    "\n",
    "- **Equal Odds Difference (−0.1042)** and **Equal Odds Ratio (1.2500):**  \n",
    "  Indicates a **moderate disparity in error rates** (TPR and FPR) across genders, with outcomes skewed toward males.\n",
    "\n",
    "- **Positive Predictive Parity Difference (−0.4367)** and **Ratio (0.5338):**  \n",
    "  Predictions are **far less reliable for females**, showing significantly reduced precision compared to males.\n",
    "\n",
    "- **Statistical Parity Difference (−0.3306):**  \n",
    "  Confirms that **females are selected at a much lower rate**, reinforcing evidence of systemic imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation\n",
    "\n",
    "- The **Decision Tree model demonstrates clear fairness concerns**:  \n",
    "  - Females suffer from **lower AUC and balanced accuracy**, meaning worse overall predictive performance.  \n",
    "  - They face **reduced selection rates and weaker precision**, highlighting disadvantages in how positive predictions are distributed.  \n",
    "  - While error rate disparities (Equal Odds) are not extreme, they consistently disadvantage females.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "The Decision Tree model shows a **systematic bias against the unprivileged group (females)**.  \n",
    "- Females face **reduced ranking performance, accuracy, and reliability of predictions**, alongside lower selection rates.  \n",
    "- Males, as the privileged group, benefit from more favorable outcomes across multiple fairness dimensions.  \n",
    "\n",
    "Overall, these results indicate that the Decision Tree introduces **significant gender bias** and would require **mitigation strategies** before deployment decision-making contexts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "631b7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - DT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0646</td>\n",
       "      <td>1.0838</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.4367</td>\n",
       "      <td>1.8734</td>\n",
       "      <td>0.3306</td>\n",
       "      <td>2.5702</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>1.1563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0646</td>\n",
       "      <td>0.9227</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1.25</td>\n",
       "      <td>-0.4367</td>\n",
       "      <td>0.5338</td>\n",
       "      <td>-0.3306</td>\n",
       "      <td>0.3891</td>\n",
       "      <td>-0.1042</td>\n",
       "      <td>0.8649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                        0.0646   \n",
       "1          Sex             1                       -0.0646   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0838    -0.025       0.80    0.4367     1.8734   \n",
       "1                   0.9227     0.025       1.25   -0.4367     0.5338   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.3306           2.5702    0.1042     1.1563  \n",
       "1         -0.3306           0.3891   -0.1042     0.8649  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - DT\")\n",
    "measure.bias(X_test, y_test, y_pred_dt, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa9add",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – Decision Tree by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the Decision Tree (DT) model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Balanced Accuracy Difference = **+0.0646**, Ratio = **1.0838**  \n",
    "- **Males (1):** Balanced Accuracy Difference = **−0.0646**, Ratio = **0.9227**  \n",
    "- ➝ The model achieves **better balanced accuracy for females**, while males experience reduced performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** FPR Difference = **−0.0250**, Ratio = **0.80**  \n",
    "- **Males (1):** FPR Difference = **+0.0250**, Ratio = **1.25**  \n",
    "- ➝ Females face a **slightly lower false positive rate**, while males are more likely to be incorrectly flagged.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** PPV Difference = **+0.4367**, Ratio = **1.8734**  \n",
    "- **Males (1):** PPV Difference = **−0.4367**, Ratio = **0.5338**  \n",
    "- ➝ Predictions are **much more reliable for females**, while males show substantially lower precision.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Selection Difference = **+0.3306**, Ratio = **2.5702**  \n",
    "- **Males (1):** Selection Difference = **−0.3306**, Ratio = **0.3891**  \n",
    "- ➝ Females are **selected far more frequently**, whereas males are under-selected.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** TPR Difference = **+0.1042**, Ratio = **1.1563**  \n",
    "- **Males (1):** TPR Difference = **−0.1042**, Ratio = **0.8649**  \n",
    "- ➝ The model is **more sensitive for females**, correctly identifying a higher proportion of true positives compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- The Decision Tree model introduces a **reverse bias**, with **females (unprivileged group)** benefiting from higher balanced accuracy, lower false positive rates, better precision, higher selection rates, and stronger sensitivity.  \n",
    "- **Males (privileged group)** are consistently disadvantaged, facing more false positives, lower precision, and fewer positive selections.  \n",
    "\n",
    "Overall, the DT model systematically **favors females over males**, highlighting fairness concerns that should be addressed to ensure more equitable performance across genders.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1acccc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.4728</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.8254</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8966</td>\n",
       "      <td>0.8724</td>\n",
       "      <td>0.7647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.8421</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>—</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.8151</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.8151</td>\n",
       "      <td>0.8457</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9367</td>\n",
       "      <td>0.8786</td>\n",
       "      <td>0.7708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.4728    0.8207   \n",
       "1           Sex             0   38.0       0.1579           0.2105    0.8421   \n",
       "2           Sex             1  146.0       0.6575           0.5411    0.8151   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8254  0.1098      —     0.8966   0.8724  0.7647  \n",
       "1    0.5714  0.1250      —     0.5000   0.8151  0.6667  \n",
       "2    0.8457  0.1000      —     0.9367   0.8786  0.7708  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_dt = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_dt = perf_table_dt.fillna(\"—\")\n",
    "\n",
    "# Display pretty table\n",
    "display(perf_table_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936a469",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Decision Tree by Gender\n",
    "\n",
    "This table shows the **performance metrics** of the Decision Tree (DT) model stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.8207)** and **F1-score (0.8254)** indicate moderate classification performance overall.  \n",
    "- **ROC AUC (0.8724)** suggests the model has good discriminative ability.  \n",
    "- **Precision (0.8966)** is relatively high, but subgroup analysis reveals substantial disparities.  \n",
    "- **TPR (0.7647)** shows moderate sensitivity, meaning not all true cases are being detected.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8421     | 0.8151   | Accuracy is slightly higher for females. |\n",
    "| **F1-Score**  | 0.5714     | 0.8457   | The model performs much better for males in terms of balanced precision/recall. |\n",
    "| **FPR**       | 0.1250     | 0.1000   | Females experience a slightly higher false positive rate. |\n",
    "| **Precision** | 0.5000     | 0.9367   | Predictions for males are far more reliable; females’ predictions are weak. |\n",
    "| **ROC AUC**   | 0.8151     | 0.8786   | Males benefit from stronger ranking performance. |\n",
    "| **TPR**       | 0.6667     | 0.7708   | Sensitivity is higher for males, meaning they are more likely to be correctly identified. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged group):**  \n",
    "  - Experience **weaker overall performance**, with much lower F1-score and precision.  \n",
    "  - Their predictions are **less trustworthy** (precision only 0.5000), and they also face a slightly higher false positive rate.  \n",
    "  - Lower TPR (66.7%) means more true female cases are missed compared to males.  \n",
    "\n",
    "- **Males (privileged group):**  \n",
    "  - Benefit from **substantially stronger performance**: higher F1, precision, ROC AUC, and TPR.  \n",
    "  - Predictions for males are highly reliable, with strong sensitivity and ranking ability.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Decision Tree model introduces a **systematic disadvantage for females**, who face:  \n",
    "- **Lower predictive reliability (precision and F1)**,  \n",
    "- **Higher false positives**, and  \n",
    "- **Lower sensitivity (TPR)**.  \n",
    "\n",
    "Males consistently receive more favorable outcomes, confirming that the DT model exhibits **gender bias in favor of males**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53556439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6667\n",
      "  False Positive Rate (FPR): 0.1250\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.7708\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_dt == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_dt == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97580f6",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Decision Tree Model\n",
    "\n",
    "To further examine subgroup fairness, we compare the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** between the unprivileged (female) and privileged (male) groups.\n",
    "\n",
    "---\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.6667  | 0.1250  |\n",
    "| Privileged (Male)      | 0.7708  | 0.1000  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- The **TPR is higher for males (77.08%)** compared to females (66.67%).  \n",
    "  - This means males are more likely to be correctly identified when they actually have CVD, while females face a greater risk of missed diagnoses.  \n",
    "- The **FPR is slightly lower for males (10.00%)** than for females (12.50%).  \n",
    "  - This indicates that females are somewhat more likely to be incorrectly flagged as having CVD.  \n",
    "- Together, these disparities show that the model is **both more sensitive and more specific for males**, while females experience more errors in both directions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "\n",
    "The Decision Tree model shows a **systematic disadvantage for the unprivileged group (females)**:  \n",
    "- They are **less likely to be correctly diagnosed (lower TPR)**,  \n",
    "- And **more likely to be misclassified with false alarms (higher FPR)**.  \n",
    "\n",
    "This reflects a fairness concern, as error distribution favors the privileged group (males), underscoring the need for **bias mitigation strategies**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5168d",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaa2a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true  y_prob  y_pred\n",
      "0    1       1    0.96       1\n",
      "1    1       1    0.19       0\n",
      "2    1       1    0.97       1\n",
      "3    1       1    0.31       0\n",
      "4    0       0    0.37       0\n"
     ]
    }
   ],
   "source": [
    "rf_df = pd.read_csv(\"HeartFailureData_50_50_RF_predictions.csv\")\n",
    "print(rf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "278af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_rf = rf_df[\"y_true\"].values\n",
    "y_pred_rf = rf_df[\"y_pred\"].values\n",
    "y_prob_rf = rf_df[\"y_prob\"].values\n",
    "gender_rf = rf_df[\"Sex\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_rf = gender_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "689f0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0663\n",
      "               Balanced Accuracy Difference            0.0119\n",
      "               Balanced Accuracy Ratio                 1.0144\n",
      "               Disparate Impact Ratio                  0.4317\n",
      "               Equal Odds Difference                  -0.0237\n",
      "               Equal Odds Ratio                        0.8681\n",
      "               Positive Predictive Parity Difference  -0.3989\n",
      "               Positive Predictive Parity Ratio        0.5562\n",
      "               Statistical Parity Difference          -0.3464\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Gender Bias Report\n",
    "print(\"\\n--- Random Forest Gender Bias Report ---\")\n",
    "\n",
    "rf_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf,\n",
    "    prtc_attr=protected_attr_rf,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(rf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7a8e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bcf92_row0_col0, #T_bcf92_row3_col0, #T_bcf92_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bcf92\">\n",
       "  <caption>Random Forest Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bcf92_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bcf92_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_bcf92_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_bcf92_row0_col0\" class=\"data row0 col0\" >0.0663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bcf92_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_bcf92_row1_col0\" class=\"data row1 col0\" >0.0119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bcf92_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_bcf92_row2_col0\" class=\"data row2 col0\" >1.0144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bcf92_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_bcf92_row3_col0\" class=\"data row3 col0\" >0.4317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bcf92_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_bcf92_row4_col0\" class=\"data row4 col0\" >-0.0237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bcf92_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_bcf92_row5_col0\" class=\"data row5 col0\" >0.8681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bcf92_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_bcf92_row6_col0\" class=\"data row6 col0\" >-0.3989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bcf92_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_bcf92_row7_col0\" class=\"data row7 col0\" >0.5562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bcf92_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_bcf92_row8_col0\" class=\"data row8 col0\" >-0.3464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bcf92_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_bcf92_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_bcf92_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x190b2269cf0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for Random Forest\n",
    "styled_rf = MyFlagger().apply_flag(\n",
    "    df=rf_bias,\n",
    "    caption=\"Random Forest Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7c650",
   "metadata": {},
   "source": [
    "## Gender Bias Detection Results – Random Forest Model\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (0.0663):** The ROC AUC differs slightly between genders, suggesting somewhat better ranking performance for one group.  \n",
    "- **Balanced Accuracy Difference (0.0119)** and **Ratio (1.0144):** Very small disparity, indicating similar balanced accuracy across genders.  \n",
    "- **Disparate Impact Ratio (0.4317):** Far below the fairness threshold of 0.80–1.25, showing **substantial inequality in selection rates**, with females disadvantaged.  \n",
    "- **Equal Odds Difference (−0.0237)** and **Ratio (0.8681):** Small disparity in error rates (TPR and FPR), still favoring males slightly.  \n",
    "- **Positive Predictive Parity Difference (−0.3989)** and **Ratio (0.5562):** Predictions are much less reliable for females, reflecting lower precision.  \n",
    "- **Statistical Parity Difference (−0.3464):** Females are selected at a much lower rate than males, reinforcing evidence of systemic imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation\n",
    "\n",
    "- The Random Forest model exhibits **clear fairness concerns**:  \n",
    "  - **Selection rates and precision** heavily favor males (privileged group).  \n",
    "  - While **balanced accuracy and AUC disparities are small**, suggesting broadly similar predictive ability,  \n",
    "  - The **disparate impact and statistical parity values highlight structural disadvantages** for females.  \n",
    "- Females experience both **reduced chances of being selected** and **lower predictive reliability**, which points to biased treatment despite overall model strength.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "The Random Forest model demonstrates **systematic gender bias**, with females (unprivileged group) disadvantaged in **selection rates and predictive parity**.  \n",
    "Although differences in balanced accuracy and error rates appear small, the **large gaps in statistical and disparate impact measures** confirm that the model **favors males significantly**, warranting fairness mitigation before deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47479386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - RF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0119</td>\n",
       "      <td>0.9858</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>1.1520</td>\n",
       "      <td>0.3989</td>\n",
       "      <td>1.7978</td>\n",
       "      <td>0.3464</td>\n",
       "      <td>2.3164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>1.0144</td>\n",
       "      <td>-0.0237</td>\n",
       "      <td>0.8681</td>\n",
       "      <td>-0.3989</td>\n",
       "      <td>0.5562</td>\n",
       "      <td>-0.3464</td>\n",
       "      <td>0.4317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                       -0.0119   \n",
       "1          Sex             1                        0.0119   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9858    0.0237     1.1520    0.3989     1.7978   \n",
       "1                   1.0144   -0.0237     0.8681   -0.3989     0.5562   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.3464           2.3164       0.0        1.0  \n",
       "1         -0.3464           0.4317       0.0        1.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - RF\")\n",
    "measure.bias(X_test, y_test, y_pred_rf, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be772b16",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – Random Forest by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the Random Forest model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Balanced Accuracy Difference = **−0.0119**, Ratio = **0.9858**  \n",
    "- **Males (1):** Balanced Accuracy Difference = **+0.0119**, Ratio = **1.0144**  \n",
    "- ➝ Very small differences, suggesting **nearly equal balanced accuracy** across genders.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** FPR Difference = **+0.0237**, Ratio = **1.1520**  \n",
    "- **Males (1):** FPR Difference = **−0.0237**, Ratio = **0.8681**  \n",
    "- ➝ Females experience **slightly higher false positive rates**, meaning they are somewhat more often incorrectly flagged as positive.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** PPV Difference = **+0.3989**, Ratio = **1.7978**  \n",
    "- **Males (1):** PPV Difference = **−0.3989**, Ratio = **0.5562**  \n",
    "- ➝ Precision is **much higher for females**, suggesting predictions for women are more reliable, while males suffer from reduced predictive reliability.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Selection Difference = **+0.3464**, Ratio = **2.3164**  \n",
    "- **Males (1):** Selection Difference = **−0.3464**, Ratio = **0.4317**  \n",
    "- ➝ Females are **selected more than twice as often** as males, indicating a strong bias in favor of women in this model.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Both groups:** TPR Difference = **0.0**, Ratio = **1.0**  \n",
    "- ➝ Sensitivity is **equal across genders**, meaning both males and females have the same chance of being correctly identified when they truly have the condition.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- The Random Forest model shows **balanced accuracy and sensitivity (TPR) across genders**, suggesting no disparity in detecting true cases.  \n",
    "- However, females benefit from **much higher selection rates and precision**, while males are disadvantaged with lower predictive reliability.  \n",
    "- Although the error rates are relatively close, the **large disparities in selection and precision** indicate that the model introduces a **systematic bias in favor of females**.\n",
    "\n",
    "This reflects that while the RF model treats genders similarly in sensitivity, its **decision outcomes (selection rates and precision) are strongly skewed**, requiring fairness adjustments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6573254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>0.8315</td>\n",
       "      <td>0.8458</td>\n",
       "      <td>0.1707</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8586</td>\n",
       "      <td>0.9165</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.2632</td>\n",
       "      <td>0.8421</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>—</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.9661</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.6096</td>\n",
       "      <td>0.8288</td>\n",
       "      <td>0.8649</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8989</td>\n",
       "      <td>0.8999</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.5380    0.8315   \n",
       "1           Sex             0   38.0       0.1579           0.2632    0.8421   \n",
       "2           Sex             1  146.0       0.6575           0.6096    0.8288   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8458  0.1707      —     0.8586   0.9165  0.8333  \n",
       "1    0.6250  0.1562      —     0.5000   0.9661  0.8333  \n",
       "2    0.8649  0.1800      —     0.8989   0.8999  0.8333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_rf = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_rf = perf_table_rf.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de72316",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Random Forest by Gender  \n",
    "\n",
    "This table shows the **stratified performance metrics** of the Random Forest (RF) model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.8315)** and **F1-score (0.8458)** show good overall predictive performance.  \n",
    "- **ROC AUC (0.9165)** indicates strong discriminatory ability.  \n",
    "- **Precision (0.8586)** is high, suggesting reliable positive predictions.  \n",
    "- **TPR (0.8333)** shows the model correctly identifies most true cases.  \n",
    "- **Note**: PR AUC is not available (“—”), likely due to subgroup size limitations.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8421     | 0.8288   | Accuracy is slightly higher for females. |\n",
    "| **F1-Score**  | 0.6250     | 0.8649   | Model performs much better for males in terms of F1. |\n",
    "| **FPR**       | 0.1562     | 0.1800   | Females experience slightly fewer false positives than males. |\n",
    "| **Precision** | 0.5000     | 0.8989   | Precision is much lower for females, meaning predictions for them are less reliable. |\n",
    "| **ROC AUC**   | 0.9661     | 0.8999   | The model ranks females better than males. |\n",
    "| **TPR**       | 0.8333     | 0.8333   | Sensitivity is equal across genders. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged):**\n",
    "  - Benefit from slightly higher accuracy and much higher ROC AUC (0.9661).  \n",
    "  - However, they face a **low F1-score (0.6250)** due to poor precision (0.5000), meaning the model produces many false alarms for women.  \n",
    "  - False positive rate is slightly lower than for men.\n",
    "\n",
    "- **Males (privileged):**\n",
    "  - Receive **more balanced and reliable predictions**, with very high precision (0.8989) and strong F1-score (0.8649).  \n",
    "  - Their ROC AUC is lower than for females, meaning the ranking ability is weaker.  \n",
    "  - Slightly higher FPR indicates more false positives compared to females.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Random Forest model provides **equal sensitivity (TPR)** across genders but exhibits trade-offs:  \n",
    "- **Females** benefit from higher ranking quality (ROC AUC) and fewer false positives but suffer from **very low precision and weak F1 performance**.  \n",
    "- **Males** receive more consistent performance with strong precision and F1 but are ranked less effectively.  \n",
    "\n",
    "This indicates that while the RF model does not disadvantage either group in sensitivity, it shows **imbalances in prediction quality**: females face unreliable predictions, while males face slightly higher misclassification rates.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a52946a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.8333\n",
      "  False Positive Rate (FPR): 0.1562\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.8333\n",
      "  False Positive Rate (FPR): 0.1800\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_rf == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_rf == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06d248",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Random Forest (RF) Model\n",
    "\n",
    "To assess fairness at the subgroup level, we compared the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for females (unprivileged) and males (privileged).\n",
    "\n",
    "---\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.8333  | 0.1562  |\n",
    "| Privileged (Male)      | 0.8333  | 0.1800  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR):** Both groups achieve the **same sensitivity (83.33%)**, meaning the model is equally effective at detecting true positive cases for females and males.  \n",
    "- **False Positive Rate (FPR):** The rate of false alarms is slightly **lower for females (15.62%)** compared to males (18.00%). This means men are marginally more likely to be incorrectly classified as positive.  \n",
    "- The error distribution here is relatively balanced, with **minimal disparity in TPR** and only a **small difference in FPR**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "\n",
    "The Random Forest model demonstrates **fairly equitable performance across genders** in terms of sensitivity and specificity. While males experience a slightly higher false positive rate, the differences are minor, indicating that this model shows **low levels of gender bias** based on TPR and FPR.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136de6",
   "metadata": {},
   "source": [
    "### Deep Learning Model - Feed Forward Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c69d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true    y_prob  y_pred\n",
      "0    1       1  0.976401       1\n",
      "1    1       1  0.011376       0\n",
      "2    1       1  0.839763       1\n",
      "3    1       1  0.144366       0\n",
      "4    0       0  0.204195       0\n"
     ]
    }
   ],
   "source": [
    "mlp_df = pd.read_csv(\"HeartFailureData_50_50_RecallFirstTunedMLP_predictions.csv\")\n",
    "print(mlp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f65a498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns \n",
    "y_true_mlp = mlp_df[\"y_true\"].values \n",
    "y_prob_mlp = mlp_df[\"y_prob\"].values\n",
    "y_pred_mlp = mlp_df[\"y_pred\"].values\n",
    "gender_mlp = mlp_df[\"Sex\"].values \n",
    "\n",
    "# Use gender_mlp as the protected attribute\n",
    "protected_attr_mlp = gender_mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6969bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0175\n",
      "               Balanced Accuracy Difference           -0.0650\n",
      "               Balanced Accuracy Ratio                 0.9237\n",
      "               Disparate Impact Ratio                  0.3164\n",
      "               Equal Odds Difference                  -0.1562\n",
      "               Equal Odds Ratio                        0.7812\n",
      "               Positive Predictive Parity Difference  -0.3580\n",
      "               Positive Predictive Parity Ratio        0.6148\n",
      "               Statistical Parity Difference          -0.3980\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "#Run fairmlhealth bias detection for MLP \n",
    "\n",
    "mlp_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp,\n",
    "    prtc_attr=protected_attr_mlp,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(mlp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbf7eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_08e0b_row1_col0, #T_08e0b_row3_col0, #T_08e0b_row4_col0, #T_08e0b_row5_col0, #T_08e0b_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_08e0b\">\n",
       "  <caption>MLP Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_08e0b_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_08e0b_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_08e0b_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_08e0b_row0_col0\" class=\"data row0 col0\" >0.0175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_08e0b_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_08e0b_row1_col0\" class=\"data row1 col0\" >-0.0650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_08e0b_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_08e0b_row2_col0\" class=\"data row2 col0\" >0.9237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_08e0b_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_08e0b_row3_col0\" class=\"data row3 col0\" >0.3164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_08e0b_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_08e0b_row4_col0\" class=\"data row4 col0\" >-0.1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_08e0b_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_08e0b_row5_col0\" class=\"data row5 col0\" >0.7812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_08e0b_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_08e0b_row6_col0\" class=\"data row6 col0\" >-0.3580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_08e0b_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_08e0b_row7_col0\" class=\"data row7 col0\" >0.6148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_08e0b_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_08e0b_row8_col0\" class=\"data row8 col0\" >-0.3980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_08e0b_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_08e0b_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_08e0b_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x190b2268970>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for MLP\n",
    "styled_mlp = MyFlagger().apply_flag(\n",
    "    df=mlp_bias,\n",
    "    caption=\"MLP Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713caa5",
   "metadata": {},
   "source": [
    "## Gender Fairness Analysis – MLP Model\n",
    "\n",
    "This table reports the fairness metrics of the **MLP model** stratified by gender.  \n",
    "Here, males represent the **privileged group** and females the **unprivileged group**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Performance Disparities\n",
    "- **AUC Difference (0.0175):** Very small difference, indicating that the model ranks predictions similarly across genders.  \n",
    "- **Balanced Accuracy Difference (−0.0650) & Ratio (0.9237):** Balanced accuracy is lower for females, meaning the model detects outcomes less effectively for them.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Group Fairness\n",
    "- **Disparate Impact Ratio (0.3164):** Well below the acceptable fairness threshold (0.80–1.25). This indicates **females are selected at a much lower rate** than males.  \n",
    "- **Equal Odds Difference (−0.1562) & Ratio (0.7812):** Error rates (TPR and FPR) differ across genders, with males performing more favorably.  \n",
    "- **Positive Predictive Parity Difference (−0.3580) & Ratio (0.6148):** Precision is significantly lower for females, suggesting their positive predictions are less reliable.  \n",
    "- **Statistical Parity Difference (−0.3980):** Strong evidence of under-selection for females, as they are less likely to receive positive predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- The MLP model shows **systematic disadvantages for females**:  \n",
    "  - Lower **balanced accuracy**, meaning the model struggles more to correctly identify true outcomes for women.  \n",
    "  - Strong disparities in **selection rates, precision, and error balance**, indicating both reliability and fairness gaps.  \n",
    "  - Metrics like **Disparate Impact Ratio (0.3164)** and **Statistical Parity Difference (−0.3980)** confirm structural bias against females.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The MLP model demonstrates **substantial gender bias**, with females receiving fewer positive predictions, less reliable results, and reduced accuracy.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aa66097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - MLP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>0.065</td>\n",
       "      <td>1.0826</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>1.2800</td>\n",
       "      <td>0.358</td>\n",
       "      <td>1.6265</td>\n",
       "      <td>0.398</td>\n",
       "      <td>3.1605</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>1.2344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.9237</td>\n",
       "      <td>-0.0262</td>\n",
       "      <td>0.7812</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.6148</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>0.3164</td>\n",
       "      <td>-0.1562</td>\n",
       "      <td>0.8101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                         0.065   \n",
       "1          Sex             1                        -0.065   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0826    0.0262     1.2800     0.358     1.6265   \n",
       "1                   0.9237   -0.0262     0.7812    -0.358     0.6148   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0           0.398           3.1605    0.1562     1.2344  \n",
       "1          -0.398           0.3164   -0.1562     0.8101  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - MLP\")\n",
    "measure.bias(X_test, y_test, y_pred_mlp, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576976f",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – MLP by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the MLP model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Difference = **+0.065**, Ratio = **1.0826**  \n",
    "- **Males (1):** Difference = **−0.065**, Ratio = **0.9237**  \n",
    "- ➝ The model achieves **better balanced accuracy for females**, while males face a relative disadvantage.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** FPR Difference = **+0.0262**, Ratio = **1.2800**  \n",
    "- **Males (1):** FPR Difference = **−0.0262**, Ratio = **0.7812**  \n",
    "- ➝ Females have a **slightly higher false positive rate**, meaning they are more often misclassified as positive compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** PPV Difference = **+0.358**, Ratio = **1.6265**  \n",
    "- **Males (1):** PPV Difference = **−0.358**, Ratio = **0.6148**  \n",
    "- ➝ Predictions are **much more precise for females**, while males experience significantly less reliable positive predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Selection Difference = **+0.398**, Ratio = **3.1605**  \n",
    "- **Males (1):** Selection Difference = **−0.398**, Ratio = **0.3164**  \n",
    "- ➝ Females are **selected much more frequently**, whereas males are under-selected relative to their target distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** TPR Difference = **+0.1562**, Ratio = **1.2344**  \n",
    "- **Males (1):** TPR Difference = **−0.1562**, Ratio = **0.8101**  \n",
    "- ➝ The model is **considerably more sensitive for females**, detecting more true positives compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The MLP model shows a **clear bias in favor of females (unprivileged group)**:  \n",
    "- They benefit from **higher balanced accuracy, higher precision, stronger sensitivity, and much higher selection rates**.  \n",
    "- However, this comes with a **slightly elevated false positive rate**.  \n",
    "- Conversely, males (privileged group) face **lower selection, weaker predictive reliability, and reduced sensitivity**, making them systematically disadvantaged.\n",
    "\n",
    "In essence, the MLP introduces a **reverse bias**, strongly favoring females over males in classification outcomes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbc2caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.8478</td>\n",
       "      <td>0.8557</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9022</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.8137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>0.6154</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>—</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.5822</td>\n",
       "      <td>0.8425</td>\n",
       "      <td>0.8729</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9294</td>\n",
       "      <td>0.8992</td>\n",
       "      <td>0.8229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.5000    0.8478   \n",
       "1           Sex             0   38.0       0.1579           0.1842    0.8684   \n",
       "2           Sex             1  146.0       0.6575           0.5822    0.8425   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8557  0.1098      —     0.9022   0.9139  0.8137  \n",
       "1    0.6154  0.0938      —     0.5714   0.9167  0.6667  \n",
       "2    0.8729  0.1200      —     0.9294   0.8992  0.8229  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_mlp = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_mlp = perf_table_mlp.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec7c2f",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – MLP by Gender\n",
    "\n",
    "This table shows the **stratified performance metrics** of the MLP model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.848)** and **F1-score (0.856)** indicate strong overall classification ability.  \n",
    "- **ROC AUC (0.914)** shows very good discriminatory power.  \n",
    "- **Precision (0.902)** is high, suggesting predictions are generally reliable.  \n",
    "- **TPR (0.814)** reflects good sensitivity, though subgroup comparisons highlight disparities.  \n",
    "- **Note**: PR AUC is not available (“—”).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8684     | 0.8425   | Accuracy is slightly higher for females. |\n",
    "| **F1-Score**  | 0.6154     | 0.8729   | Model performs substantially better for males. |\n",
    "| **FPR**       | 0.0938     | 0.1200   | Females face fewer false positives than males. |\n",
    "| **Precision** | 0.5714     | 0.9294   | Predictions for males are much more reliable. |\n",
    "| **ROC AUC**   | 0.9167     | 0.8992   | Females benefit from slightly higher ranking performance. |\n",
    "| **TPR**       | 0.6667     | 0.8229   | Females are more likely to be missed (lower sensitivity). |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Show **higher accuracy and ROC AUC**, but suffer from **low F1-score and precision**.  \n",
    "  - Their **TPR (66.7%)** indicates many missed true cases, making the model less sensitive for them.  \n",
    "  - Despite fewer false positives (lower FPR), their positive predictions are less reliable.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Enjoy **much stronger F1-score, higher precision, and greater sensitivity (82.3% TPR)**.  \n",
    "  - They face more false positives than females, but overall predictions are much more trustworthy.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The MLP model reveals a **systematic disadvantage for females**:  \n",
    "- While they achieve comparable accuracy and even slightly better ROC AUC, their **precision, recall, and F1-score are significantly worse**.  \n",
    "- This means that **females are both more likely to be missed and less likely to receive reliable positive predictions**.  \n",
    "- Males, in contrast, consistently benefit from stronger predictive reliability and higher sensitivity.  \n",
    "\n",
    "This pattern indicates a fairness concern, where the model systematically favors the privileged group (males) in outcome quality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379dbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6667\n",
      "  False Positive Rate (FPR): 0.0938\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.8229\n",
      "  False Positive Rate (FPR): 0.1200\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_mlp == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_mlp == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fea8d1",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – MLP Model\n",
    "\n",
    "To further assess fairness at the subgroup level, we compare the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.\n",
    "\n",
    "---\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.6667  | 0.0938  |\n",
    "| Privileged (Male)      | 0.8229  | 0.1200  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR):**  \n",
    "  - Males achieve a higher TPR (**82.29%**) compared to females (**66.67%**).  \n",
    "  - This means the model is **more sensitive for males**, correctly identifying more true positive cases in this group.  \n",
    "  - Females face a higher risk of missed diagnoses.  \n",
    "\n",
    "- **False Positive Rate (FPR):**  \n",
    "  - Females have a slightly lower FPR (**9.38%**) than males (**12.00%**).  \n",
    "  - This indicates that females are **less likely to be incorrectly flagged** as positive when they are actually negative.  \n",
    "\n",
    "- **Overall trade-off:**  \n",
    "  - For males, the model sacrifices a **higher false positive rate** in exchange for a **much better TPR**.  \n",
    "  - For females, the model has **fewer false alarms** but also **misses more true cases**, raising concerns for fairness in high-stakes domains like healthcare.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary**\n",
    "The MLP model demonstrates a **performance imbalance**:  \n",
    "- **Males (privileged group)** benefit from **higher sensitivity**, ensuring more of their true cases are detected.  \n",
    "- **Females (unprivileged group)** experience **lower sensitivity**, meaning more missed cases, although they face fewer false positives.  \n",
    "\n",
    "This asymmetry highlights a **fairness concern**, as the model may systematically disadvantage females by under-diagnosing their true conditions.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
