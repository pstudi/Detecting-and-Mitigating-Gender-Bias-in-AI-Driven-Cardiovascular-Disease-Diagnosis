{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba0b2b9",
   "metadata": {},
   "source": [
    "### Bias Detection and Fairness Evaluation on Heart Failure Prediction Dataset (Kaggle) using FairMLhealth\n",
    "(Source: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c29a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load X_test set\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57b21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup successful\n"
     ]
    }
   ],
   "source": [
    "import fairmlhealth\n",
    "import aif360\n",
    "print(\"Environment setup successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f126c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fairmlhealth\n",
      "Version: 1.0.2\n",
      "Summary: Health-centered variation analysis\n",
      "Home-page: https://github.com/KenSciResearch/fairMLHealth\n",
      "Author: Christine Allen\n",
      "Author-email: ca.magallen@gmail.com\n",
      "License: \n",
      "Location: c:\\users\\patri\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: aif360, ipython, jupyter, numpy, pandas, requests, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#have a look at the details of fairmlhealth - especially the version\n",
    "!pip show fairmlhealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76186324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "#have a look at the modules that are within fairmlhealth\n",
    "\n",
    "print(dir(fairmlhealth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44657ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "#load necessary modules \n",
    "\n",
    "#import module measure to use measure.summary for bias detection\n",
    "from fairmlhealth import measure\n",
    "\n",
    "#import module for investigation of individual cohorts \n",
    "from fairmlhealth.__utils import iterate_cohorts\n",
    "\n",
    "#import FairRanges to flag high values\n",
    "from fairmlhealth.__utils import FairRanges\n",
    "\n",
    "# Wrap the fairness summary function for cohort-wise analysis\n",
    "@iterate_cohorts\n",
    "def cohort_summary(**kwargs):\n",
    "    return measure.summary(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10ccd4",
   "metadata": {},
   "source": [
    "During the execution of FairMLHealth and AIF360, several runtime warnings were raised (e.g., “AdversarialDebiasing will be unavailable” due to the absence of TensorFlow, and deprecation warnings from the inFairness package regarding PyTorch’s functorch.vmap). These warnings do not affect the fairness metrics or results presented in this study, as the unavailable components were not used. To maintain clarity of output, the warnings were silenced programmatically, and the analysis was conducted without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb6c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"inFairness\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"AdversarialDebiasing will be unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a099295",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models - KNN & DT\n",
    "\n",
    "#### K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d817251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true    y_prob  y_pred\n",
      "0    1       1  0.810107       1\n",
      "1    1       1  0.720647       1\n",
      "2    1       1  0.906213       1\n",
      "3    1       1  0.728721       1\n",
      "4    0       0  0.478459       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "knn_df = pd.read_csv(\"HeartFailureData_75M25F_PCA_KNN_predictions.csv\")\n",
    "\n",
    "print(knn_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871aa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_knn = knn_df[\"y_true\"].values\n",
    "y_prob_knn = knn_df[\"y_prob\"].values\n",
    "y_pred_knn = knn_df[\"y_pred\"].values\n",
    "gender_knn = knn_df[\"Sex\"].values\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_knn = gender_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad78836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0497\n",
      "               Balanced Accuracy Difference           -0.0385\n",
      "               Balanced Accuracy Ratio                 0.9568\n",
      "               Disparate Impact Ratio                  0.3842\n",
      "               Equal Odds Difference                  -0.0521\n",
      "               Equal Odds Ratio                        1.2500\n",
      "               Positive Predictive Parity Difference  -0.3889\n",
      "               Positive Predictive Parity Ratio        0.5882\n",
      "               Statistical Parity Difference          -0.3796\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "knn_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_true_knn,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn,\n",
    "    prtc_attr=protected_attr_knn,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,   # skip inconsistency metrics that cause NearestNeighbors error\n",
    "    skip_performance=True\n",
    ")\n",
    "\n",
    "print(knn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ba3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Custom scenario oriented bounds\n",
    "\n",
    "custom_ranges = {\n",
    "    \"tpr diff\": (-0.03, 0.03),\n",
    "    \"fpr diff\": (-0.03, 0.03),\n",
    "    \"equal odds difference\": (-0.04, 0.04),\n",
    "    \"statistical parity difference\": (-0.05, 0.05),\n",
    "    \"disparate impact ratio\": (0.9, 1.1),\n",
    "    \"selection ratio\": (0.9, 1.1),\n",
    "    \"auc difference\": (-0.02, 0.02),\n",
    "    \"balanced accuracy difference\": (-0.02, 0.02),\n",
    "}\n",
    "\n",
    "bounds = FairRanges().load_fair_ranges(custom_ranges=custom_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14d2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for highlighting metrics outside of the thresholds\n",
    "#  restore Styler.set_precision to adjust the highlighting color in the styled table\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "Styler = type(pd.DataFrame({\"_\":[0]}).style)  \n",
    "\n",
    "if not hasattr(Styler, \"set_precision\"):\n",
    "    def _set_precision(self, precision=4):\n",
    "        try:\n",
    "            return self.format(precision=precision)\n",
    "        except TypeError:\n",
    "            return self.format(formatter=lambda x:\n",
    "                f\"{x:.{precision}g}\" if isinstance(x, (int, float, np.floating)) else x\n",
    "            )\n",
    "    setattr(Styler, \"set_precision\", _set_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90435472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_85bb0_row0_col0, #T_85bb0_row1_col0, #T_85bb0_row3_col0, #T_85bb0_row4_col0, #T_85bb0_row5_col0, #T_85bb0_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_85bb0\">\n",
       "  <caption>KNN Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_85bb0_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_85bb0_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_85bb0_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_85bb0_row0_col0\" class=\"data row0 col0\" >0.0497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85bb0_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_85bb0_row1_col0\" class=\"data row1 col0\" >-0.0385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85bb0_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_85bb0_row2_col0\" class=\"data row2 col0\" >0.9568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85bb0_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_85bb0_row3_col0\" class=\"data row3 col0\" >0.3842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85bb0_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_85bb0_row4_col0\" class=\"data row4 col0\" >-0.0521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85bb0_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_85bb0_row5_col0\" class=\"data row5 col0\" >1.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85bb0_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_85bb0_row6_col0\" class=\"data row6 col0\" >-0.3889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85bb0_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_85bb0_row7_col0\" class=\"data row7 col0\" >0.5882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85bb0_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_85bb0_row8_col0\" class=\"data row8 col0\" >-0.3796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85bb0_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_85bb0_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_85bb0_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b0748a3370>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "from fairmlhealth.__utils import Flagger\n",
    "\n",
    "class MyFlagger(Flagger):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.flag_color = \"#491ee6\"   \n",
    "        self.flag_type = \"background-color\"\n",
    "\n",
    "styled_knn = MyFlagger().apply_flag(\n",
    "    df=knn_bias,\n",
    "    caption=\"KNN Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d3278",
   "metadata": {},
   "source": [
    "## Gender Bias Detection Results for KNN Model  \n",
    "\n",
    "---\n",
    "\n",
    "### Group Fairness Metrics  \n",
    "\n",
    "- **AUC Difference (0.0497):** A small gap in ranking performance exists, with slightly lower ROC-AUC for one gender group.  \n",
    "- **Balanced Accuracy Difference (-0.0385)** and **Ratio (0.9568):** Indicate a mild disparity, where females achieve lower balanced accuracy than males.  \n",
    "- **Disparate Impact Ratio (0.3842):** Well below the acceptable range of 0.80–1.25, showing **strong inequality in selection rates**, with females being much less likely to receive positive predictions.  \n",
    "- **Equal Odds Difference (-0.0521)** and **Equal Odds Ratio (1.2500):** Suggest some disparity in error rates (TPR/FPR), though less severe than other metrics.  \n",
    "- **Positive Predictive Parity Difference (-0.3889)** and **Ratio (0.5882):** Precision is significantly lower for females, making their positive predictions much less reliable.  \n",
    "- **Statistical Parity Difference (-0.3796):** Confirms a strong imbalance, with females being systematically under-selected compared to males.  \n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation  \n",
    "\n",
    "- The KNN model exhibits **serious fairness concerns**:  \n",
    "  - Females experience **lower predictive quality** (balanced accuracy, precision).  \n",
    "  - **Selection rates** are highly imbalanced, as shown by the disparate impact and statistical parity metrics.  \n",
    "  - Error-rate disparities (equal odds) are present but less extreme than the gaps in outcome allocation and predictive reliability.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "\n",
    "The KNN model shows **systematic gender bias**, disproportionately disadvantaging females (unprivileged group) while favoring males (privileged group).  \n",
    "The largest issues are found in **selection fairness (disparate impact, statistical parity)** and **prediction reliability (positive predictive parity)**.  \n",
    "These results suggest that KNN, in its current form, is **not a fair model** and requires mitigation before use in decision-making contexts.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0eeae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - KNN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>1.0451</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>2.6027</td>\n",
       "      <td>0.0521</td>\n",
       "      <td>1.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0385</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1.25</td>\n",
       "      <td>-0.3889</td>\n",
       "      <td>0.5882</td>\n",
       "      <td>-0.3796</td>\n",
       "      <td>0.3842</td>\n",
       "      <td>-0.0521</td>\n",
       "      <td>0.9412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                        0.0385   \n",
       "1          Sex             1                       -0.0385   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0451    -0.025       0.80    0.3889     1.7000   \n",
       "1                   0.9568     0.025       1.25   -0.3889     0.5882   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.3796           2.6027    0.0521     1.0625  \n",
       "1         -0.3796           0.3842   -0.0521     0.9412  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - KNN\")\n",
    "measure.bias(X_test, y_test, y_pred_knn, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05764be",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – KNN by Gender  \n",
    "\n",
    "This table presents **group-specific fairness metrics** for the KNN model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy  \n",
    "- **Females (0):** Balanced Accuracy Difference = **+0.0385**, Ratio = **1.0451**  \n",
    "- **Males (1):** Balanced Accuracy Difference = **−0.0385**, Ratio = **0.9568**  \n",
    "- ➝ The model is **slightly more balanced and accurate for females**, while males are at a minor disadvantage.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)  \n",
    "- **Females (0):** FPR Difference = **−0.0250**, Ratio = **0.80**  \n",
    "- **Males (1):** FPR Difference = **+0.0250**, Ratio = **1.25**  \n",
    "- ➝ Females face a **lower false positive rate**, while males are more often incorrectly classified as positive.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)  \n",
    "- **Females (0):** PPV Difference = **+0.3889**, Ratio = **1.7000**  \n",
    "- **Males (1):** PPV Difference = **−0.3889**, Ratio = **0.5882**  \n",
    "- ➝ Predictions are **far more reliable for females**, while males experience substantially lower precision.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate  \n",
    "- **Females (0):** Selection Difference = **+0.3796**, Ratio = **2.6027**  \n",
    "- **Males (1):** Selection Difference = **−0.3796**, Ratio = **0.3842**  \n",
    "- ➝ Females are **selected much more often** for positive predictions, while males are under-selected.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)  \n",
    "- **Females (0):** TPR Difference = **+0.0521**, Ratio = **1.0625**  \n",
    "- **Males (1):** TPR Difference = **−0.0521**, Ratio = **0.9412**  \n",
    "- ➝ Females have a **slight advantage in sensitivity**, meaning their true cases are more consistently detected compared to males.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "- The KNN model shows a **reverse bias**:  \n",
    "  - **Females (unprivileged group)** benefit from higher balanced accuracy, lower false positives, stronger precision, much higher selection rates, and slightly higher sensitivity.  \n",
    "  - **Males (privileged group)** are disadvantaged across nearly all metrics, with higher false positives, lower precision, under-selection, and slightly weaker sensitivity.  \n",
    "\n",
    "- Although females gain favorable outcomes, the **magnitude of disparities (especially in PPV and selection rate)** suggests that the KNN model introduces fairness concerns by systematically **favoring females at the expense of males**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fdeff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>0.8859</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.8824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.2368</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>—</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.9583</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.6164</td>\n",
       "      <td>0.8904</td>\n",
       "      <td>0.9140</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>0.8854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.5380    0.8859   \n",
       "1           Sex             0   38.0       0.1579           0.2368    0.8684   \n",
       "2           Sex             1  146.0       0.6575           0.6164    0.8904   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8955  0.1098      —     0.9091   0.9253  0.8824  \n",
       "1    0.6667  0.1250      —     0.5556   0.9583  0.8333  \n",
       "2    0.9140  0.1000      —     0.9444   0.9086  0.8854  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fairmlhealth import measure\n",
    "import pandas as pd\n",
    "from IPython.display import display  \n",
    "\n",
    "# convert gender into DataFrame with a clear column name to get a nice table as output\n",
    "gender_df = pd.DataFrame({\"Sex\": X_test[\"Sex\"].astype(int)})\n",
    "\n",
    "\n",
    "# Get the stratified table\n",
    "perf_table_knn = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_knn = perf_table_knn.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e845f8a",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – KNN by Gender  \n",
    "\n",
    "This table shows the **stratified performance metrics** of the KNN model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)  \n",
    "- **Accuracy (0.8859)** and **F1-score (0.8955)** indicate good overall performance.  \n",
    "- **ROC AUC (0.9253)** demonstrates strong discriminatory ability.  \n",
    "- **Precision (0.9091)** is high, meaning predictions are generally reliable.  \n",
    "- **TPR (0.8824)** shows strong sensitivity overall.  \n",
    "- **Note:** PR AUC is not available (“—”) in this report.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison  \n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8684     | 0.8904   | Accuracy is slightly higher for males. |\n",
    "| **F1-Score**  | 0.6667     | 0.9140   | Performance is much stronger for males; females face lower F1. |\n",
    "| **FPR**       | 0.1250     | 0.1000   | Females have more false positives. |\n",
    "| **Precision** | 0.5556     | 0.9444   | Male predictions are far more reliable. |\n",
    "| **ROC AUC**   | 0.9583     | 0.9086   | Females benefit from stronger ranking quality despite lower precision. |\n",
    "| **TPR**       | 0.8333     | 0.8854   | Sensitivity is slightly better for males. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation  \n",
    "\n",
    "- **Females (unprivileged):**  \n",
    "  - Struggle with much lower **precision (0.5556)** and **F1-score (0.6667)**, meaning predictions are less reliable and effective.  \n",
    "  - Have a **higher false positive rate (12.5%)**, causing more false alarms.  \n",
    "  - ROC AUC (0.9583) is strong, showing the model ranks female cases well, but precision issues limit trust in predictions.  \n",
    "  - Slightly lower TPR (0.8333) indicates more missed true cases.  \n",
    "\n",
    "- **Males (privileged):**  \n",
    "  - Achieve consistently better performance: higher accuracy, F1, and precision.  \n",
    "  - Very strong precision (0.9444) and F1 (0.9140), with fewer false positives (10%).  \n",
    "  - ROC AUC (0.9086) is slightly lower than females, but overall predictions for males are **much more reliable**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "The KNN model introduces **clear disparities across genders**:  \n",
    "- **Females** are disadvantaged by lower precision, weaker F1, and higher false positives, making predictions less trustworthy.  \n",
    "- **Males** benefit from more accurate and reliable outcomes across nearly all performance metrics.  \n",
    "\n",
    "While females show a strong ROC AUC, this advantage does not translate into practical reliability, confirming that KNN systematically **favors males in prediction quality**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5db1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.8333\n",
      "  False Positive Rate (FPR): 0.1250\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.8854\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#group specific error analysis\n",
    "\n",
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_knn == 0)  # unprivileged group (female)\n",
    "male_mask   = (protected_attr_knn == 1)  # privileged group (male)\n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba4143",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – KNN Model  \n",
    "\n",
    "To further examine fairness at the subgroup level, we compared the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.  \n",
    "\n",
    "#### Results by Gender Group  \n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.8333  | 0.1250  |\n",
    "| Privileged (Male)      | 0.8854  | 0.1000  |\n",
    "\n",
    "#### Interpretation  \n",
    "\n",
    "- The **privileged group (male)** has a slightly higher **TPR (88.54%)** than the unprivileged group (83.33%), meaning males are somewhat more likely to be correctly identified when they truly have CVD.  \n",
    "- The **FPR is lower for males (10.00%)** compared to females (12.50%), suggesting females are more often incorrectly flagged as having CVD.  \n",
    "- Although the disparities are smaller than in some earlier cases, the model still shows **better sensitivity and specificity for males**, while females face relatively more false alarms and a higher risk of missed detections.  \n",
    "\n",
    "#### Summary  \n",
    "\n",
    "The subgroup-level results reveal a **persistent disadvantage for the unprivileged group (females)**.  \n",
    "- They experience **slightly lower sensitivity** (TPR) and **higher false positive rates** than males.  \n",
    "- These patterns confirm that even modest differences in error distribution can contribute to **systematic gender bias**, highlighting the importance of fairness-aware evaluation and potential bias mitigation.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb317ca",
   "metadata": {},
   "source": [
    "### Decision Tree - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adbaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true  y_prob  y_pred\n",
      "0    1       1     1.0       1\n",
      "1    1       1     0.0       0\n",
      "2    1       1     1.0       1\n",
      "3    1       1     0.0       0\n",
      "4    0       0     0.0       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "dt_df = pd.read_csv(\"HeartFailureData_75M25F_BaselineDT_predictions.csv\")\n",
    "\n",
    "print(dt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06530601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract common columns\n",
    "y_true_dt = dt_df[\"y_true\"].values\n",
    "y_prob_dt = dt_df[\"y_prob\"].values\n",
    "y_pred_dt = dt_df[\"y_pred\"].values\n",
    "gender_dt = dt_df[\"Sex\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_dt = gender_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b83e2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Decision Tree Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0579\n",
      "               Balanced Accuracy Difference            0.0579\n",
      "               Balanced Accuracy Ratio                 1.0727\n",
      "               Disparate Impact Ratio                  0.3885\n",
      "               Equal Odds Difference                  -0.0950\n",
      "               Equal Odds Ratio                        0.5682\n",
      "               Positive Predictive Parity Difference  -0.3208\n",
      "               Positive Predictive Parity Ratio        0.6339\n",
      "               Statistical Parity Difference          -0.3727\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Gender Bias Report\n",
    "print(\"\\n--- Decision Tree Gender Bias Report ---\")\n",
    "\n",
    "dt_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt,\n",
    "    prtc_attr=protected_attr_dt,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,  \n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(dt_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc2d88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bd6e3_row0_col0, #T_bd6e3_row1_col0, #T_bd6e3_row3_col0, #T_bd6e3_row4_col0, #T_bd6e3_row5_col0, #T_bd6e3_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bd6e3\">\n",
       "  <caption>Decision Tree Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bd6e3_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bd6e3_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_bd6e3_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_bd6e3_row0_col0\" class=\"data row0 col0\" >0.0579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd6e3_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_bd6e3_row1_col0\" class=\"data row1 col0\" >0.0579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd6e3_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_bd6e3_row2_col0\" class=\"data row2 col0\" >1.0727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd6e3_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_bd6e3_row3_col0\" class=\"data row3 col0\" >0.3885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd6e3_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_bd6e3_row4_col0\" class=\"data row4 col0\" >-0.0950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd6e3_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_bd6e3_row5_col0\" class=\"data row5 col0\" >0.5682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd6e3_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_bd6e3_row6_col0\" class=\"data row6 col0\" >-0.3208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd6e3_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_bd6e3_row7_col0\" class=\"data row7 col0\" >0.6339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd6e3_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_bd6e3_row8_col0\" class=\"data row8 col0\" >-0.3727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd6e3_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_bd6e3_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_bd6e3_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b0748a23e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "styled_dt = MyFlagger().apply_flag(\n",
    "    df=dt_bias,\n",
    "    caption=\"Decision Tree Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1c7f5",
   "metadata": {},
   "source": [
    "## Gender Bias Detection Results – Decision Tree Model  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics  \n",
    "\n",
    "- **AUC Difference (0.0579):** The ROC AUC is moderately higher for one gender, suggesting better ranking ability for that group.  \n",
    "- **Balanced Accuracy Difference (0.0579)** and **Ratio (1.0727):** Indicates that the model achieves somewhat better balanced accuracy for one gender, showing mild disparity.  \n",
    "- **Disparate Impact Ratio (0.3885):** Well below the fairness threshold of 0.80–1.25, pointing to strong inequality in selection rates.  \n",
    "- **Equal Odds Difference (−0.0950)** and **Equal Odds Ratio (0.5682):** Error rates (TPR and FPR) differ significantly across genders, favoring one group.  \n",
    "- **Positive Predictive Parity Difference (−0.3208)** and **Ratio (0.6339):** Precision is much lower for one gender, showing predictions are less reliable for that subgroup.  \n",
    "- **Statistical Parity Difference (−0.3727):** A substantial negative value, confirming that females are selected at a much lower rate than males.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation  \n",
    "\n",
    "- The Decision Tree model demonstrates **considerable fairness concerns**:  \n",
    "  - While differences in **AUC and balanced accuracy** are modest, **disparate impact and statistical parity differences are severe**, indicating large selection imbalances.  \n",
    "  - **Equal odds disparity** suggests that error rates are skewed, disadvantaging one group in terms of both false positives and false negatives.  \n",
    "  - Precision (positive predictive value) is substantially worse for the disadvantaged group, lowering trust in positive predictions.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "\n",
    "The Decision Tree model shows **systematic gender bias**, with strong disparities in **selection rate, predictive precision, and error rates**.  \n",
    "Although ranking ability (AUC) and balanced accuracy differences are relatively small, the large gaps in fairness metrics (disparate impact and statistical parity) highlight that the model **favors males while disadvantaging females**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "631b7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - DT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0579</td>\n",
       "      <td>0.9322</td>\n",
       "      <td>0.095</td>\n",
       "      <td>1.7600</td>\n",
       "      <td>0.3208</td>\n",
       "      <td>1.5775</td>\n",
       "      <td>0.3727</td>\n",
       "      <td>2.5738</td>\n",
       "      <td>-0.0208</td>\n",
       "      <td>0.9750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>1.0727</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>-0.3208</td>\n",
       "      <td>0.6339</td>\n",
       "      <td>-0.3727</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>1.0256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                       -0.0579   \n",
       "1          Sex             1                        0.0579   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9322     0.095     1.7600    0.3208     1.5775   \n",
       "1                   1.0727    -0.095     0.5682   -0.3208     0.6339   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.3727           2.5738   -0.0208     0.9750  \n",
       "1         -0.3727           0.3885    0.0208     1.0256  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - DT\")\n",
    "measure.bias(X_test, y_test, y_pred_dt, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa9add",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – Decision Tree (DT) by Gender  \n",
    "\n",
    "This table presents **group-specific fairness metrics** for the Decision Tree model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy  \n",
    "- **Females (0):** Difference = **−0.0579**, Ratio = **0.9322**  \n",
    "- **Males (1):** Difference = **+0.0579**, Ratio = **1.0727**  \n",
    "- ➝ The model achieves **better balanced accuracy for males**, while females face a noticeable disadvantage.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)  \n",
    "- **Females (0):** FPR Difference = **+0.0950**, Ratio = **1.7600**  \n",
    "- **Males (1):** FPR Difference = **−0.0950**, Ratio = **0.5682**  \n",
    "- ➝ Females are **far more likely to be incorrectly classified as positive**, while males are less exposed to false positives.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)  \n",
    "- **Females (0):** PPV Difference = **+0.3208**, Ratio = **1.5775**  \n",
    "- **Males (1):** PPV Difference = **−0.3208**, Ratio = **0.6339**  \n",
    "- ➝ Predictions are **more precise for females**, while males suffer from lower reliability of positive predictions.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate  \n",
    "- **Females (0):** Selection Difference = **+0.3727**, Ratio = **2.5738**  \n",
    "- **Males (1):** Selection Difference = **−0.3727**, Ratio = **0.3885**  \n",
    "- ➝ The model **selects females much more frequently** than males, pointing to imbalance in decision allocation.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)  \n",
    "- **Females (0):** TPR Difference = **−0.0208**, Ratio = **0.9750**  \n",
    "- **Males (1):** TPR Difference = **+0.0208**, Ratio = **1.0256**  \n",
    "- ➝ Males have a **slight advantage in sensitivity**, meaning their true positive cases are more often detected.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "The Decision Tree model displays **mixed fairness outcomes**:  \n",
    "- **Females** face a **higher false positive rate** but benefit from **higher precision and selection rates**.  \n",
    "- **Males** enjoy **better balanced accuracy and slightly higher sensitivity (TPR)**, but their positive predictions are less precise and they are under-selected overall.  \n",
    "\n",
    "Overall, the model exhibits **imbalances in both error rates and selection fairness**, suggesting that while females are more frequently selected, this comes at the cost of **more false alarms**, whereas males are somewhat overlooked but benefit from **lower misclassification risk**.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1acccc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.8152</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.1829</td>\n",
       "      <td>0.3276</td>\n",
       "      <td>0.8469</td>\n",
       "      <td>0.8154</td>\n",
       "      <td>0.8137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.2368</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.5497</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.8542</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.6096</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.8432</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.2486</td>\n",
       "      <td>0.8764</td>\n",
       "      <td>0.7962</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.5326    0.8152   \n",
       "1           Sex             0   38.0       0.1579           0.2368    0.8684   \n",
       "2           Sex             1  146.0       0.6575           0.6096    0.8014   \n",
       "\n",
       "   F1-Score     FPR  PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8300  0.1829  0.3276     0.8469   0.8154  0.8137  \n",
       "1    0.6667  0.1250  0.5497     0.5556   0.8542  0.8333  \n",
       "2    0.8432  0.2200  0.2486     0.8764   0.7962  0.8125  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_dt = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_dt = perf_table_dt.fillna(\"—\")\n",
    "\n",
    "# Display pretty table\n",
    "display(perf_table_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936a469",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Decision Tree (DT) by Gender  \n",
    "\n",
    "This table shows the **stratified performance metrics** of the Decision Tree model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)  \n",
    "- **Accuracy (0.8152)** and **F1-score (0.8300)** indicate moderate overall classification quality.  \n",
    "- **ROC AUC (0.8154)** suggests decent but not excellent ability to distinguish between classes.  \n",
    "- **Precision (0.8469)** is relatively high, meaning predictions are fairly reliable.  \n",
    "- **True Positive Rate (TPR = 0.8137)** reflects good sensitivity overall, but subgroup breakdowns reveal disparities.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison  \n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8684     | 0.8014   | Accuracy is higher for females than males. |\n",
    "| **F1-Score**  | 0.6667     | 0.8432   | Model performs better for males in balancing precision and recall. |\n",
    "| **FPR**       | 0.1250     | 0.2200   | Females face fewer false positives compared to males. |\n",
    "| **Precision** | 0.5556     | 0.8764   | Predictions are much more reliable for males. |\n",
    "| **ROC AUC**   | 0.8542     | 0.7962   | Females benefit from stronger ranking performance. |\n",
    "| **TPR**       | 0.8333     | 0.8125   | Sensitivity is slightly higher for females. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation  \n",
    "- **Females (unprivileged):**  \n",
    "  - Show **better accuracy and ROC AUC**, suggesting the model discriminates cases more effectively for this group.  \n",
    "  - However, they experience **lower precision (0.5556)** and **weaker F1-score (0.6667)**, meaning their positive predictions are less reliable.  \n",
    "  - They also benefit from a **lower false positive rate (12.5%)**, reducing the chance of being incorrectly flagged.  \n",
    "\n",
    "- **Males (privileged):**  \n",
    "  - Have **higher precision (0.8764) and stronger F1 (0.8432)**, meaning the model is more consistent in positive predictions for this group.  \n",
    "  - However, they face a **higher false positive rate (22.0%)**, indicating more incorrect alarms.  \n",
    "  - Their ROC AUC is lower, suggesting weaker ranking ability compared to females.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "The Decision Tree model demonstrates a **trade-off in gender-specific performance**:  \n",
    "- **Females** benefit from **higher accuracy, ROC AUC, and lower false positives**, but their predictions are less precise and less balanced.  \n",
    "- **Males** benefit from **greater precision and F1 performance**, but at the cost of **more false positives** and a weaker ROC AUC.  \n",
    "\n",
    "This indicates that the model does not consistently favor one group but instead introduces **different types of disparities**, highlighting the need for fairness adjustments to balance error trade-offs across genders.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53556439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.8333\n",
      "  False Positive Rate (FPR): 0.1250\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.8125\n",
      "  False Positive Rate (FPR): 0.2200\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_dt == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_dt == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97580f6",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Decision Tree Model  \n",
    "\n",
    "To further examine fairness at the subgroup level, we compared the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.  \n",
    "\n",
    "#### Results by Gender Group  \n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.8333  | 0.1250  |\n",
    "| Privileged (Male)      | 0.8125  | 0.2200  |\n",
    "\n",
    "#### Interpretation  \n",
    "\n",
    "- The **True Positive Rate (TPR)** is slightly higher for females (83.33%) than for males (81.25%), meaning the model is marginally better at correctly detecting positive cases among females.  \n",
    "- The **False Positive Rate (FPR)** is considerably lower for females (12.50%) compared to males (22.00%), indicating that males are more likely to be incorrectly flagged as positive cases.  \n",
    "- Overall, this suggests the model is **both more sensitive and more specific for females**, while males face a greater risk of false alarms despite nearly equal sensitivity.  \n",
    "\n",
    "#### Summary  \n",
    "\n",
    "The Decision Tree model introduces a **systematic disadvantage for males (privileged group)**: although detection rates are nearly equal, males are far more prone to false positives. In contrast, females (unprivileged group) benefit from slightly higher sensitivity and significantly better specificity. This indicates that, unlike typical gender bias patterns, the model demonstrates a **reverse bias**, favoring females in error distribution.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5168d",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaa2a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true  y_prob  y_pred\n",
      "0    1       1    0.91       1\n",
      "1    1       1    0.41       0\n",
      "2    1       1    0.97       1\n",
      "3    1       1    0.55       1\n",
      "4    0       0    0.56       1\n"
     ]
    }
   ],
   "source": [
    "rf_df = pd.read_csv(\"HeartFailureData_75M25F_BaselineRF_predictions.csv\")\n",
    "print(rf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "278af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_rf = rf_df[\"y_true\"].values\n",
    "y_pred_rf = rf_df[\"y_pred\"].values\n",
    "y_prob_rf = rf_df[\"y_prob\"].values\n",
    "gender_rf = rf_df[\"Sex\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_rf = gender_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "689f0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0877\n",
      "               Balanced Accuracy Difference            0.0792\n",
      "               Balanced Accuracy Ratio                 1.0922\n",
      "               Disparate Impact Ratio                  0.3921\n",
      "               Equal Odds Difference                   0.0833\n",
      "               Equal Odds Ratio                        0.6250\n",
      "               Positive Predictive Parity Difference  -0.2980\n",
      "               Positive Predictive Parity Ratio        0.6682\n",
      "               Statistical Parity Difference          -0.4081\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Gender Bias Report\n",
    "print(\"\\n--- Random Forest Gender Bias Report ---\")\n",
    "\n",
    "rf_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf,\n",
    "    prtc_attr=protected_attr_rf,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(rf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7a8e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_23e83_row0_col0, #T_23e83_row1_col0, #T_23e83_row3_col0, #T_23e83_row4_col0, #T_23e83_row5_col0, #T_23e83_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_23e83\">\n",
       "  <caption>Random Forest Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_23e83_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_23e83_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_23e83_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_23e83_row0_col0\" class=\"data row0 col0\" >0.0877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23e83_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_23e83_row1_col0\" class=\"data row1 col0\" >0.0792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23e83_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_23e83_row2_col0\" class=\"data row2 col0\" >1.0922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23e83_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_23e83_row3_col0\" class=\"data row3 col0\" >0.3921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23e83_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_23e83_row4_col0\" class=\"data row4 col0\" >0.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23e83_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_23e83_row5_col0\" class=\"data row5 col0\" >0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23e83_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_23e83_row6_col0\" class=\"data row6 col0\" >-0.2980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23e83_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_23e83_row7_col0\" class=\"data row7 col0\" >0.6682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23e83_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_23e83_row8_col0\" class=\"data row8 col0\" >-0.4081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23e83_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_23e83_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_23e83_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b074a9d660>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for Random Forest\n",
    "styled_rf = MyFlagger().apply_flag(\n",
    "    df=rf_bias,\n",
    "    caption=\"Random Forest Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7c650",
   "metadata": {},
   "source": [
    "## Gender Bias Detection Results for Random Forest Model  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics  \n",
    "\n",
    "- **AUC Difference (0.0877):** The ROC AUC is higher for one gender group, indicating better ranking performance compared to the other.  \n",
    "- **Balanced Accuracy Difference (0.0792)** and **Ratio (1.0922):** Suggest that one gender benefits from notably better balanced accuracy.  \n",
    "- **Disparate Impact Ratio (0.3921):** Far below the acceptable fairness threshold (0.80–1.25), highlighting **substantial inequality in selection rates**, strongly disadvantaging one gender.  \n",
    "- **Equal Odds Difference (0.0833)** and **Equal Odds Ratio (0.6250):** Indicate meaningful disparities in error rates (TPR and FPR), with the model treating genders inconsistently.  \n",
    "- **Positive Predictive Parity Difference (−0.2980)** and **Ratio (0.6682):** Predictions are less reliable for the unprivileged group, showing lower precision.  \n",
    "- **Statistical Parity Difference (−0.4081):** Confirms a large imbalance in selection between genders, strongly disadvantaging the unprivileged group.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation  \n",
    "\n",
    "- The Random Forest model shows **clear evidence of gender bias**.  \n",
    "- Females (unprivileged group) face lower **selection rates, precision, and statistical parity**, while males (privileged group) consistently benefit.  \n",
    "- Disparities in **AUC and balanced accuracy** further confirm uneven predictive quality across genders.  \n",
    "- The combination of **low disparate impact ratio and high statistical parity difference** signals a **systematic disadvantage for females**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "\n",
    "The Random Forest model introduces **strong gender bias**.  \n",
    "- **Females are disadvantaged**, experiencing reduced selection opportunities, less reliable predictions, and poorer error balance.  \n",
    "- **Males are favored**, with higher predictive reliability and overall performance.  \n",
    "- The fairness metrics suggest that Random Forest is **not equitable across genders** and requires bias mitigation before deployment.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47479386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - RF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0792</td>\n",
       "      <td>0.9156</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.600</td>\n",
       "      <td>0.298</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>0.4081</td>\n",
       "      <td>2.5507</td>\n",
       "      <td>-0.0833</td>\n",
       "      <td>0.9167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0792</td>\n",
       "      <td>1.0922</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.6682</td>\n",
       "      <td>-0.4081</td>\n",
       "      <td>0.3921</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>1.0909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                       -0.0792   \n",
       "1          Sex             1                        0.0792   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9156     0.075      1.600     0.298     1.4966   \n",
       "1                   1.0922    -0.075      0.625    -0.298     0.6682   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.4081           2.5507   -0.0833     0.9167  \n",
       "1         -0.4081           0.3921    0.0833     1.0909  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - RF\")\n",
    "measure.bias(X_test, y_test, y_pred_rf, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be772b16",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – Random Forest by Gender  \n",
    "\n",
    "This table presents **group-specific fairness metrics** for the Random Forest model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy  \n",
    "- **Females (0):** Balanced Accuracy Difference = **−0.0792**, Ratio = **0.9156**  \n",
    "- **Males (1):** Balanced Accuracy Difference = **+0.0792**, Ratio = **1.0922**  \n",
    "- ➝ The model provides **higher balanced accuracy for males**, while females experience reduced predictive balance.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)  \n",
    "- **Females (0):** FPR Difference = **+0.075**, Ratio = **1.600**  \n",
    "- **Males (1):** FPR Difference = **−0.075**, Ratio = **0.625**  \n",
    "- ➝ Females suffer from a **notably higher false positive rate**, being more often incorrectly flagged as positive cases.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)  \n",
    "- **Females (0):** PPV Difference = **+0.298**, Ratio = **1.4966**  \n",
    "- **Males (1):** PPV Difference = **−0.298**, Ratio = **0.6682**  \n",
    "- ➝ Predictions are **more reliable for females** (higher precision), while males face reduced reliability.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate  \n",
    "- **Females (0):** Selection Difference = **+0.4081**, Ratio = **2.5507**  \n",
    "- **Males (1):** Selection Difference = **−0.4081**, Ratio = **0.3921**  \n",
    "- ➝ Females are **selected far more frequently** than expected, while males are substantially under-selected.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)  \n",
    "- **Females (0):** TPR Difference = **−0.0833**, Ratio = **0.9167**  \n",
    "- **Males (1):** TPR Difference = **+0.0833**, Ratio = **1.0909**  \n",
    "- ➝ The model is **slightly more sensitive for males**, meaning their true cases are more often detected.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "- The Random Forest model displays **mixed bias patterns**:  \n",
    "  - **Females** face disadvantages in **balanced accuracy** and **false positive rates**, but benefit from **higher precision and selection rates**.  \n",
    "  - **Males** gain from **better sensitivity (TPR)** and **balanced accuracy**, but are **under-selected and predicted with lower reliability**.  \n",
    "- These results suggest that Random Forest introduces **inconsistent fairness trade-offs**, with both genders experiencing different advantages and disadvantages.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6573254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>0.8952</td>\n",
       "      <td>0.1707</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.9186</td>\n",
       "      <td>0.9216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.2632</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.6712</td>\n",
       "      <td>0.8767</td>\n",
       "      <td>0.9072</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8980</td>\n",
       "      <td>0.8941</td>\n",
       "      <td>0.9167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.5870    0.8804   \n",
       "1           Sex             0   38.0       0.1579           0.2632    0.8947   \n",
       "2           Sex             1  146.0       0.6575           0.6712    0.8767   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8952  0.1707      —     0.8704   0.9186  0.9216  \n",
       "1    0.7500  0.1250      —     0.6000   0.9818  1.0000  \n",
       "2    0.9072  0.2000      —     0.8980   0.8941  0.9167  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_rf = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_rf = perf_table_rf.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de72316",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Random Forest by Gender  \n",
    "\n",
    "This table shows the **stratified performance metrics** of the Random Forest model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)  \n",
    "- **Accuracy (0.8804)** and **F1-score (0.8952)** indicate strong overall classification performance.  \n",
    "- **ROC AUC (0.9186)** shows very good discriminatory ability.  \n",
    "- **Precision (0.8704)** suggests predictions are generally reliable.  \n",
    "- **TPR (0.9216)** demonstrates high sensitivity overall.  \n",
    "- **Note**: PR AUC is not reported (“—”) due to dataset constraints.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison  \n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8947     | 0.8767   | Accuracy is slightly higher for females. |\n",
    "| **F1-Score**  | 0.7500     | 0.9072   | Males achieve much better balance between precision and recall. |\n",
    "| **FPR**       | 0.1250     | 0.2000   | Males are more likely to be incorrectly flagged as positive (higher false positives). |\n",
    "| **Precision** | 0.6000     | 0.8980   | Predictions for males are more reliable. |\n",
    "| **ROC AUC**   | 0.9818     | 0.8941   | Females benefit from stronger ranking performance. |\n",
    "| **TPR**       | 1.0000     | 0.9167   | Females have perfect sensitivity (no missed positives), while males have slightly lower sensitivity. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation  \n",
    "- **Females (unprivileged):**  \n",
    "  - Benefit from **higher accuracy, ROC AUC, and perfect sensitivity (TPR = 1.0)**.  \n",
    "  - However, they face **lower F1-score and precision**, meaning that while all true positives are caught, many false alarms reduce reliability.  \n",
    "\n",
    "- **Males (privileged):**  \n",
    "  - Achieve **higher F1 and precision**, showing more reliable predictions.  \n",
    "  - However, they are disadvantaged by **higher false positive rates** and **lower ROC AUC**, indicating weaker ranking performance compared to females.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "The Random Forest model presents a **mixed fairness picture**:  \n",
    "- **Females** gain from higher sensitivity and AUC, ensuring they are rarely missed.  \n",
    "- **Males** gain from better reliability (precision and F1), but at the cost of more false positives.  \n",
    "\n",
    "This indicates that the model distributes strengths and weaknesses differently across genders, leading to **trade-offs in fairness** rather than a clear bias toward one group.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a52946a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 1.0000\n",
      "  False Positive Rate (FPR): 0.1250\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9167\n",
      "  False Positive Rate (FPR): 0.2000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_rf == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_rf == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06d248",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Random Forest Model  \n",
    "\n",
    "To further examine fairness at the subgroup level, we compared the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.  \n",
    "\n",
    "#### Results by Gender Group  \n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 1.0000  | 0.1250  |\n",
    "| Privileged (Male)      | 0.9167  | 0.2000  |  \n",
    "\n",
    "#### Interpretation  \n",
    "\n",
    "- The **unprivileged group (female)** achieves a **perfect TPR (100%)**, meaning all true positive cases are correctly identified. However, their **FPR (12.5%)** indicates they are more likely to be incorrectly flagged as positive compared to an ideal outcome.  \n",
    "- The **privileged group (male)** shows a **slightly lower TPR (91.67%)**, which means some true positive cases are missed, but they face a **higher FPR (20.0%)**, resulting in more false alarms compared to females.  \n",
    "- This distribution suggests a **trade-off in errors**: females are guaranteed not to be missed (no false negatives), but at the cost of more false positives; males, on the other hand, benefit less from sensitivity but are disproportionately affected by false alarms.  \n",
    "\n",
    "#### Summary  \n",
    "\n",
    "The Random Forest model exhibits **asymmetric fairness trade-offs**:  \n",
    "- **Females (unprivileged)**: advantaged in sensitivity (perfect TPR) but slightly disadvantaged in false positives.  \n",
    "- **Males (privileged)**: disadvantaged in both sensitivity and false positives, leading to comparatively less favorable error distribution.  \n",
    "\n",
    "This outcome shows that the model does **not consistently favor one gender**, but rather creates **imbalanced error profiles across groups**, raising fairness concerns that need to be addressed.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136de6",
   "metadata": {},
   "source": [
    "### Deep Learning Model - Feed Forward Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c69d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true    y_prob  y_pred\n",
      "0    1       1  0.951479       1\n",
      "1    1       1  0.029132       0\n",
      "2    1       1  0.810705       1\n",
      "3    1       1  0.225562       0\n",
      "4    0       0  0.612828       1\n"
     ]
    }
   ],
   "source": [
    "mlp_df = pd.read_csv(\"HeartFailureData_75M25F_AdamMLP_predictions.csv\")\n",
    "print(mlp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f65a498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns \n",
    "y_true_mlp = mlp_df[\"y_true\"].values \n",
    "y_prob_mlp = mlp_df[\"y_prob\"].values\n",
    "y_pred_mlp = mlp_df[\"y_pred\"].values\n",
    "gender_mlp = mlp_df[\"Sex\"].values \n",
    "\n",
    "# Use gender_mlp as the protected attribute\n",
    "protected_attr_mlp = gender_mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6969bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0646\n",
      "               Balanced Accuracy Difference            0.0856\n",
      "               Balanced Accuracy Ratio                 1.1005\n",
      "               Disparate Impact Ratio                  0.4366\n",
      "               Equal Odds Difference                   0.1562\n",
      "               Equal Odds Ratio                        1.1852\n",
      "               Positive Predictive Parity Difference  -0.3205\n",
      "               Positive Predictive Parity Ratio        0.6519\n",
      "               Statistical Parity Difference          -0.3396\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "#Run fairmlhealth bias detection for MLP \n",
    "\n",
    "mlp_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp,\n",
    "    prtc_attr=protected_attr_mlp,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(mlp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbf7eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5def7_row0_col0, #T_5def7_row1_col0, #T_5def7_row3_col0, #T_5def7_row4_col0, #T_5def7_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5def7\">\n",
       "  <caption>MLP Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5def7_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5def7_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_5def7_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_5def7_row0_col0\" class=\"data row0 col0\" >0.0646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5def7_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_5def7_row1_col0\" class=\"data row1 col0\" >0.0856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5def7_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_5def7_row2_col0\" class=\"data row2 col0\" >1.1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5def7_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_5def7_row3_col0\" class=\"data row3 col0\" >0.4366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5def7_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_5def7_row4_col0\" class=\"data row4 col0\" >0.1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5def7_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_5def7_row5_col0\" class=\"data row5 col0\" >1.1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5def7_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_5def7_row6_col0\" class=\"data row6 col0\" >-0.3205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5def7_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_5def7_row7_col0\" class=\"data row7 col0\" >0.6519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5def7_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_5def7_row8_col0\" class=\"data row8 col0\" >-0.3396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5def7_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_5def7_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_5def7_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b074ae6ad0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for MLP\n",
    "styled_mlp = MyFlagger().apply_flag(\n",
    "    df=mlp_bias,\n",
    "    caption=\"MLP Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713caa5",
   "metadata": {},
   "source": [
    "## Fairness Evaluation – MLP by Gender\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics  \n",
    "\n",
    "- **AUC Difference (0.0646):** Indicates a moderate disparity in ranking performance across genders, with one group benefiting from slightly better discriminative ability.  \n",
    "- **Balanced Accuracy Difference (0.0856)** and **Ratio (1.1005):** Shows imbalances in sensitivity and specificity, with the model performing more favorably for one gender.  \n",
    "- **Disparate Impact Ratio (0.4366):** Well below the fairness threshold of **0.80–1.25**, revealing a substantial imbalance in selection rates, strongly disadvantaging one group.  \n",
    "- **Equal Odds Difference (0.1562)** and **Equal Odds Ratio (1.1852):** Reflect moderate disparities in error rates (TPR/FPR) across genders, though within the acceptable ratio range, indicating unequal error distribution.  \n",
    "- **Positive Predictive Parity Difference (−0.3205)** and **Ratio (0.6519):** Precision is notably lower for one group, meaning predictions are significantly less reliable for them.  \n",
    "- **Statistical Parity Difference (−0.3396):** Confirms that the disadvantaged group is selected far less often than the privileged group, reinforcing the presence of systematic bias.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation  \n",
    "\n",
    "- The MLP model exhibits **clear gender bias** across several fairness metrics.  \n",
    "- Disparate Impact and Statistical Parity strongly indicate that one gender (most likely females as the unprivileged group) is **under-selected and disadvantaged**.  \n",
    "- Error rate disparities (Equal Odds Difference) show that predictive performance is **not equally distributed**, with unprivileged individuals more likely to face unfair outcomes.  \n",
    "- Lower predictive precision for the disadvantaged group further highlights a **consistency problem in prediction reliability**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "\n",
    "The MLP model demonstrates **systematic bias against the unprivileged gender group**, reflected in:  \n",
    "- **Lower selection rates** (DIR = 0.4366).  \n",
    "- **Weaker precision and error rate fairness**.  \n",
    "- **Moderate ranking performance disparities**.  \n",
    "\n",
    "Overall, while the MLP achieves competitive accuracy, it introduces **significant fairness concerns**, particularly disadvantaging the unprivileged group in both selection outcomes and predictive reliability.  \n",
    "\n",
    "---  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aa66097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - MLP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0856</td>\n",
       "      <td>0.9087</td>\n",
       "      <td>0.015</td>\n",
       "      <td>1.1200</td>\n",
       "      <td>0.3205</td>\n",
       "      <td>1.5341</td>\n",
       "      <td>0.3396</td>\n",
       "      <td>2.2904</td>\n",
       "      <td>-0.1562</td>\n",
       "      <td>0.8438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0856</td>\n",
       "      <td>1.1005</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>-0.3205</td>\n",
       "      <td>0.6519</td>\n",
       "      <td>-0.3396</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>1.1852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                       -0.0856   \n",
       "1          Sex             1                        0.0856   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9087     0.015     1.1200    0.3205     1.5341   \n",
       "1                   1.1005    -0.015     0.8929   -0.3205     0.6519   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.3396           2.2904   -0.1562     0.8438  \n",
       "1         -0.3396           0.4366    0.1562     1.1852  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - MLP\")\n",
    "measure.bias(X_test, y_test, y_pred_mlp, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576976f",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – MLP by Gender  \n",
    "\n",
    "This table presents **group-specific fairness metrics** for the MLP model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy  \n",
    "- **Females (0):** Difference = **−0.0856**, Ratio = **0.9087**  \n",
    "- **Males (1):** Difference = **+0.0856**, Ratio = **1.1005**  \n",
    "- ➝ The model is **less accurate for females**, with better balanced accuracy for males.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)  \n",
    "- **Females (0):** FPR Diff = **+0.015**, Ratio = **1.1200**  \n",
    "- **Males (1):** FPR Diff = **−0.015**, Ratio = **0.8929**  \n",
    "- ➝ Females experience a **slightly higher false positive rate**, meaning they are more often incorrectly flagged.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)  \n",
    "- **Females (0):** PPV Diff = **+0.3205**, Ratio = **1.5341**  \n",
    "- **Males (1):** PPV Diff = **−0.3205**, Ratio = **0.6519**  \n",
    "- ➝ Predictions are **more reliable for females**, while males face reduced precision.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate  \n",
    "- **Females (0):** Selection Diff = **+0.3396**, Ratio = **2.2904**  \n",
    "- **Males (1):** Selection Diff = **−0.3396**, Ratio = **0.4366**  \n",
    "- ➝ Females are **selected more often**, while males are **under-selected** by the model.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)  \n",
    "- **Females (0):** TPR Diff = **−0.1562**, Ratio = **0.8438**  \n",
    "- **Males (1):** TPR Diff = **+0.1562**, Ratio = **1.1852**  \n",
    "- ➝ The model detects **male cases more effectively**, with females facing a risk of missed true positives.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "The MLP model exhibits **mixed fairness outcomes**:  \n",
    "- **Females** benefit from **higher selection rates and precision**, but face **more false positives** and a **lower true positive rate**.  \n",
    "- **Males** are disadvantaged in terms of **precision and selection**, but benefit from **higher sensitivity and balanced accuracy**.  \n",
    "\n",
    "Overall, the MLP introduces **trade-offs**: it favors females in terms of being selected and prediction reliability, but it **favors males in sensitivity and balanced accuracy**, leading to **asymmetric bias across metrics**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbc2caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.8587</td>\n",
       "      <td>0.8700</td>\n",
       "      <td>0.1341</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8878</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.8529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.2632</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>0.8493</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9205</td>\n",
       "      <td>0.9042</td>\n",
       "      <td>0.8438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.5326    0.8587   \n",
       "1           Sex             0   38.0       0.1579           0.2632    0.8947   \n",
       "2           Sex             1  146.0       0.6575           0.6027    0.8493   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8700  0.1341      —     0.8878   0.9139  0.8529  \n",
       "1    0.7500  0.1250      —     0.6000   0.9688  1.0000  \n",
       "2    0.8804  0.1400      —     0.9205   0.9042  0.8438  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_mlp = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_mlp = perf_table_mlp.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec7c2f",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – MLP by Gender  \n",
    "\n",
    "This table shows the **stratified performance metrics** of the MLP model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)  \n",
    "- **Accuracy (0.8587)** and **F1-score (0.8700)** suggest strong general predictive performance.  \n",
    "- **ROC AUC (0.9139)** indicates very good discriminative ability.  \n",
    "- **Precision (0.8878)** is high, reflecting reliable positive predictions.  \n",
    "- **TPR (0.8529)** shows good sensitivity overall, though group disparities are present.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison  \n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8947     | 0.8493   | Accuracy is higher for females, with males performing slightly worse. |\n",
    "| **F1-Score**  | 0.7500     | 0.8804   | Males achieve better balance between precision and recall. |\n",
    "| **FPR**       | 0.1250     | 0.1400   | False positives are slightly higher for males. |\n",
    "| **Precision** | 0.6000     | 0.9205   | Predictions are much more reliable for males, while females face many false alarms. |\n",
    "| **ROC AUC**   | 0.9688     | 0.9042   | Ranking performance is better for females. |\n",
    "| **TPR**       | 1.0000     | 0.8438   | Sensitivity is perfect for females but weaker for males. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation  \n",
    "\n",
    "- **Females (unprivileged):**  \n",
    "  - Benefit from **higher accuracy (89.5%)** and **perfect sensitivity (TPR = 1.0)**, meaning no true positive cases were missed.  \n",
    "  - However, precision is **much lower (0.6000)**, showing a high false alarm rate despite strong detection ability.  \n",
    "\n",
    "- **Males (privileged):**  \n",
    "  - Have **higher precision (0.9205)** and a better F1-score, making their predictions more balanced and reliable.  \n",
    "  - But sensitivity (TPR = 0.8438) is lower, meaning some true cases are missed compared to females.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "\n",
    "The MLP model exhibits **asymmetric performance trade-offs**:  \n",
    "- **Females** are almost always correctly detected (high TPR) but with poor precision, leading to many false positives.  \n",
    "- **Males** enjoy much more reliable predictions (high precision and F1) but face a **higher risk of missed detections**.  \n",
    "\n",
    "This indicates that the MLP introduces **different types of bias**: females are over-flagged (false alarms), while males are under-detected (missed cases).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379dbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 1.0000\n",
      "  False Positive Rate (FPR): 0.1250\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.8438\n",
      "  False Positive Rate (FPR): 0.1400\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_mlp == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_mlp == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fea8d1",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – MLP Model  \n",
    "\n",
    "To further examine subgroup fairness, we compare the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** between the unprivileged (female) and privileged (male) groups.  \n",
    "\n",
    "#### Results by Gender Group  \n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 1.0000  | 0.1250  |\n",
    "| Privileged (Male)      | 0.8438  | 0.1400  |\n",
    "\n",
    "#### Interpretation  \n",
    "\n",
    "- **Females (unprivileged)** achieve a **perfect TPR (100%)**, meaning no positive cases were missed. However, their **FPR (12.5%)** indicates a moderate level of false alarms.  \n",
    "- **Males (privileged)** show a **lower TPR (84.38%)**, meaning some positive cases are missed, while their **FPR (14.0%)** is slightly higher than for females, suggesting more false positives.  \n",
    "- Overall, the model is **more sensitive for females**, detecting all true cases, but at the cost of moderate false positives. Males face both a **higher chance of being missed (lower TPR)** and **slightly more false alarms (higher FPR)**.  \n",
    "\n",
    "#### Summary  \n",
    "\n",
    "The MLP model introduces a **reverse bias pattern**, favoring **females (unprivileged group)** in terms of detection accuracy. Females benefit from **perfect sensitivity**, while males are disadvantaged by lower sensitivity and marginally higher error rates.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
