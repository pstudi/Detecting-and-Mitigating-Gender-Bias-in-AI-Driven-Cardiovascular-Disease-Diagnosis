{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba0b2b9",
   "metadata": {},
   "source": [
    "### Bias Detection and Fairness Evaluation on Cardiovascular Disease (Kaggle) using FairMLhealth\n",
    "(Source: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c29a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load X_test set\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57b21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup successful\n"
     ]
    }
   ],
   "source": [
    "import fairmlhealth\n",
    "import aif360\n",
    "print(\"Environment setup successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f126c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fairmlhealth\n",
      "Version: 1.0.2\n",
      "Summary: Health-centered variation analysis\n",
      "Home-page: https://github.com/KenSciResearch/fairMLHealth\n",
      "Author: Christine Allen\n",
      "Author-email: ca.magallen@gmail.com\n",
      "License: \n",
      "Location: c:\\users\\patri\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: aif360, ipython, jupyter, numpy, pandas, requests, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#have a look at the details of fairmlhealth - especially the version\n",
    "!pip show fairmlhealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76186324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "#have a look at the modules that are within fairmlhealth\n",
    "\n",
    "print(dir(fairmlhealth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44657ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "#load necessary modules \n",
    "\n",
    "#import module measure to use measure.summary for bias detection\n",
    "from fairmlhealth import measure\n",
    "\n",
    "#import module for investigation of individual cohorts \n",
    "from fairmlhealth.__utils import iterate_cohorts\n",
    "\n",
    "#import FairRanges to flag high values\n",
    "from fairmlhealth.__utils import FairRanges\n",
    "\n",
    "# Wrap the fairness summary function for cohort-wise analysis\n",
    "@iterate_cohorts\n",
    "def cohort_summary(**kwargs):\n",
    "    return measure.summary(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb6c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"inFairness\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"AdversarialDebiasing will be unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a099295",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models - KNN & DT\n",
    "\n",
    "#### K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d817251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true    y_prob  y_pred\n",
      "0       0       0  0.517241       1\n",
      "1       0       0  0.793103       1\n",
      "2       1       0  0.413793       0\n",
      "3       0       0  0.275862       0\n",
      "4       0       0  0.172414       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "knn_df = pd.read_csv(\"CVDKaggleData_50F50M__tunedKNN_predictions.csv\")\n",
    "\n",
    "print(knn_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871aa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_knn = knn_df[\"y_true\"].values\n",
    "y_prob_knn = knn_df[\"y_prob\"].values\n",
    "y_pred_knn = knn_df[\"y_pred\"].values\n",
    "gender_knn = knn_df[\"gender\"].values\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_knn = gender_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad78836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0070\n",
      "               Balanced Accuracy Difference           -0.0022\n",
      "               Balanced Accuracy Ratio                 0.9968\n",
      "               Disparate Impact Ratio                  1.0142\n",
      "               Equal Odds Difference                   0.0060\n",
      "               Equal Odds Ratio                        1.0200\n",
      "               Positive Predictive Parity Difference   0.0031\n",
      "               Positive Predictive Parity Ratio        1.0045\n",
      "               Statistical Parity Difference           0.0071\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "knn_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_true_knn,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn,\n",
    "    prtc_attr=protected_attr_knn,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,   # skip inconsistency metrics that cause NearestNeighbors error\n",
    "    skip_performance=True\n",
    ")\n",
    "\n",
    "print(knn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ba3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Custom scenario oriented bounds\n",
    "\n",
    "custom_ranges = {\n",
    "    \"tpr diff\": (-0.03, 0.03),\n",
    "    \"fpr diff\": (-0.03, 0.03),\n",
    "    \"equal odds difference\": (-0.04, 0.04),\n",
    "    \"statistical parity difference\": (-0.05, 0.05),\n",
    "    \"disparate impact ratio\": (0.9, 1.1),\n",
    "    \"selection ratio\": (0.9, 1.1),\n",
    "    \"auc difference\": (-0.02, 0.02),\n",
    "    \"balanced accuracy difference\": (-0.02, 0.02),\n",
    "}\n",
    "\n",
    "bounds = FairRanges().load_fair_ranges(custom_ranges=custom_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14d2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  restore Styler.set_precision to adjust the highlighting color in the styled table\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "Styler = type(pd.DataFrame({\"_\":[0]}).style)  \n",
    "\n",
    "if not hasattr(Styler, \"set_precision\"):\n",
    "    def _set_precision(self, precision=4):\n",
    "        try:\n",
    "            return self.format(precision=precision)\n",
    "        except TypeError:\n",
    "            return self.format(formatter=lambda x:\n",
    "                f\"{x:.{precision}g}\" if isinstance(x, (int, float, np.floating)) else x\n",
    "            )\n",
    "    setattr(Styler, \"set_precision\", _set_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90435472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6dedd\">\n",
       "  <caption>KNN Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6dedd_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6dedd_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_6dedd_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_6dedd_row0_col0\" class=\"data row0 col0\" >0.0070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dedd_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_6dedd_row1_col0\" class=\"data row1 col0\" >-0.0022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dedd_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_6dedd_row2_col0\" class=\"data row2 col0\" >0.9968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dedd_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_6dedd_row3_col0\" class=\"data row3 col0\" >1.0142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dedd_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_6dedd_row4_col0\" class=\"data row4 col0\" >0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dedd_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_6dedd_row5_col0\" class=\"data row5 col0\" >1.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dedd_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_6dedd_row6_col0\" class=\"data row6 col0\" >0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dedd_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_6dedd_row7_col0\" class=\"data row7 col0\" >1.0045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dedd_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_6dedd_row8_col0\" class=\"data row8 col0\" >0.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6dedd_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_6dedd_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_6dedd_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1af68a3d180>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "from fairmlhealth.__utils import Flagger\n",
    "\n",
    "class MyFlagger(Flagger):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.flag_color = \"#491ee6\"   \n",
    "        self.flag_type = \"background-color\"\n",
    "\n",
    "styled_knn = MyFlagger().apply_flag(\n",
    "    df=knn_bias,\n",
    "    caption=\"KNN Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0731b8f",
   "metadata": {},
   "source": [
    "### Interpretation of KNN Fairness Metrics by Gender\n",
    "\n",
    "The table reports fairness metrics for the **K-Nearest Neighbors (KNN) model**, using gender as the sensitive attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0070):**  \n",
    "  → Very small; ranking performance (ability to separate positives from negatives) is nearly identical across genders.\n",
    "\n",
    "- **Balanced Accuracy Difference (-0.0022)** and **Ratio (0.9968):**  \n",
    "  → Almost no difference in balanced accuracy; the ratio close to 1 confirms that both genders are treated equally well.\n",
    "\n",
    "- **Disparate Impact Ratio (1.0142):**  \n",
    "  → Very close to 1, indicating that the likelihood of receiving a positive prediction is nearly the same across genders.  \n",
    "  → This is well within the fairness guideline range (0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (0.0060)** and **Equal Odds Ratio (1.0200):**  \n",
    "  → Error rates (true positive rate and false positive rate) are very similar, with only a marginal difference between genders.\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0031)** and **Ratio (1.0045):**  \n",
    "  → Precision is nearly identical, with males having a negligible advantage.\n",
    "\n",
    "- **Statistical Parity Difference (0.0071):**  \n",
    "  → Suggests a very small difference in the overall rate of positive predictions across genders, but the value is close to zero.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → The privileged group (males) makes up 35% of the dataset. This imbalance is present in the data but does not cause strong disparities in fairness outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The KNN model shows **very strong fairness across gender groups**:  \n",
    "- All differences are **extremely small** (≤0.01), and ratios remain close to 1.  \n",
    "- Both genders receive nearly equal treatment in terms of accuracy, prediction rates, and error distribution.  \n",
    "- The results suggest that the model is **highly balanced and does not exhibit systematic gender bias**.\n",
    "\n",
    "Among fairness evaluations, this KNN run demonstrates **excellent parity between groups**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0eeae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - KNN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>1.0032</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>-0.0071</td>\n",
       "      <td>0.9860</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>0.9977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1.0200</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>1.0045</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>1.0142</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>1.0023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                        0.0022   \n",
       "1       gender             1                       -0.0022   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0032    -0.006     0.9804   -0.0031     0.9955   \n",
       "1                   0.9968     0.006     1.0200    0.0031     1.0045   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0         -0.0071           0.9860   -0.0016     0.9977  \n",
       "1          0.0071           1.0142    0.0016     1.0023  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - KNN\")\n",
    "measure.bias(X_test, y_test, y_pred_knn, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7770fe",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – KNN (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **K-Nearest Neighbors (KNN) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **0.0022**, Ratio = **1.0032**  \n",
    "- **Male (1):** Difference = **-0.0022**, Ratio = **0.9968**  \n",
    "➡️ Balanced accuracy is **nearly identical** for both genders, with only a ±0.2% deviation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **-0.006**, Ratio = **0.9804**  \n",
    "- **Male (1):** FPR Diff = **0.006**, Ratio = **1.0200**  \n",
    "➡️ Females experience a **slightly lower false positive rate**, while males face marginally more false positives.  \n",
    "The difference is negligible (~0.6%).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0031**, Ratio = **0.9955**  \n",
    "- **Male (1):** PPV Diff = **0.0031**, Ratio = **1.0045**  \n",
    "➡️ Males have a **slight precision advantage**, but the difference is minimal (<0.4%).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **-0.0071**, Ratio = **0.9860**  \n",
    "- **Male (1):** Selection Diff = **0.0071**, Ratio = **1.0142**  \n",
    "➡️ Males are **slightly more likely** to be predicted positive than females, but the difference is very small (~0.7%).\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **-0.0016**, Ratio = **0.9977**  \n",
    "- **Male (1):** TPR Diff = **0.0016**, Ratio = **1.0023**  \n",
    "➡️ Recall is **almost identical** across genders, with only a negligible advantage for males.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Benefit from a **slightly lower false positive rate**, but are marginally less likely to be predicted positive and have a very small disadvantage in precision/recall.  \n",
    "- **Males (1):** Enjoy a **tiny advantage in precision, recall, and selection rate**, but face a slightly higher false positive rate.  \n",
    "\n",
    "All differences are **extremely small (≤ 1%)**, meaning the KNN model is **highly fair across gender groups** with no meaningful systematic bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fdeff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.5052</td>\n",
       "      <td>0.7007</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>0.3054</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6963</td>\n",
       "      <td>0.7618</td>\n",
       "      <td>0.7068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.5076</td>\n",
       "      <td>0.6999</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>0.3075</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6974</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.7074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.7021</td>\n",
       "      <td>0.6999</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6942</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>0.7058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.5052   \n",
       "1        gender             0   7362.0       0.5004           0.5076   \n",
       "2        gender             1   3894.0       0.4923           0.5005   \n",
       "\n",
       "   Accuracy  F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7007    0.7015  0.3054      —     0.6963   0.7618  0.7068  \n",
       "1    0.6999    0.7023  0.3075      —     0.6974   0.7639  0.7074  \n",
       "2    0.7021    0.6999  0.3015      —     0.6942   0.7569  0.7058  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fairmlhealth import measure\n",
    "import pandas as pd\n",
    "from IPython.display import display  \n",
    "\n",
    "# convert gender into DataFrame with a clear column name to get a nice table as output\n",
    "gender_df = pd.DataFrame({\"gender\": X_test[\"gender\"].astype(int)})\n",
    "\n",
    "\n",
    "# Get the stratified table\n",
    "perf_table_knn = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_knn = perf_table_knn.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ea758",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – KNN (by Gender)\n",
    "\n",
    "The table shows performance results for the **K-Nearest Neighbors (KNN) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7007  \n",
    "- **F1-Score:** 0.7015  \n",
    "- **Precision:** 0.6963  \n",
    "- **ROC AUC:** 0.7618  \n",
    "- **TPR (Recall):** 0.7068  \n",
    "→ The KNN model achieves **solid and balanced predictive performance**, with a good balance of recall and precision.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.6999 (very close to males)  \n",
    "- **F1-Score:** 0.7023 (slightly higher than males)  \n",
    "- **FPR:** 0.3075 (slightly higher than males → more false positives)  \n",
    "- **Precision:** 0.6974 (slightly higher than males)  \n",
    "- **ROC AUC:** 0.7639 (slightly better discrimination ability)  \n",
    "- **TPR (Recall):** 0.7074 (almost identical to males)  \n",
    "\n",
    "➡️ Females show a **tiny advantage in F1-score, precision, and ROC AUC**, but this comes with a **slightly higher false positive rate**.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.7021 (slightly higher than females)  \n",
    "- **F1-Score:** 0.6999 (slightly lower than females)  \n",
    "- **FPR:** 0.3015 (lower than females → fewer false positives)  \n",
    "- **Precision:** 0.6942 (slightly lower than females)  \n",
    "- **ROC AUC:** 0.7569 (slightly lower discrimination ability)  \n",
    "- **TPR (Recall):** 0.7058 (almost identical to females)  \n",
    "\n",
    "➡️ Males perform **slightly better in accuracy and false positive rate**, but lag behind in F1-score, precision, and ROC AUC.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Slight advantage in **F1-score, precision, and ROC AUC**, but at the cost of a slightly higher false positive rate.  \n",
    "- **Males (1):** Slight advantage in **accuracy and lower false positive rate**, but at the cost of slightly lower F1-score and precision.  \n",
    "\n",
    "The differences are **tiny (≤ 1%)**, indicating that the KNN model is **very fair across gender groups**, with only negligible trade-offs in error distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5db1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.7074\n",
      "  False Positive Rate (FPR): 0.3075\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.7058\n",
      "  False Positive Rate (FPR): 0.3015\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#group specific error analysis\n",
    "\n",
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_knn == 0)  # unprivileged group (female)\n",
    "male_mask   = (protected_attr_knn == 1)  # privileged group (male)\n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74986e6d",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – KNN Model\n",
    "\n",
    "This section reports the classification performance of the KNN model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.7074  | 0.3075  |\n",
    "| Male (Privileged = 1)        | 0.7058  | 0.3015  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Have a **TPR of 70.74%**, almost identical to males.  \n",
    "  - Experience a **slightly higher FPR (30.75%)**, meaning they receive marginally more false positives.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Have a **TPR of 70.58%**, virtually the same as females.  \n",
    "  - Benefit from a **slightly lower FPR (30.15%)**, meaning fewer false alarms.\n",
    "\n",
    "#### Conclusion\n",
    "The KNN model shows **very balanced fairness** across genders:  \n",
    "- Both groups achieve nearly identical recall (TPR).  \n",
    "- Females incur a **tiny disadvantage in terms of false positives**, but the gap (~0.6%) is negligible.  \n",
    "\n",
    "Overall, the results indicate that the KNN model is **highly fair and unbiased with respect to gender**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb317ca",
   "metadata": {},
   "source": [
    "### Decision Tree - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adbaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_dt    y_prob\n",
      "0       0       0          0  0.301981\n",
      "1       0       0          1  0.862548\n",
      "2       1       0          0  0.301981\n",
      "3       0       0          0  0.301981\n",
      "4       0       0          0  0.301981\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "dt_df = pd.read_csv(\"CVDKaggleData_50F50M_DT_tunedpruned_predictions.csv\")\n",
    "\n",
    "print(dt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06530601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract common columns\n",
    "y_true_dt = dt_df[\"y_true\"].values\n",
    "y_prob_dt = dt_df[\"y_prob\"].values\n",
    "y_pred_dt = dt_df[\"y_pred_dt\"].values\n",
    "gender_dt = dt_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_dt = gender_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b83e2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Decision Tree Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0047\n",
      "               Balanced Accuracy Difference            0.0069\n",
      "               Balanced Accuracy Ratio                 1.0098\n",
      "               Disparate Impact Ratio                  0.9489\n",
      "               Equal Odds Difference                  -0.0357\n",
      "               Equal Odds Ratio                        0.8775\n",
      "               Positive Predictive Parity Difference   0.0268\n",
      "               Positive Predictive Parity Ratio        1.0382\n",
      "               Statistical Parity Difference          -0.0254\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Gender Bias Report\n",
    "print(\"\\n--- Decision Tree Gender Bias Report ---\")\n",
    "\n",
    "dt_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt,\n",
    "    prtc_attr=protected_attr_dt,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,  \n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(dt_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc2d88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_22680\">\n",
       "  <caption>Decision Tree Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_22680_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_22680_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_22680_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_22680_row0_col0\" class=\"data row0 col0\" >0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22680_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_22680_row1_col0\" class=\"data row1 col0\" >0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22680_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_22680_row2_col0\" class=\"data row2 col0\" >1.0098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22680_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_22680_row3_col0\" class=\"data row3 col0\" >0.9489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22680_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_22680_row4_col0\" class=\"data row4 col0\" >-0.0357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22680_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_22680_row5_col0\" class=\"data row5 col0\" >0.8775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22680_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_22680_row6_col0\" class=\"data row6 col0\" >0.0268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22680_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_22680_row7_col0\" class=\"data row7 col0\" >1.0382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22680_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_22680_row8_col0\" class=\"data row8 col0\" >-0.0254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22680_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_22680_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_22680_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1af68c154b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "styled_dt = MyFlagger().apply_flag(\n",
    "    df=dt_bias,\n",
    "    caption=\"Decision Tree Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2830b1e",
   "metadata": {},
   "source": [
    "### Interpretation of Decision Tree Fairness Metrics by Gender\n",
    "\n",
    "The table reports fairness metrics for the **Decision Tree (DT) model**, using gender as the sensitive attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0047):**  \n",
    "  → Extremely small; ranking ability is nearly identical between genders.\n",
    "\n",
    "- **Balanced Accuracy Difference (0.0069)** and **Ratio (1.0098):**  \n",
    "  → Indicates a very slight advantage in balanced accuracy for one gender, but the gap is minimal (~0.7%).\n",
    "\n",
    "- **Disparate Impact Ratio (0.9489):**  \n",
    "  → Slightly below 1, suggesting that females (unprivileged group) are **less likely to receive positive predictions** compared to males.  \n",
    "  → Still within the fairness guideline range (0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (-0.0357)** and **Equal Odds Ratio (0.8775):**  \n",
    "  → This is the **largest disparity** observed.  \n",
    "  - The negative difference suggests females have an advantage in error distribution (lower FPR or higher TPR).  \n",
    "  - The ratio indicates that males may face relatively higher error rates.  \n",
    "  → This shows some imbalance, but the difference (~3.6%) is still moderate.\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0268)** and **Ratio (1.0382):**  \n",
    "  → Males have slightly **higher precision**, meaning their positive predictions are somewhat more reliable.\n",
    "\n",
    "- **Statistical Parity Difference (-0.0254):**  \n",
    "  → Suggests females are **predicted positive at a slightly lower rate** than males, though the disparity is small.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → Males represent 35% of the dataset. Despite this imbalance, fairness outcomes remain reasonably balanced.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The Decision Tree model shows **generally fair outcomes across gender groups**, but with a few **minor disparities**:\n",
    "- **Females (unprivileged group):** Slightly lower rate of positive predictions but benefit from more favorable error distribution.  \n",
    "- **Males (privileged group):** Gain a small advantage in precision but appear slightly disadvantaged in equalized odds.  \n",
    "\n",
    "Overall, disparities remain **moderate and within acceptable ranges**, though the **Equal Odds metric (-0.0357)** indicates the largest imbalance compared to other fairness measures.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "631b7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - DT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0069</td>\n",
       "      <td>0.9903</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>1.1395</td>\n",
       "      <td>-0.0268</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>1.0538</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>1.0318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>1.0098</td>\n",
       "      <td>-0.0357</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>1.0382</td>\n",
       "      <td>-0.0254</td>\n",
       "      <td>0.9489</td>\n",
       "      <td>-0.0219</td>\n",
       "      <td>0.9692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0069   \n",
       "1       gender             1                        0.0069   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9903    0.0357     1.1395   -0.0268     0.9632   \n",
       "1                   1.0098   -0.0357     0.8775    0.0268     1.0382   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0254           1.0538    0.0219     1.0318  \n",
       "1         -0.0254           0.9489   -0.0219     0.9692  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - DT\")\n",
    "measure.bias(X_test, y_test, y_pred_dt, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b884a10",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – Decision Tree (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **Decision Tree (DT) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **-0.0069**, Ratio = **0.9903**  \n",
    "- **Male (1):** Difference = **0.0069**, Ratio = **1.0098**  \n",
    "➡️ Males have a **slight advantage in balanced accuracy** (~0.7%), but the difference is very small.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **0.0357**, Ratio = **1.1395**  \n",
    "- **Male (1):** FPR Diff = **-0.0357**, Ratio = **0.8775**  \n",
    "➡️ Females face a **higher false positive rate**, meaning they are more often incorrectly classified as positive.  \n",
    "➡️ Males benefit from fewer false positives.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0268**, Ratio = **0.9632**  \n",
    "- **Male (1):** PPV Diff = **0.0268**, Ratio = **1.0382**  \n",
    "➡️ Precision is **higher for males**, meaning their positive predictions are more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **0.0254**, Ratio = **1.0538**  \n",
    "- **Male (1):** Selection Diff = **-0.0254**, Ratio = **0.9489**  \n",
    "➡️ Females are **more likely to be predicted positive**, while males are predicted positive less often.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **0.0219**, Ratio = **1.0318**  \n",
    "- **Male (1):** TPR Diff = **-0.0219**, Ratio = **0.9692**  \n",
    "➡️ Females achieve **higher recall**, meaning they are more likely to have their true positives correctly identified.  \n",
    "➡️ Males experience slightly more false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):**  \n",
    "  - Advantages: **Higher recall (TPR)** and **higher selection rate**.  \n",
    "  - Disadvantages: **Higher false positive rate** and **lower precision**.  \n",
    "\n",
    "- **Males (1):**  \n",
    "  - Advantages: **Lower false positive rate** and **higher precision**.  \n",
    "  - Disadvantages: **Lower recall** and **less likely to be predicted positive**.  \n",
    "\n",
    "The disparities are **moderate**: females trade precision for recall (more positives detected but with more false alarms), while males enjoy more reliable predictions but risk missing true cases. This suggests the Decision Tree model introduces **noticeable but not extreme fairness trade-offs across genders**.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1acccc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.4808</td>\n",
       "      <td>0.7133</td>\n",
       "      <td>0.7070</td>\n",
       "      <td>0.2686</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7193</td>\n",
       "      <td>0.7394</td>\n",
       "      <td>0.6951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.4720</td>\n",
       "      <td>0.7157</td>\n",
       "      <td>0.7076</td>\n",
       "      <td>0.2561</td>\n",
       "      <td>0.2609</td>\n",
       "      <td>0.7289</td>\n",
       "      <td>0.7410</td>\n",
       "      <td>0.6876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7058</td>\n",
       "      <td>0.2919</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7021</td>\n",
       "      <td>0.7364</td>\n",
       "      <td>0.7094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.4808   \n",
       "1        gender             0   7362.0       0.5004           0.4720   \n",
       "2        gender             1   3894.0       0.4923           0.4974   \n",
       "\n",
       "   Accuracy  F1-Score     FPR  PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7133    0.7070  0.2686       —     0.7193   0.7394  0.6951  \n",
       "1    0.7157    0.7076  0.2561  0.2609     0.7289   0.7410  0.6876  \n",
       "2    0.7088    0.7058  0.2919       —     0.7021   0.7364  0.7094  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_dt = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_dt = perf_table_dt.fillna(\"—\")\n",
    "\n",
    "# Display pretty table\n",
    "display(perf_table_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a06dc",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – Decision Tree (by Gender)\n",
    "\n",
    "The table shows performance results for the **Decision Tree model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7133  \n",
    "- **F1-Score:** 0.7070  \n",
    "- **Precision:** 0.7193  \n",
    "- **ROC AUC:** 0.7394  \n",
    "- **TPR (Recall):** 0.6951  \n",
    "→ The DT achieves **balanced predictive performance**, with moderate recall and relatively good precision.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.7157 (slightly higher than males)  \n",
    "- **F1-Score:** 0.7076 (slightly higher)  \n",
    "- **FPR:** 0.2561 (lower than males → fewer false positives)  \n",
    "- **Precision:** 0.7289 (higher precision than males)  \n",
    "- **ROC AUC:** 0.7410 (slightly higher discrimination ability)  \n",
    "- **TPR (Recall):** 0.6876 (lower than males)  \n",
    "\n",
    "➡️ Females show **stronger performance in precision, accuracy, and fewer false positives**, but **recall is slightly weaker** compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.7088 (slightly lower than females)  \n",
    "- **F1-Score:** 0.7058 (slightly lower)  \n",
    "- **FPR:** 0.2919 (higher → more false positives)  \n",
    "- **Precision:** 0.7021 (lower precision than females)  \n",
    "- **ROC AUC:** 0.7364 (slightly weaker discrimination ability)  \n",
    "- **TPR (Recall):** 0.7094 (higher than females)  \n",
    "\n",
    "➡️ Males achieve **better recall (higher TPR)**, meaning more true positives are detected, but this comes with **more false positives** and lower precision.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Advantage in **precision, accuracy, ROC AUC, and lower false positive rate**, but recall is weaker.  \n",
    "- **Males (1):** Advantage in **recall (TPR)**, but face more false positives and lower precision.  \n",
    "\n",
    "The Decision Tree model shows a **clear trade-off**:  \n",
    "- **Females** → more reliable predictions (fewer false alarms, higher precision).  \n",
    "- **Males** → more sensitive predictions (higher recall), but with more false positives.  \n",
    "\n",
    "The differences are noticeable but not extreme, reflecting **moderate gender trade-offs** rather than systematic bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53556439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6876\n",
      "  False Positive Rate (FPR): 0.2561\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.7094\n",
      "  False Positive Rate (FPR): 0.2919\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_dt == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_dt == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e7685",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Decision Tree Model\n",
    "\n",
    "This section reports the classification performance of the Decision Tree model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.6876  | 0.2561  |\n",
    "| Male (Privileged = 1)        | 0.7094  | 0.2919  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Achieve a **TPR of 68.76%**, slightly lower than males, meaning fewer true positives are detected.  \n",
    "  - Benefit from a **lower FPR (25.61%)**, which means they are less frequently misclassified as positive.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Achieve a **TPR of 70.94%**, indicating better sensitivity (more true positives detected).  \n",
    "  - However, they also face a **higher FPR (29.19%)**, meaning they are more often falsely flagged as positive.\n",
    "\n",
    "#### Conclusion\n",
    "The Decision Tree model demonstrates a **trade-off in error distribution**:  \n",
    "- **Males** → better recall (higher TPR), but at the cost of more false positives.  \n",
    "- **Females** → fewer false positives, but with slightly weaker recall.  \n",
    "\n",
    "The differences (≈2% in TPR and ≈3.5% in FPR) are **moderate but not extreme**, indicating that the model remains **reasonably fair**, though it slightly favors males in sensitivity and females in prediction reliability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5168d",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaa2a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_rf    y_prob\n",
      "0       0       0          0  0.363411\n",
      "1       0       0          1  0.812063\n",
      "2       1       0          0  0.313309\n",
      "3       0       0          0  0.276039\n",
      "4       0       0          0  0.319559\n"
     ]
    }
   ],
   "source": [
    "rf_df = pd.read_csv(\"CVDKaggleData_50M50F_RF_tuned_predictions.csv\")\n",
    "print(rf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "278af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_rf = rf_df[\"y_true\"].values\n",
    "y_pred_rf = rf_df[\"y_pred_rf\"].values\n",
    "y_prob_rf = rf_df[\"y_prob\"].values\n",
    "gender_rf = rf_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_rf = gender_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "689f0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0137\n",
      "               Balanced Accuracy Difference            0.0026\n",
      "               Balanced Accuracy Ratio                 1.0037\n",
      "               Disparate Impact Ratio                  0.9674\n",
      "               Equal Odds Difference                  -0.0215\n",
      "               Equal Odds Ratio                        0.9209\n",
      "               Positive Predictive Parity Difference   0.0183\n",
      "               Positive Predictive Parity Ratio        1.0258\n",
      "               Statistical Parity Difference          -0.0155\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Gender Bias Report\n",
    "print(\"\\n--- Random Forest Gender Bias Report ---\")\n",
    "\n",
    "rf_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf,\n",
    "    prtc_attr=protected_attr_rf,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(rf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7a8e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_978a5\">\n",
       "  <caption>Random Forest Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_978a5_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_978a5_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_978a5_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_978a5_row0_col0\" class=\"data row0 col0\" >0.0137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_978a5_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_978a5_row1_col0\" class=\"data row1 col0\" >0.0026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_978a5_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_978a5_row2_col0\" class=\"data row2 col0\" >1.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_978a5_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_978a5_row3_col0\" class=\"data row3 col0\" >0.9674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_978a5_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_978a5_row4_col0\" class=\"data row4 col0\" >-0.0215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_978a5_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_978a5_row5_col0\" class=\"data row5 col0\" >0.9209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_978a5_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_978a5_row6_col0\" class=\"data row6 col0\" >0.0183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_978a5_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_978a5_row7_col0\" class=\"data row7 col0\" >1.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_978a5_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_978a5_row8_col0\" class=\"data row8 col0\" >-0.0155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_978a5_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_978a5_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_978a5_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1af68c823e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for Random Forest\n",
    "styled_rf = MyFlagger().apply_flag(\n",
    "    df=rf_bias,\n",
    "    caption=\"Random Forest Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c96281",
   "metadata": {},
   "source": [
    "### Interpretation of Random Forest Fairness Metrics by Gender\n",
    "\n",
    "The table reports fairness metrics for the **Random Forest (RF) model**, using gender as the sensitive attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0137):**  \n",
    "  → Very small; the model’s ability to rank predictions is nearly identical across genders.\n",
    "\n",
    "- **Balanced Accuracy Difference (0.0026)** and **Ratio (1.0037):**  \n",
    "  → Almost no disparity in balanced accuracy. Both genders are treated similarly well.\n",
    "\n",
    "- **Disparate Impact Ratio (0.9674):**  \n",
    "  → Slightly below 1, suggesting that females are **somewhat less likely to receive positive predictions** compared to males.  \n",
    "  → Still falls within the general fairness guideline (0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (-0.0215)** and **Equal Odds Ratio (0.9209):**  \n",
    "  → Indicates a moderate imbalance in error distribution (TPR/FPR).  \n",
    "  - The negative difference suggests **females may be slightly advantaged** in terms of lower error rates.  \n",
    "  - The ratio below 1 shows some discrepancy between genders.\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0183)** and **Ratio (1.0258):**  \n",
    "  → Precision is slightly higher for males, meaning their positive predictions are somewhat more reliable.\n",
    "\n",
    "- **Statistical Parity Difference (-0.0155):**  \n",
    "  → Indicates females are **predicted positive at a slightly lower rate** than males, but the disparity is small.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → Males represent 35% of the dataset. Despite being the minority, the fairness outcomes remain relatively balanced.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The Random Forest model demonstrates **fairness with only small disparities**:  \n",
    "- **Females (unprivileged group):** Slightly less likely to receive positive predictions, but they may experience somewhat more favorable error distribution.  \n",
    "- **Males (privileged group):** Enjoy a small advantage in **precision** (positive predictions are more reliable).  \n",
    "\n",
    "The **largest observed gap** is in **Equal Odds Difference (-0.0215)**, suggesting a small trade-off in error distribution. However, all disparities are **minor and within acceptable ranges**, indicating the model is **reasonably fair across genders**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47479386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - RF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>1.0859</td>\n",
       "      <td>-0.0183</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>1.0337</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>1.0243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>1.0037</td>\n",
       "      <td>-0.0215</td>\n",
       "      <td>0.9209</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>1.0258</td>\n",
       "      <td>-0.0155</td>\n",
       "      <td>0.9674</td>\n",
       "      <td>-0.0163</td>\n",
       "      <td>0.9763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0026   \n",
       "1       gender             1                        0.0026   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9963    0.0215     1.0859   -0.0183     0.9748   \n",
       "1                   1.0037   -0.0215     0.9209    0.0183     1.0258   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0155           1.0337    0.0163     1.0243  \n",
       "1         -0.0155           0.9674   -0.0163     0.9763  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - RF\")\n",
    "measure.bias(X_test, y_test, y_pred_rf, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da34894",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – Random Forest (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **Random Forest (RF) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **-0.0026**, Ratio = **0.9963**  \n",
    "- **Male (1):** Difference = **0.0026**, Ratio = **1.0037**  \n",
    "➡️ Balanced accuracy is almost identical, with males having a negligible advantage (~0.3%).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **0.0215**, Ratio = **1.0859**  \n",
    "- **Male (1):** FPR Diff = **-0.0215**, Ratio = **0.9209**  \n",
    "➡️ Females experience a **slightly higher false positive rate** (~2%), while males benefit from fewer false positives.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0183**, Ratio = **0.9748**  \n",
    "- **Male (1):** PPV Diff = **0.0183**, Ratio = **1.0258**  \n",
    "➡️ Precision is **higher for males**, meaning their positive predictions are somewhat more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **0.0155**, Ratio = **1.0337**  \n",
    "- **Male (1):** Selection Diff = **-0.0155**, Ratio = **0.9674**  \n",
    "➡️ Females are **slightly more likely** to be predicted positive, while males are predicted positive at a slightly lower rate.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **0.0163**, Ratio = **1.0243**  \n",
    "- **Male (1):** TPR Diff = **-0.0163**, Ratio = **0.9763**  \n",
    "➡️ Females achieve **higher recall**, meaning more true positives are detected.  \n",
    "➡️ Males have slightly lower recall (more false negatives).\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):**  \n",
    "  - Advantages: **Higher recall and selection rate** (more positives detected).  \n",
    "  - Disadvantages: **Higher false positive rate** and **slightly lower precision**.  \n",
    "\n",
    "- **Males (1):**  \n",
    "  - Advantages: **Lower false positive rate** and **higher precision** (positive predictions are more reliable).  \n",
    "  - Disadvantages: **Lower recall** and **less likely to be predicted positive**.  \n",
    "\n",
    "The disparities are **small (1–2%)** and reflect a **trade-off**:  \n",
    "- **Females** → better sensitivity (more positives detected) but more false alarms.  \n",
    "- **Males** → fewer false alarms, but at the cost of missing some positives.  \n",
    "\n",
    "Overall, the Random Forest model remains **reasonably fair across genders** with only minor imbalances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6573254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.4657</td>\n",
       "      <td>0.7092</td>\n",
       "      <td>0.6981</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7221</td>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.6758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.7100</td>\n",
       "      <td>0.6981</td>\n",
       "      <td>0.2501</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7285</td>\n",
       "      <td>0.7715</td>\n",
       "      <td>0.6702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.4759</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.6981</td>\n",
       "      <td>0.2716</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7102</td>\n",
       "      <td>0.7578</td>\n",
       "      <td>0.6865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.4657   \n",
       "1        gender             0   7362.0       0.5004           0.4603   \n",
       "2        gender             1   3894.0       0.4923           0.4759   \n",
       "\n",
       "   Accuracy  F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7092    0.6981  0.2576      —     0.7221   0.7667  0.6758  \n",
       "1    0.7100    0.6981  0.2501      —     0.7285   0.7715  0.6702  \n",
       "2    0.7078    0.6981  0.2716      —     0.7102   0.7578  0.6865  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_rf = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_rf = perf_table_rf.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46300b07",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – Random Forest (by Gender)\n",
    "\n",
    "The table shows performance results for the **Random Forest (RF) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7092  \n",
    "- **F1-Score:** 0.6981  \n",
    "- **Precision:** 0.7221  \n",
    "- **ROC AUC:** 0.7667  \n",
    "- **TPR (Recall):** 0.6758  \n",
    "→ The RF model achieves **solid overall performance**, balancing precision and recall with good discriminative ability.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.7100 (slightly higher than males)  \n",
    "- **F1-Score:** 0.6981 (same as males)  \n",
    "- **FPR:** 0.2501 (lower than males → fewer false positives)  \n",
    "- **Precision:** 0.7285 (higher than males → more reliable predictions)  \n",
    "- **ROC AUC:** 0.7715 (higher than males → better discrimination ability)  \n",
    "- **TPR (Recall):** 0.6702 (lower recall → more false negatives)  \n",
    "\n",
    "➡️ Females benefit from **better precision, fewer false positives, and higher ROC AUC**, but they sacrifice recall.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.7078 (slightly lower than females)  \n",
    "- **F1-Score:** 0.6981 (same as females)  \n",
    "- **FPR:** 0.2716 (higher → more false positives)  \n",
    "- **Precision:** 0.7102 (lower precision)  \n",
    "- **ROC AUC:** 0.7578 (lower than females)  \n",
    "- **TPR (Recall):** 0.6865 (higher recall → fewer false negatives)  \n",
    "\n",
    "➡️ Males achieve **better recall**, but this comes at the cost of **more false positives and less precise predictions**.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Stronger in **precision, ROC AUC, and lower FPR** → more reliable predictions, fewer false alarms, but slightly worse sensitivity.  \n",
    "- **Males (1):** Stronger in **recall (TPR)** → more positives detected, but with higher false positive rates and lower precision.  \n",
    "\n",
    "The Random Forest model therefore shows a **typical fairness trade-off**:  \n",
    "- **Females** → more accurate and reliable predictions.  \n",
    "- **Males** → more sensitive detection but at the cost of increased false alarms.  \n",
    "\n",
    "The differences are **small to moderate (≈2%–3%)**, suggesting the model is **reasonably balanced** but introduces a subtle gender trade-off.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a52946a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6702\n",
      "  False Positive Rate (FPR): 0.2501\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.6865\n",
      "  False Positive Rate (FPR): 0.2716\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_rf == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_rf == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1818aaf",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Random Forest Model\n",
    "\n",
    "This section presents the classification performance of the Random Forest model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.6702  | 0.2501  |\n",
    "| Male (Privileged = 1)        | 0.6865  | 0.2716  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Achieve a **TPR of 67.02%**, slightly lower than males → fewer true positives are identified.  \n",
    "  - Benefit from a **lower FPR (25.01%)**, meaning they are less often incorrectly classified as positive.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Achieve a **TPR of 68.65%**, indicating stronger recall and more true positives captured.  \n",
    "  - However, they also face a **higher FPR (27.16%)**, meaning more false positives occur.\n",
    "\n",
    "#### Conclusion\n",
    "The Random Forest model demonstrates a **performance trade-off**:  \n",
    "- **Males** → better recall (higher TPR) but at the cost of more false alarms (higher FPR).  \n",
    "- **Females** → fewer false positives, but slightly weaker sensitivity.  \n",
    "\n",
    "The differences (≈1.6% in TPR and ≈2.1% in FPR) are **small**, suggesting the model is **reasonably fair across genders**, with only minor imbalances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136de6",
   "metadata": {},
   "source": [
    "### Deep Learning Model - Feed Forward Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c69d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred    y_prob\n",
      "0       0       0       0  0.321685\n",
      "1       0       0       1  0.868185\n",
      "2       1       0       0  0.412735\n",
      "3       0       0       0  0.285116\n",
      "4       0       0       0  0.242904\n"
     ]
    }
   ],
   "source": [
    "mlp_df = pd.read_csv(\"CVDKaggleData_50M50F_MLP_adamtuned_predictions.csv\")\n",
    "print(mlp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f65a498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns \n",
    "y_true_mlp = mlp_df[\"y_true\"].values \n",
    "y_prob_mlp = mlp_df[\"y_prob\"].values\n",
    "y_pred_mlp = mlp_df[\"y_pred\"].values\n",
    "gender_mlp = mlp_df[\"gender\"].values \n",
    "\n",
    "# Use gender_mlp as the protected attribute\n",
    "protected_attr_mlp = gender_mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6969bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0073\n",
      "               Balanced Accuracy Difference            0.0034\n",
      "               Balanced Accuracy Ratio                 1.0047\n",
      "               Disparate Impact Ratio                  0.9596\n",
      "               Equal Odds Difference                  -0.0267\n",
      "               Equal Odds Ratio                        0.9052\n",
      "               Positive Predictive Parity Difference   0.0209\n",
      "               Positive Predictive Parity Ratio        1.0294\n",
      "               Statistical Parity Difference          -0.0199\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "#Run fairmlhealth bias detection for MLP \n",
    "\n",
    "mlp_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp,\n",
    "    prtc_attr=protected_attr_mlp,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(mlp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbf7eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f5fa5\">\n",
       "  <caption>MLP Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f5fa5_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f5fa5_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_f5fa5_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_f5fa5_row0_col0\" class=\"data row0 col0\" >0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5fa5_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_f5fa5_row1_col0\" class=\"data row1 col0\" >0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5fa5_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_f5fa5_row2_col0\" class=\"data row2 col0\" >1.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5fa5_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_f5fa5_row3_col0\" class=\"data row3 col0\" >0.9596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5fa5_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_f5fa5_row4_col0\" class=\"data row4 col0\" >-0.0267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5fa5_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_f5fa5_row5_col0\" class=\"data row5 col0\" >0.9052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5fa5_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_f5fa5_row6_col0\" class=\"data row6 col0\" >0.0209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5fa5_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_f5fa5_row7_col0\" class=\"data row7 col0\" >1.0294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5fa5_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_f5fa5_row8_col0\" class=\"data row8 col0\" >-0.0199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5fa5_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_f5fa5_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_f5fa5_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1af68c35990>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for MLP\n",
    "styled_mlp = MyFlagger().apply_flag(\n",
    "    df=mlp_bias,\n",
    "    caption=\"MLP Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8a2e5",
   "metadata": {},
   "source": [
    "### Interpretation of MLP Fairness Metrics by Gender\n",
    "\n",
    "The table reports fairness metrics for the **Multilayer Perceptron (MLP) model**, using gender as the sensitive attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0073):**  \n",
    "  → Very small; ranking ability is nearly identical across genders.\n",
    "\n",
    "- **Balanced Accuracy Difference (0.0034)** and **Ratio (1.0047):**  \n",
    "  → Balanced accuracy is almost the same, with only a tiny advantage for one group.  \n",
    "\n",
    "- **Disparate Impact Ratio (0.9596):**  \n",
    "  → Slightly below 1, meaning females are **a bit less likely to receive positive predictions** compared to males.  \n",
    "  → Still within the fairness guideline range (0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (-0.0267)** and **Equal Odds Ratio (0.9052):**  \n",
    "  → Shows the largest disparity: males have **slightly higher error rates** (FPR or lower TPR), while females benefit from marginally more favorable error distribution.  \n",
    "  → The imbalance is modest (~2.7%).\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0209)** and **Ratio (1.0294):**  \n",
    "  → Precision is somewhat higher for males, meaning their positive predictions are more reliable.\n",
    "\n",
    "- **Statistical Parity Difference (-0.0199):**  \n",
    "  → Females are **predicted positive at a slightly lower rate** than males, but the gap remains small.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → Males account for 35% of the dataset. Despite this lower representation, the fairness metrics remain well balanced.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The MLP model demonstrates **generally fair performance across genders**, with small disparities:\n",
    "- **Females (unprivileged group):** Slightly less likely to be predicted positive but with somewhat more favorable error rates.  \n",
    "- **Males (privileged group):** Benefit from higher precision but face slightly higher error rates.  \n",
    "\n",
    "The most notable gap is in **Equal Odds (-0.0267)**, but all values remain **close to parity**. Overall, the model can be considered **reasonably fair**, with no strong systematic bias detected.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aa66097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - MLP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>1.1047</td>\n",
       "      <td>-0.0209</td>\n",
       "      <td>0.9714</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>1.0421</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.0290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>1.0047</td>\n",
       "      <td>-0.0267</td>\n",
       "      <td>0.9052</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>1.0294</td>\n",
       "      <td>-0.0199</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.9718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0034   \n",
       "1       gender             1                        0.0034   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9953    0.0267     1.1047   -0.0209     0.9714   \n",
       "1                   1.0047   -0.0267     0.9052    0.0209     1.0294   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0199           1.0421      0.02     1.0290  \n",
       "1         -0.0199           0.9596     -0.02     0.9718  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - MLP\")\n",
    "measure.bias(X_test, y_test, y_pred_mlp, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efef5a5",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – MLP (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **Multilayer Perceptron (MLP) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **-0.0034**, Ratio = **0.9953**  \n",
    "- **Male (1):** Difference = **0.0034**, Ratio = **1.0047**  \n",
    "➡️ Balanced accuracy is almost identical, with only a tiny advantage for males.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **0.0267**, Ratio = **1.1047**  \n",
    "- **Male (1):** FPR Diff = **-0.0267**, Ratio = **0.9052**  \n",
    "➡️ Females have a **higher false positive rate**, meaning they are more often incorrectly classified as positive.  \n",
    "➡️ Males benefit from fewer false positives.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0209**, Ratio = **0.9714**  \n",
    "- **Male (1):** PPV Diff = **0.0209**, Ratio = **1.0294**  \n",
    "➡️ Males achieve **higher precision**, meaning their positive predictions are more reliable.  \n",
    "➡️ Females’ positive predictions are slightly less reliable.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **0.0199**, Ratio = **1.0421**  \n",
    "- **Male (1):** Selection Diff = **-0.0199**, Ratio = **0.9596**  \n",
    "➡️ Females are **more likely to be predicted positive**, while males are predicted positive at a slightly lower rate.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **0.0200**, Ratio = **1.0290**  \n",
    "- **Male (1):** TPR Diff = **-0.0200**, Ratio = **0.9718**  \n",
    "➡️ Females achieve **higher recall**, meaning more of their true positives are detected.  \n",
    "➡️ Males experience more false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):**  \n",
    "  - Advantages: **Higher recall and higher selection rate** (more positives detected).  \n",
    "  - Disadvantages: **Higher false positive rate** and **slightly lower precision**.  \n",
    "\n",
    "- **Males (1):**  \n",
    "  - Advantages: **Lower false positive rate** and **higher precision** (positive predictions are more reliable).  \n",
    "  - Disadvantages: **Lower recall** and **less likely to be predicted positive**.  \n",
    "\n",
    "The differences (≈2–2.7%) are **moderate** and represent a **classic trade-off**:  \n",
    "- **Females** → better sensitivity but less reliability.  \n",
    "- **Males** → fewer false alarms and higher reliability, but more missed positives.  \n",
    "\n",
    "Overall, the MLP model is **reasonably balanced**, though it leans slightly toward **favoring females in recall** and **males in precision**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbc2caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.4793</td>\n",
       "      <td>0.7161</td>\n",
       "      <td>0.7093</td>\n",
       "      <td>0.2644</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7229</td>\n",
       "      <td>0.7728</td>\n",
       "      <td>0.6963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.7172</td>\n",
       "      <td>0.7093</td>\n",
       "      <td>0.2550</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7303</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.6895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.7139</td>\n",
       "      <td>0.7094</td>\n",
       "      <td>0.2817</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7094</td>\n",
       "      <td>0.7682</td>\n",
       "      <td>0.7094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.4793   \n",
       "1        gender             0   7362.0       0.5004           0.4724   \n",
       "2        gender             1   3894.0       0.4923           0.4923   \n",
       "\n",
       "   Accuracy  F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7161    0.7093  0.2644      —     0.7229   0.7728  0.6963  \n",
       "1    0.7172    0.7093  0.2550      —     0.7303   0.7755  0.6895  \n",
       "2    0.7139    0.7094  0.2817      —     0.7094   0.7682  0.7094  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_mlp = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_mlp = perf_table_mlp.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7c382",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – MLP (by Gender)\n",
    "\n",
    "The table shows performance results for the **Multilayer Perceptron (MLP) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7161  \n",
    "- **F1-Score:** 0.7093  \n",
    "- **Precision:** 0.7229  \n",
    "- **ROC AUC:** 0.7728  \n",
    "- **TPR (Recall):** 0.6963  \n",
    "→ The MLP achieves **balanced performance overall**, with good discrimination ability and a fair trade-off between recall and precision.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.7172 (slightly higher than males)  \n",
    "- **F1-Score:** 0.7093 (same as males)  \n",
    "- **FPR:** 0.2550 (lower than males → fewer false positives)  \n",
    "- **Precision:** 0.7303 (higher than males → more reliable positive predictions)  \n",
    "- **ROC AUC:** 0.7755 (slightly better discrimination ability)  \n",
    "- **TPR (Recall):** 0.6895 (lower recall → more false negatives)  \n",
    "\n",
    "➡️ Females benefit from **higher precision, accuracy, and fewer false positives**, but recall is weaker than for males.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.7139 (slightly lower)  \n",
    "- **F1-Score:** 0.7094 (same as females)  \n",
    "- **FPR:** 0.2817 (higher → more false positives)  \n",
    "- **Precision:** 0.7094 (lower precision than females)  \n",
    "- **ROC AUC:** 0.7682 (slightly lower discrimination ability)  \n",
    "- **TPR (Recall):** 0.7094 (higher recall → fewer false negatives)  \n",
    "\n",
    "➡️ Males achieve **better recall**, but this comes at the cost of **higher false positives and lower precision**.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** More reliable predictions (higher precision, lower FPR, slightly higher ROC AUC), but weaker sensitivity.  \n",
    "- **Males (1):** Better sensitivity (higher TPR/recall), but more false positives and less precise predictions.  \n",
    "\n",
    "The disparities are **small (≈2% in FPR and ≈2% in recall)**, reflecting a **balanced model with minor trade-offs**:\n",
    "- **Females** → fewer false positives, better reliability.  \n",
    "- **Males** → more true positives detected, but noisier predictions.  \n",
    "\n",
    "Overall, the MLP demonstrates **good fairness** with **only minor gender differences**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379dbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6895\n",
      "  False Positive Rate (FPR): 0.2550\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.7094\n",
      "  False Positive Rate (FPR): 0.2817\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_mlp == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_mlp == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29443ed8",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – MLP Model\n",
    "\n",
    "This section reports the classification performance of the **Multilayer Perceptron (MLP)** model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.6895  | 0.2550  |\n",
    "| Male (Privileged = 1)        | 0.7094  | 0.2817  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Achieve a **TPR of 68.95%**, which is slightly lower than for males, meaning more missed positives (false negatives).  \n",
    "  - Benefit from a **lower FPR (25.50%)**, meaning they are less often incorrectly classified as positive.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Achieve a **TPR of 70.94%**, slightly higher than females, meaning more true positives are correctly identified.  \n",
    "  - However, they face a **higher FPR (28.17%)**, which means they are more often incorrectly flagged as positive.\n",
    "\n",
    "#### Conclusion\n",
    "The MLP model shows a **clear trade-off**:\n",
    "- **Males** → stronger recall (higher TPR) but at the cost of more false positives.  \n",
    "- **Females** → fewer false positives (lower FPR) but weaker recall.  \n",
    "\n",
    "The differences (≈2% in TPR and ≈2.7% in FPR) are **modest**, suggesting the model is **reasonably fair across genders**, with only minor performance imbalances.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
