{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba0b2b9",
   "metadata": {},
   "source": [
    "### Bias Detection and Fairness Evaluation on Heart Failure Prediction Dataset (Kaggle) using FairMLhealth\n",
    "(Source: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c29a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load X_test set\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57b21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup successful\n"
     ]
    }
   ],
   "source": [
    "import fairmlhealth\n",
    "import aif360\n",
    "print(\"Environment setup successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f126c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fairmlhealth\n",
      "Version: 1.0.2\n",
      "Summary: Health-centered variation analysis\n",
      "Home-page: https://github.com/KenSciResearch/fairMLHealth\n",
      "Author: Christine Allen\n",
      "Author-email: ca.magallen@gmail.com\n",
      "License: \n",
      "Location: c:\\users\\patri\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: aif360, ipython, jupyter, numpy, pandas, requests, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#have a look at the details of fairmlhealth - especially the version\n",
    "!pip show fairmlhealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76186324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "#have a look at the modules that are within fairmlhealth\n",
    "\n",
    "print(dir(fairmlhealth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44657ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "#load necessary modules \n",
    "\n",
    "#import module measure to use measure.summary for bias detection\n",
    "from fairmlhealth import measure\n",
    "\n",
    "#import module for investigation of individual cohorts \n",
    "from fairmlhealth.__utils import iterate_cohorts\n",
    "\n",
    "#import FairRanges to flag high values\n",
    "from fairmlhealth.__utils import FairRanges\n",
    "\n",
    "# Wrap the fairness summary function for cohort-wise analysis\n",
    "@iterate_cohorts\n",
    "def cohort_summary(**kwargs):\n",
    "    return measure.summary(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10ccd4",
   "metadata": {},
   "source": [
    "During the execution of FairMLHealth and AIF360, several runtime warnings were raised (e.g., “AdversarialDebiasing will be unavailable” due to the absence of TensorFlow, and deprecation warnings from the inFairness package regarding PyTorch’s functorch.vmap). These warnings do not affect the fairness metrics or results presented in this study, as the unavailable components were not used. To maintain clarity of output, the warnings were silenced programmatically, and the analysis was conducted without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb6c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"inFairness\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"AdversarialDebiasing will be unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a099295",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models - KNN & DT\n",
    "\n",
    "#### K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d817251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true    y_prob  y_pred\n",
      "0    1       1  0.930457       1\n",
      "1    1       1  0.580671       1\n",
      "2    1       1  0.872885       1\n",
      "3    1       1  0.062216       0\n",
      "4    0       0  0.389552       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "knn_df = pd.read_csv(\"HeartFailureData_75F25M_PCA_KNN_predictions.csv\")\n",
    "\n",
    "print(knn_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871aa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_knn = knn_df[\"y_true\"].values\n",
    "y_prob_knn = knn_df[\"y_prob\"].values\n",
    "y_pred_knn = knn_df[\"y_pred\"].values\n",
    "gender_knn = knn_df[\"Sex\"].values\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_knn = gender_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad78836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0632\n",
      "               Balanced Accuracy Difference           -0.0029\n",
      "               Balanced Accuracy Ratio                 0.9964\n",
      "               Disparate Impact Ratio                  0.2343\n",
      "               Equal Odds Difference                  -0.1146\n",
      "               Equal Odds Ratio                        0.2232\n",
      "               Positive Predictive Parity Difference  -0.1146\n",
      "               Positive Predictive Parity Ratio        0.8747\n",
      "               Statistical Parity Difference          -0.4301\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "knn_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_true_knn,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn,\n",
    "    prtc_attr=protected_attr_knn,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,   # skip inconsistency metrics that cause NearestNeighbors error\n",
    "    skip_performance=True\n",
    ")\n",
    "\n",
    "print(knn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ba3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Custom scenario oriented bounds\n",
    "\n",
    "custom_ranges = {\n",
    "    \"tpr diff\": (-0.03, 0.03),\n",
    "    \"fpr diff\": (-0.03, 0.03),\n",
    "    \"equal odds difference\": (-0.04, 0.04),\n",
    "    \"statistical parity difference\": (-0.05, 0.05),\n",
    "    \"disparate impact ratio\": (0.9, 1.1),\n",
    "    \"selection ratio\": (0.9, 1.1),\n",
    "    \"auc difference\": (-0.02, 0.02),\n",
    "    \"balanced accuracy difference\": (-0.02, 0.02),\n",
    "}\n",
    "\n",
    "bounds = FairRanges().load_fair_ranges(custom_ranges=custom_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14d2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for highlighting metrics outside of the thresholds\n",
    "#  restore Styler.set_precision to adjust the highlighting color in the styled table\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "Styler = type(pd.DataFrame({\"_\":[0]}).style)  \n",
    "\n",
    "if not hasattr(Styler, \"set_precision\"):\n",
    "    def _set_precision(self, precision=4):\n",
    "        try:\n",
    "            return self.format(precision=precision)\n",
    "        except TypeError:\n",
    "            return self.format(formatter=lambda x:\n",
    "                f\"{x:.{precision}g}\" if isinstance(x, (int, float, np.floating)) else x\n",
    "            )\n",
    "    setattr(Styler, \"set_precision\", _set_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90435472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fa3ae_row0_col0, #T_fa3ae_row3_col0, #T_fa3ae_row4_col0, #T_fa3ae_row5_col0, #T_fa3ae_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fa3ae\">\n",
       "  <caption>KNN Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fa3ae_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fa3ae_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_fa3ae_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_fa3ae_row0_col0\" class=\"data row0 col0\" >0.0632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa3ae_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_fa3ae_row1_col0\" class=\"data row1 col0\" >-0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa3ae_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_fa3ae_row2_col0\" class=\"data row2 col0\" >0.9964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa3ae_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_fa3ae_row3_col0\" class=\"data row3 col0\" >0.2343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa3ae_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_fa3ae_row4_col0\" class=\"data row4 col0\" >-0.1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa3ae_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_fa3ae_row5_col0\" class=\"data row5 col0\" >0.2232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa3ae_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_fa3ae_row6_col0\" class=\"data row6 col0\" >-0.1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa3ae_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_fa3ae_row7_col0\" class=\"data row7 col0\" >0.8747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa3ae_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_fa3ae_row8_col0\" class=\"data row8 col0\" >-0.4301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa3ae_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_fa3ae_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_fa3ae_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22ac4b69b40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "from fairmlhealth.__utils import Flagger\n",
    "\n",
    "class MyFlagger(Flagger):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.flag_color = \"#491ee6\"   \n",
    "        self.flag_type = \"background-color\"\n",
    "\n",
    "styled_knn = MyFlagger().apply_flag(\n",
    "    df=knn_bias,\n",
    "    caption=\"KNN Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d3278",
   "metadata": {},
   "source": [
    "## Gender Bias Detection Results – KNN Model  \n",
    "\n",
    "This table presents fairness metrics for the **KNN model**, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics  \n",
    "\n",
    "- **AUC Difference (0.0632):**  \n",
    "  The ranking ability of the model differs moderately across genders, with males slightly better represented.  \n",
    "\n",
    "- **Balanced Accuracy Difference (−0.0029) and Ratio (0.9964):**  \n",
    "  Balanced accuracy is nearly identical across genders, suggesting overall parity in classification performance.  \n",
    "\n",
    "- **Disparate Impact Ratio (0.2343):**  \n",
    "  Far below the acceptable threshold (0.80–1.25). This indicates that females receive positive predictions at a **much lower rate** compared to males, highlighting strong selection bias.  \n",
    "\n",
    "- **Equal Odds Difference (−0.1146) and Ratio (0.2232):**  \n",
    "  Substantial disparity in error rates (TPR/FPR) between genders. The negative value suggests that **females are treated less favorably**, with higher error disparities.  \n",
    "\n",
    "- **Positive Predictive Parity Difference (−0.1146) and Ratio (0.8747):**  \n",
    "  Precision is lower for females, meaning positive predictions for this group are less reliable compared to males.  \n",
    "\n",
    "- **Statistical Parity Difference (−0.4301):**  \n",
    "  Large negative value shows that **females are selected at a much lower rate** than males, reinforcing evidence of unequal treatment.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation  \n",
    "\n",
    "- **Disparities are most evident in outcome distribution** (statistical parity and disparate impact), showing that females are under-selected for positive outcomes.  \n",
    "- **Error distribution (equal odds)** is also imbalanced, with females disadvantaged in terms of prediction reliability.  \n",
    "- Although **balanced accuracy is nearly equal**, suggesting similar base classification performance, the **systematic under-selection of females** signals significant fairness issues.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "\n",
    "The KNN model exhibits **systematic gender bias**, primarily disadvantaging **females (unprivileged group)**.  \n",
    "- They receive far fewer positive outcomes.  \n",
    "- Their predictions are less precise and associated with higher error disparities.  \n",
    "\n",
    "This combination of **outcome imbalance and prediction unreliability** indicates that fairness mitigation is necessary before deploying KNN in practice.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0eeae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - KNN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>1.0036</td>\n",
       "      <td>0.1088</td>\n",
       "      <td>4.4800</td>\n",
       "      <td>0.1146</td>\n",
       "      <td>1.1433</td>\n",
       "      <td>0.4301</td>\n",
       "      <td>4.2685</td>\n",
       "      <td>0.1146</td>\n",
       "      <td>1.1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0029</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>-0.1088</td>\n",
       "      <td>0.2232</td>\n",
       "      <td>-0.1146</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>-0.4301</td>\n",
       "      <td>0.2343</td>\n",
       "      <td>-0.1146</td>\n",
       "      <td>0.8533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                        0.0029   \n",
       "1          Sex             1                       -0.0029   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0036    0.1088     4.4800    0.1146     1.1433   \n",
       "1                   0.9964   -0.1088     0.2232   -0.1146     0.8747   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.4301           4.2685    0.1146     1.1719  \n",
       "1         -0.4301           0.2343   -0.1146     0.8533  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - KNN\")\n",
    "measure.bias(X_test, y_test, y_pred_knn, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05764be",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – KNN by Gender  \n",
    "\n",
    "This table presents **group-specific fairness metrics** for the KNN model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy  \n",
    "- **Females (0):** Balanced Accuracy Difference = **+0.0029**, Ratio = **1.0036**  \n",
    "- **Males (1):** Balanced Accuracy Difference = **−0.0029**, Ratio = **0.9964**  \n",
    "- ➝ Balanced accuracy is nearly identical across genders, suggesting no strong disparity in base classification performance.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)  \n",
    "- **Females (0):** FPR Difference = **+0.1088**, Ratio = **4.4800**  \n",
    "- **Males (1):** FPR Difference = **−0.1088**, Ratio = **0.2232**  \n",
    "- ➝ Females experience a **substantially higher false positive rate**, meaning they are more often incorrectly flagged as positive compared to males.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)  \n",
    "- **Females (0):** PPV Difference = **+0.1146**, Ratio = **1.1433**  \n",
    "- **Males (1):** PPV Difference = **−0.1146**, Ratio = **0.8747**  \n",
    "- ➝ Precision is **higher for females**, meaning their positive predictions are somewhat more reliable than for males.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate  \n",
    "- **Females (0):** Selection Difference = **+0.4301**, Ratio = **4.2685**  \n",
    "- **Males (1):** Selection Difference = **−0.4301**, Ratio = **0.2343**  \n",
    "- ➝ Females are **selected far more often**, while males are significantly under-selected.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)  \n",
    "- **Females (0):** TPR Difference = **+0.1146**, Ratio = **1.1719**  \n",
    "- **Males (1):** TPR Difference = **−0.1146**, Ratio = **0.8533**  \n",
    "- ➝ Females have a **higher sensitivity**, meaning they are more likely to be correctly identified when truly positive.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "- The KNN model introduces a **reverse bias** in this evaluation:  \n",
    "  - **Females (unprivileged group)** benefit from **higher precision, higher TPR, and much higher selection rates**, but at the cost of also facing **significantly higher false positive rates**.  \n",
    "  - **Males (privileged group)** are disadvantaged with fewer positive predictions and lower sensitivity.  \n",
    "\n",
    "In essence, the model **favors females**, though not equitably: females get more opportunities (higher selection) but also face more false alarms. This imbalance still represents a fairness concern, as the error distribution is not consistent across genders.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fdeff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.4728</td>\n",
       "      <td>0.8315</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.0976</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9080</td>\n",
       "      <td>0.9103</td>\n",
       "      <td>0.7745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.5616</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>0.8427</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>0.8899</td>\n",
       "      <td>0.7812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.4728    0.8315   \n",
       "1           Sex             0   38.0       0.1579           0.1316    0.9211   \n",
       "2           Sex             1  146.0       0.6575           0.5616    0.8082   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8360  0.0976      —     0.9080   0.9103  0.7745  \n",
       "1    0.7273  0.0312      —     0.8000   0.9531  0.6667  \n",
       "2    0.8427  0.1400      —     0.9146   0.8899  0.7812  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fairmlhealth import measure\n",
    "import pandas as pd\n",
    "from IPython.display import display  \n",
    "\n",
    "# convert gender into DataFrame with a clear column name to get a nice table as output\n",
    "gender_df = pd.DataFrame({\"Sex\": X_test[\"Sex\"].astype(int)})\n",
    "\n",
    "\n",
    "# Get the stratified table\n",
    "perf_table_knn = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_knn = perf_table_knn.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e845f8a",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – KNN by Gender  \n",
    "\n",
    "This table shows the **stratified performance metrics** of the KNN model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)  \n",
    "- **Accuracy (0.8315)** and **F1-score (0.8360)** indicate solid overall performance.  \n",
    "- **ROC AUC (0.9103)** demonstrates strong discriminatory ability.  \n",
    "- **Precision (0.9080)** is high, suggesting reliable predictions.  \n",
    "- **TPR (0.7745)** shows reasonable sensitivity, though subgroup analysis reveals imbalances.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison  \n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.9211     | 0.8082   | Accuracy is higher for females. |\n",
    "| **F1-Score**  | 0.7273     | 0.8427   | Males benefit from stronger F1 performance. |\n",
    "| **FPR**       | 0.0312     | 0.1400   | Females have a much lower false positive rate. |\n",
    "| **Precision** | 0.8000     | 0.9146   | Predictions are more reliable for males. |\n",
    "| **ROC AUC**   | 0.9531     | 0.8899   | The model ranks female cases better. |\n",
    "| **TPR**       | 0.6667     | 0.7812   | Males are detected more often when truly positive. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation  \n",
    "\n",
    "- **Females (unprivileged):**  \n",
    "  - Benefit from **higher accuracy** and **lower FPR (3.1%)**, meaning fewer false alarms.  \n",
    "  - However, they suffer from **lower recall/TPR (66.7%)** and **weaker F1-score (0.7273)**, suggesting more missed true cases.  \n",
    "  - ROC AUC (0.9531) is stronger, showing good ranking ability.  \n",
    "\n",
    "- **Males (privileged):**  \n",
    "  - Achieve **better recall (78.1%)** and **higher F1-score (0.8427)**, showing overall stronger predictive effectiveness.  \n",
    "  - However, they face a **much higher FPR (14%)**, meaning more incorrect positive classifications.  \n",
    "  - ROC AUC is lower (0.8899), indicating weaker ranking ability compared to females.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "The KNN model reveals a **mixed fairness pattern**:  \n",
    "- **Females** benefit from **higher accuracy and fewer false positives**, but face **more missed true cases (low TPR)**.  \n",
    "- **Males** gain from **higher recall and overall predictive balance**, but are more often **incorrectly flagged as positive**.  \n",
    "\n",
    "This indicates that the model’s bias is not unidirectional: instead, it creates **different trade-offs** for each gender group.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5db1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6667\n",
      "  False Positive Rate (FPR): 0.0312\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.7812\n",
      "  False Positive Rate (FPR): 0.1400\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#group specific error analysis\n",
    "\n",
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_knn == 0)  # unprivileged group (female)\n",
    "male_mask   = (protected_attr_knn == 1)  # privileged group (male)\n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba4143",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – KNN Model  \n",
    "\n",
    "To further examine fairness at the subgroup level, we compared the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.  \n",
    "\n",
    "#### Results by Gender Group  \n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.6667  | 0.0312  |\n",
    "| Privileged (Male)      | 0.7812  | 0.1400  |  \n",
    "\n",
    "#### Interpretation  \n",
    "\n",
    "- **Females (unprivileged group):**  \n",
    "  - Show a **lower TPR (66.67%)**, meaning they are more likely to be missed when truly positive.  \n",
    "  - However, they also benefit from a **very low FPR (3.12%)**, implying fewer false alarms.  \n",
    "\n",
    "- **Males (privileged group):**  \n",
    "  - Achieve a **higher TPR (78.12%)**, so true cases are more often correctly detected.  \n",
    "  - On the downside, they experience a **much higher FPR (14%)**, meaning more incorrect positive predictions.  \n",
    "\n",
    "#### Summary  \n",
    "\n",
    "The model exhibits **different trade-offs across genders**:  \n",
    "- Females are **less often correctly identified (lower sensitivity)** but **less frequently misclassified as positive (lower FPR)**.  \n",
    "- Males benefit from **higher sensitivity**, but at the cost of **substantially more false positives**.  \n",
    "\n",
    "This pattern highlights that fairness concerns are **bidirectional**, as each group faces a different type of disadvantage.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb317ca",
   "metadata": {},
   "source": [
    "### Decision Tree - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adbaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true    y_prob  y_pred\n",
      "0    1       1  0.989399       1\n",
      "1    1       1  0.000000       0\n",
      "2    1       1  0.989399       1\n",
      "3    1       1  0.774194       1\n",
      "4    0       0  0.972973       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "dt_df = pd.read_csv(\"HeartFailureData_75F25M_AltTunedDT_predictions.csv\")\n",
    "\n",
    "print(dt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06530601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract common columns\n",
    "y_true_dt = dt_df[\"y_true\"].values\n",
    "y_prob_dt = dt_df[\"y_prob\"].values\n",
    "y_pred_dt = dt_df[\"y_pred\"].values\n",
    "gender_dt = dt_df[\"Sex\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_dt = gender_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b83e2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Decision Tree Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.0570\n",
      "               Balanced Accuracy Difference           -0.0375\n",
      "               Balanced Accuracy Ratio                 0.9539\n",
      "               Disparate Impact Ratio                  0.5172\n",
      "               Equal Odds Difference                  -0.0938\n",
      "               Equal Odds Ratio                        0.8989\n",
      "               Positive Predictive Parity Difference  -0.4986\n",
      "               Positive Predictive Parity Ratio        0.4173\n",
      "               Statistical Parity Difference          -0.3439\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Gender Bias Report\n",
    "print(\"\\n--- Decision Tree Gender Bias Report ---\")\n",
    "\n",
    "dt_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt,\n",
    "    prtc_attr=protected_attr_dt,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,  \n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(dt_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc2d88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_999e9_row0_col0, #T_999e9_row1_col0, #T_999e9_row3_col0, #T_999e9_row4_col0, #T_999e9_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_999e9\">\n",
       "  <caption>Decision Tree Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_999e9_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_999e9_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_999e9_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_999e9_row0_col0\" class=\"data row0 col0\" >-0.0570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_999e9_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_999e9_row1_col0\" class=\"data row1 col0\" >-0.0375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_999e9_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_999e9_row2_col0\" class=\"data row2 col0\" >0.9539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_999e9_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_999e9_row3_col0\" class=\"data row3 col0\" >0.5172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_999e9_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_999e9_row4_col0\" class=\"data row4 col0\" >-0.0938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_999e9_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_999e9_row5_col0\" class=\"data row5 col0\" >0.8989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_999e9_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_999e9_row6_col0\" class=\"data row6 col0\" >-0.4986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_999e9_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_999e9_row7_col0\" class=\"data row7 col0\" >0.4173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_999e9_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_999e9_row8_col0\" class=\"data row8 col0\" >-0.3439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_999e9_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_999e9_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_999e9_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22ac4d3e410>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "styled_dt = MyFlagger().apply_flag(\n",
    "    df=dt_bias,\n",
    "    caption=\"Decision Tree Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1c7f5",
   "metadata": {},
   "source": [
    "## Gender Bias Detection Results – Decision Tree Model  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics  \n",
    "\n",
    "- **AUC Difference (−0.0570):** Females achieve slightly better ranking performance (ROC AUC) than males.  \n",
    "- **Balanced Accuracy Difference (−0.0375), Ratio (0.9539):** Balanced accuracy is lower for males, suggesting the model performs more evenly for females.  \n",
    "- **Disparate Impact Ratio (0.5172):** Well below the acceptable range of **0.80–1.25**, indicating a strong imbalance in selection rates that disadvantages females.  \n",
    "- **Equal Odds Difference (−0.0938), Ratio (0.8989):** Error rates (TPR and FPR) differ, but the gap is moderate, with males slightly favored.  \n",
    "- **Positive Predictive Parity Difference (−0.4986), Ratio (0.4173):** Precision is substantially lower for females, meaning predictions are much less reliable for them.  \n",
    "- **Statistical Parity Difference (−0.3439):** Females are selected at a much lower rate than males, reinforcing evidence of underrepresentation.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation  \n",
    "\n",
    "- The Decision Tree shows **clear fairness concerns**:  \n",
    "  - **Selection disparities** are large (low disparate impact and statistical parity values).  \n",
    "  - **Precision (PPV)** is much worse for females, showing their positive predictions are far less trustworthy.  \n",
    "  - **Equal odds** metrics suggest some imbalance in error distribution, with a small tendency to favor males.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "\n",
    "The Decision Tree model demonstrates **systematic disadvantages for females**:  \n",
    "- They face **lower precision** and **lower selection rates**, meaning they are both underrepresented and less reliably classified when selected.  \n",
    "- While AUC and balanced accuracy differences are relatively small, the **impact on fairness is significant** due to disparities in parity metrics.  \n",
    "\n",
    "This indicates that the Decision Tree introduces a **notable gender bias** that requires mitigation to achieve equitable performance.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "631b7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - DT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>1.0483</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>1.0667</td>\n",
       "      <td>0.4986</td>\n",
       "      <td>2.3962</td>\n",
       "      <td>0.3439</td>\n",
       "      <td>1.9335</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>1.1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0375</td>\n",
       "      <td>0.9539</td>\n",
       "      <td>-0.0187</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>-0.4986</td>\n",
       "      <td>0.4173</td>\n",
       "      <td>-0.3439</td>\n",
       "      <td>0.5172</td>\n",
       "      <td>-0.0938</td>\n",
       "      <td>0.8989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                        0.0375   \n",
       "1          Sex             1                       -0.0375   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0483    0.0187     1.0667    0.4986     2.3962   \n",
       "1                   0.9539   -0.0187     0.9375   -0.4986     0.4173   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.3439           1.9335    0.0938     1.1125  \n",
       "1         -0.3439           0.5172   -0.0938     0.8989  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - DT\")\n",
    "measure.bias(X_test, y_test, y_pred_dt, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa9add",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – Decision Tree by Gender  \n",
    "\n",
    "This table shows the **group-specific fairness metrics** of the Decision Tree model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy  \n",
    "- **Females (0):** Difference = **+0.0375**, Ratio = **1.0483**  \n",
    "- **Males (1):** Difference = **−0.0375**, Ratio = **0.9539**  \n",
    "- ➝ The model is slightly more balanced and accurate for females compared to males.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)  \n",
    "- **Females (0):** FPR Difference = **+0.0187**, Ratio = **1.0667**  \n",
    "- **Males (1):** FPR Difference = **−0.0187**, Ratio = **0.9375**  \n",
    "- ➝ Females experience a marginally higher false positive rate than males.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)  \n",
    "- **Females (0):** PPV Difference = **+0.4986**, Ratio = **2.3962**  \n",
    "- **Males (1):** PPV Difference = **−0.4986**, Ratio = **0.4173**  \n",
    "- ➝ Predictions are **far more reliable for females**, while males face a strong disadvantage in precision.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate  \n",
    "- **Females (0):** Selection Difference = **+0.3439**, Ratio = **1.9335**  \n",
    "- **Males (1):** Selection Difference = **−0.3439**, Ratio = **0.5172**  \n",
    "- ➝ Females are selected at nearly **double the rate of males**, suggesting overrepresentation.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)  \n",
    "- **Females (0):** TPR Difference = **+0.0938**, Ratio = **1.1125**  \n",
    "- **Males (1):** TPR Difference = **−0.0938**, Ratio = **0.8989**  \n",
    "- ➝ Females have a slightly higher sensitivity, meaning their true cases are more likely to be detected.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "- The Decision Tree appears to **favor females across multiple metrics**:  \n",
    "  - Higher balanced accuracy, PPV, selection rates, and TPR.  \n",
    "  - However, they also face a **slightly higher false positive rate**.  \n",
    "- Males are disadvantaged in terms of **precision and selection**, with notably worse predictive reliability.  \n",
    "\n",
    "This indicates that the Decision Tree introduces a **reverse bias**, systematically favoring females (unprivileged group) while disadvantaging males.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1acccc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.6413</td>\n",
       "      <td>0.8261</td>\n",
       "      <td>0.8545</td>\n",
       "      <td>0.2927</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7966</td>\n",
       "      <td>0.8578</td>\n",
       "      <td>0.9216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.3684</td>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>—</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.7969</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>0.8493</td>\n",
       "      <td>0.8900</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8558</td>\n",
       "      <td>0.8539</td>\n",
       "      <td>0.9271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.6413    0.8261   \n",
       "1           Sex             0   38.0       0.1579           0.3684    0.7368   \n",
       "2           Sex             1  146.0       0.6575           0.7123    0.8493   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8545  0.2927      —     0.7966   0.8578  0.9216  \n",
       "1    0.5000  0.2812      —     0.3571   0.7969  0.8333  \n",
       "2    0.8900  0.3000      —     0.8558   0.8539  0.9271  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_dt = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_dt = perf_table_dt.fillna(\"—\")\n",
    "\n",
    "# Display pretty table\n",
    "display(perf_table_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936a469",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Decision Tree by Gender  \n",
    "\n",
    "This table shows the **stratified performance metrics** of the Decision Tree model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)  \n",
    "- **Accuracy (0.8261)** and **F1-score (0.8545)** show moderate overall performance.  \n",
    "- **ROC AUC (0.8578)** indicates fair discriminatory ability.  \n",
    "- **TPR (0.9216)** highlights strong sensitivity overall, though subgroup breakdowns reveal disparities.  \n",
    "- **Precision (0.7966)** suggests predictions are fairly reliable.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison  \n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.7368     | 0.8493   | Model accuracy is considerably lower for females. |\n",
    "| **F1-Score**  | 0.5000     | 0.8900   | Model is much less effective for females. |\n",
    "| **FPR**       | 0.2812     | 0.3000   | Both groups face high false positive rates, but males slightly more so. |\n",
    "| **Precision** | 0.3571     | 0.8558   | Predictions are far less reliable for females. |\n",
    "| **ROC AUC**   | 0.7969     | 0.8539   | Females experience weaker ranking performance. |\n",
    "| **TPR**       | 0.8333     | 0.9271   | Females are more likely to be missed compared to males. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation  \n",
    "\n",
    "- **Females (unprivileged):**  \n",
    "  - Experience **substantially worse performance** across accuracy, F1, and precision.  \n",
    "  - Their **precision (0.3571)** is especially poor, meaning many of their positive predictions are false alarms.  \n",
    "  - Although sensitivity (TPR = 0.8333) is acceptable, it lags behind males, leading to more missed cases.  \n",
    "\n",
    "- **Males (privileged):**  \n",
    "  - Benefit from higher performance across nearly all metrics.  \n",
    "  - Stronger accuracy (0.8493), much higher F1 (0.8900), and reliable precision (0.8558).  \n",
    "  - Their ROC AUC (0.8539) also reflects better ranking ability than for females.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "The Decision Tree model introduces a **systematic disadvantage for females**:  \n",
    "- Their predictions are much less reliable, with **very low precision and F1-score**.  \n",
    "- While males achieve strong predictive performance, females face both higher false positives and weaker sensitivity.  \n",
    "\n",
    "This aligns with the fairness metrics, showing that the Decision Tree disproportionately **favors males (privileged group)** while disadvantaging females in predictive reliability.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53556439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.8333\n",
      "  False Positive Rate (FPR): 0.2812\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9271\n",
      "  False Positive Rate (FPR): 0.3000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_dt == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_dt == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97580f6",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Decision Tree Model  \n",
    "\n",
    "To further assess fairness at the subgroup level, we examine the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Results by Gender Group  \n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.8333  | 0.2812  |\n",
    "| Privileged (Male)      | 0.9271  | 0.3000  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation  \n",
    "\n",
    "- The **privileged group (male)** achieves a **higher TPR (92.71%)** compared to females (83.33%).  \n",
    "  - This means males are **more likely to be correctly identified** when they truly have the condition.  \n",
    "\n",
    "- However, the **FPR is high for both groups**:  \n",
    "  - Females: **28.12%**  \n",
    "  - Males: **30.00%**  \n",
    "  - Both groups face a considerable rate of false alarms, but males slightly more so.  \n",
    "\n",
    "- The disparity is **more pronounced in sensitivity (TPR)**, where females risk being under-identified compared to males.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Summary  \n",
    "\n",
    "The Decision Tree model shows a **gender imbalance in detection**:  \n",
    "- **Males benefit from stronger sensitivity**, meaning they are more reliably detected when positive.  \n",
    "- **Both groups suffer from high false positive rates**, which reduces overall trust in predictions.  \n",
    "- The results highlight that while detection is slightly skewed in favor of males, **false alarms remain a fairness concern across both genders**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5168d",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaa2a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true  y_prob  y_pred\n",
      "0    1       1  0.8975       1\n",
      "1    1       1  0.1550       0\n",
      "2    1       1  0.8625       1\n",
      "3    1       1  0.2225       0\n",
      "4    0       0  0.2400       0\n"
     ]
    }
   ],
   "source": [
    "rf_df = pd.read_csv(\"HeartFailureData_75F25M_RF_predictions.csv\")\n",
    "print(rf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "278af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_rf = rf_df[\"y_true\"].values\n",
    "y_pred_rf = rf_df[\"y_pred\"].values\n",
    "y_prob_rf = rf_df[\"y_prob\"].values\n",
    "gender_rf = rf_df[\"Sex\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_rf = gender_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "689f0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0494\n",
      "               Balanced Accuracy Difference            0.0800\n",
      "               Balanced Accuracy Ratio                 1.0974\n",
      "               Disparate Impact Ratio                  0.2712\n",
      "               Equal Odds Difference                  -0.1288\n",
      "               Equal Odds Ratio                        0.1953\n",
      "               Positive Predictive Parity Difference  -0.0725\n",
      "               Positive Predictive Parity Ratio        0.9199\n",
      "               Statistical Parity Difference          -0.4243\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Gender Bias Report\n",
    "print(\"\\n--- Random Forest Gender Bias Report ---\")\n",
    "\n",
    "rf_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf,\n",
    "    prtc_attr=protected_attr_rf,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(rf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7a8e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1d1a5_row0_col0, #T_1d1a5_row1_col0, #T_1d1a5_row3_col0, #T_1d1a5_row4_col0, #T_1d1a5_row5_col0, #T_1d1a5_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1d1a5\">\n",
       "  <caption>Random Forest Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1d1a5_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1d1a5_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_1d1a5_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_1d1a5_row0_col0\" class=\"data row0 col0\" >0.0494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d1a5_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_1d1a5_row1_col0\" class=\"data row1 col0\" >0.0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d1a5_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_1d1a5_row2_col0\" class=\"data row2 col0\" >1.0974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d1a5_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_1d1a5_row3_col0\" class=\"data row3 col0\" >0.2712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d1a5_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_1d1a5_row4_col0\" class=\"data row4 col0\" >-0.1288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d1a5_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_1d1a5_row5_col0\" class=\"data row5 col0\" >0.1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d1a5_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_1d1a5_row6_col0\" class=\"data row6 col0\" >-0.0725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d1a5_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_1d1a5_row7_col0\" class=\"data row7 col0\" >0.9199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d1a5_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_1d1a5_row8_col0\" class=\"data row8 col0\" >-0.4243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1d1a5_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_1d1a5_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_1d1a5_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22ac4db3be0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for Random Forest\n",
    "styled_rf = MyFlagger().apply_flag(\n",
    "    df=rf_bias,\n",
    "    caption=\"Random Forest Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7c650",
   "metadata": {},
   "source": [
    "## Random Forest Fairness Analysis by Gender  \n",
    "\n",
    "This table summarizes **fairness metrics** for the Random Forest model, focusing on gender bias.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics  \n",
    "\n",
    "- **AUC Difference (0.0494)**: Slight difference in ROC AUC between genders, suggesting somewhat better ranking performance for one group.  \n",
    "- **Balanced Accuracy Difference (0.0800)** and **Ratio (1.0974)**: Indicates that males benefit from higher balanced accuracy, reflecting better sensitivity-specificity balance.  \n",
    "- **Disparate Impact Ratio (0.2712)**: Far below the acceptable fairness threshold (0.80–1.25), indicating **substantial inequality in selection rates**, disadvantaging females.  \n",
    "- **Equal Odds Difference (−0.1288)** and **Equal Odds Ratio (0.1953)**: Shows notable disparities in error rates (TPR and FPR), suggesting males are treated more favorably.  \n",
    "- **Positive Predictive Parity Difference (−0.0725)** and **Ratio (0.9199)**: Predictions are less precise for females, indicating lower reliability of positive classifications.  \n",
    "- **Statistical Parity Difference (−0.4243)**: Strongly negative value confirms that females are **selected much less frequently** than males.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation  \n",
    "\n",
    "- The Random Forest model introduces **systematic bias** against females.  \n",
    "- Females experience **lower selection rates, reduced precision, and worse balanced accuracy**, making their predictions less reliable.  \n",
    "- Error distribution (Equal Odds) shows clear imbalances, with males receiving more favorable treatment across both true and false positive outcomes.  \n",
    "- Statistical parity and disparate impact measures further confirm **structural disadvantage for females**, as they are consistently under-selected compared to males.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "\n",
    "The Random Forest model demonstrates **strong gender bias**, primarily disadvantaging females (unprivileged group).  \n",
    "Despite overall good model performance, fairness metrics reveal significant disparities in **selection rates, predictive reliability, and error distribution**.   \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47479386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - RF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.9112</td>\n",
       "      <td>0.1288</td>\n",
       "      <td>5.1200</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>1.0871</td>\n",
       "      <td>0.4243</td>\n",
       "      <td>3.6872</td>\n",
       "      <td>-0.0312</td>\n",
       "      <td>0.9625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.0974</td>\n",
       "      <td>-0.1288</td>\n",
       "      <td>0.1953</td>\n",
       "      <td>-0.0725</td>\n",
       "      <td>0.9199</td>\n",
       "      <td>-0.4243</td>\n",
       "      <td>0.2712</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>1.0390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                         -0.08   \n",
       "1          Sex             1                          0.08   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9112    0.1288     5.1200    0.0725     1.0871   \n",
       "1                   1.0974   -0.1288     0.1953   -0.0725     0.9199   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.4243           3.6872   -0.0312     0.9625  \n",
       "1         -0.4243           0.2712    0.0312     1.0390  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - RF\")\n",
    "measure.bias(X_test, y_test, y_pred_rf, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be772b16",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – Random Forest by Gender  \n",
    "\n",
    "This table presents **group-specific fairness metrics** for the Random Forest model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy  \n",
    "- **Females (0):** Balanced Accuracy Difference = **−0.0800**, Ratio = **0.9112**  \n",
    "- **Males (1):** Balanced Accuracy Difference = **+0.0800**, Ratio = **1.0974**  \n",
    "- ➝ The model is more balanced and accurate for **males**, while females experience worse balanced accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)  \n",
    "- **Females (0):** FPR Difference = **+0.1288**, Ratio = **5.1200**  \n",
    "- **Males (1):** FPR Difference = **−0.1288**, Ratio = **0.1953**  \n",
    "- ➝ Females face a **much higher false positive rate**, meaning they are more often incorrectly classified as positive.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)  \n",
    "- **Females (0):** PPV Difference = **+0.0725**, Ratio = **1.0871**  \n",
    "- **Males (1):** PPV Difference = **−0.0725**, Ratio = **0.9199**  \n",
    "- ➝ Predictions are **slightly more precise for females**, while males experience lower precision.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate  \n",
    "- **Females (0):** Selection Difference = **+0.4243**, Ratio = **3.6872**  \n",
    "- **Males (1):** Selection Difference = **−0.4243**, Ratio = **0.2712**  \n",
    "- ➝ Females are **selected much more often** than males, showing a strong imbalance.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)  \n",
    "- **Females (0):** TPR Difference = **−0.0312**, Ratio = **0.9625**  \n",
    "- **Males (1):** TPR Difference = **+0.0312**, Ratio = **1.0390**  \n",
    "- ➝ The model is **slightly more sensitive for males**, detecting a few more true cases compared to females.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "- The Random Forest model shows **mixed bias patterns**:  \n",
    "  - **Females** suffer from a **high false positive rate**, meaning they are more often incorrectly flagged as positive.  \n",
    "  - At the same time, they benefit from **higher selection rates and slightly better precision**.  \n",
    "  - **Males** have **lower FPR and higher sensitivity**, suggesting they are more accurately classified overall, but their predictions are slightly less precise.  \n",
    "\n",
    "Overall, the Random Forest introduces **imbalances in error distribution and selection**, disadvantaging **females in terms of false positives**, while **males face disadvantages in selection and precision**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6573254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.4946</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.8497</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9011</td>\n",
       "      <td>0.9231</td>\n",
       "      <td>0.8039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>0.8333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.5822</td>\n",
       "      <td>0.8151</td>\n",
       "      <td>0.8508</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9059</td>\n",
       "      <td>0.9038</td>\n",
       "      <td>0.8021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.4946    0.8424   \n",
       "1           Sex             0   38.0       0.1579           0.1579    0.9474   \n",
       "2           Sex             1  146.0       0.6575           0.5822    0.8151   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8497  0.1098      —     0.9011   0.9231  0.8039  \n",
       "1    0.8333  0.0312      —     0.8333   0.9531  0.8333  \n",
       "2    0.8508  0.1600      —     0.9059   0.9038  0.8021  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_rf = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_rf = perf_table_rf.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de72316",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Random Forest by Gender  \n",
    "\n",
    "This table shows the **stratified performance metrics** of the Random Forest model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)  \n",
    "- **Accuracy (0.8424)** and **F1-score (0.8497)** indicate solid overall classification performance.  \n",
    "- **ROC AUC (0.9231)** shows strong discriminatory ability.  \n",
    "- **Precision (0.9011)** is high, suggesting reliable positive predictions.  \n",
    "- **TPR (0.8039)** indicates reasonable sensitivity, though subgroup breakdowns reveal imbalances.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison  \n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.9474     | 0.8151   | Accuracy is substantially higher for females. |\n",
    "| **F1-Score**  | 0.8333     | 0.8508   | F1 is slightly better for males, though close. |\n",
    "| **FPR**       | 0.0312     | 0.1600   | Females experience far fewer false positives. |\n",
    "| **Precision** | 0.8333     | 0.9059   | Males benefit from more reliable predictions. |\n",
    "| **ROC AUC**   | 0.9531     | 0.9038   | ROC AUC is higher for females, suggesting stronger ranking performance. |\n",
    "| **TPR**       | 0.8333     | 0.8021   | Females have a slightly higher sensitivity than males. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation  \n",
    "- **Females (unprivileged)**:  \n",
    "  - Benefit from **higher accuracy (94.7%)** and **better ROC AUC (0.9531)**.  \n",
    "  - Experience **much lower false positive rates (3.1%)**, which reduces unnecessary misclassifications.  \n",
    "  - Precision (0.8333) is lower than for males, meaning their positive predictions are less reliable.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Have **lower accuracy (81.5%)** and weaker ROC AUC (0.9038).  \n",
    "  - Precision (0.9059) is higher, making their positive predictions more trustworthy.  \n",
    "  - However, they face **more false positives (16%)** and slightly worse sensitivity (TPR = 0.8021).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "The Random Forest model shows a **mixed fairness pattern**:  \n",
    "- **Females** perform better in terms of accuracy, sensitivity, ROC AUC, and especially false positive rate.  \n",
    "- **Males** benefit from higher precision and slightly better F1.  \n",
    "\n",
    "This indicates that while females are overall **better protected against misclassification errors**, males enjoy **more reliable positive predictions**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a52946a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.8333\n",
      "  False Positive Rate (FPR): 0.0312\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.8021\n",
      "  False Positive Rate (FPR): 0.1600\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_rf == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_rf == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06d248",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Random Forest Model  \n",
    "\n",
    "To assess fairness at the subgroup level, we compared the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Results by Gender Group  \n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.8333  | 0.0312  |\n",
    "| Privileged (Male)      | 0.8021  | 0.1600  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation  \n",
    "\n",
    "- **Females (unprivileged group)** achieve a slightly higher **TPR (83.33%)** than males (80.21%), meaning women are somewhat more likely to be correctly identified when they have CVD.  \n",
    "- At the same time, females have a **much lower FPR (3.12%)** compared to males (16.00%), indicating that they are far less likely to be incorrectly flagged as having CVD.  \n",
    "- This combination suggests that the model is **both more sensitive and more specific for females**, providing them with more favorable outcomes.  \n",
    "- In contrast, males are disadvantaged, as they face both a higher risk of **missed true cases (lower TPR)** and **false alarms (higher FPR)**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Summary  \n",
    "\n",
    "The Random Forest model shows a **reverse bias pattern**, favoring the unprivileged group (females).  \n",
    "- Females experience **better diagnostic accuracy**, with stronger sensitivity and far fewer false positives.  \n",
    "- Males, despite being the privileged group, face **higher misclassification risks**, which highlights an imbalance in model behavior that may require bias mitigation.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136de6",
   "metadata": {},
   "source": [
    "### Deep Learning Model - Feed Forward Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c69d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sex  y_true    y_prob  y_pred\n",
      "0    1       1  0.997220       1\n",
      "1    1       1  0.000009       0\n",
      "2    1       1  0.791310       1\n",
      "3    1       1  0.001596       0\n",
      "4    0       0  0.000887       0\n"
     ]
    }
   ],
   "source": [
    "mlp_df = pd.read_csv(\"HeartFailureData_75F25M_RecallFirstTunedMLP_predictions.csv\")\n",
    "print(mlp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f65a498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns \n",
    "y_true_mlp = mlp_df[\"y_true\"].values \n",
    "y_prob_mlp = mlp_df[\"y_prob\"].values\n",
    "y_pred_mlp = mlp_df[\"y_pred\"].values\n",
    "gender_mlp = mlp_df[\"Sex\"].values \n",
    "\n",
    "# Use gender_mlp as the protected attribute\n",
    "protected_attr_mlp = gender_mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6969bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.0065\n",
      "               Balanced Accuracy Difference            0.0267\n",
      "               Balanced Accuracy Ratio                 1.0337\n",
      "               Disparate Impact Ratio                  0.2183\n",
      "               Equal Odds Difference                  -0.1888\n",
      "               Equal Odds Ratio                        0.1420\n",
      "               Positive Predictive Parity Difference  -0.0750\n",
      "               Positive Predictive Parity Ratio        0.9143\n",
      "               Statistical Parity Difference          -0.4712\n",
      "Data Metrics   Prevalence of Privileged Class (%)     79.0000\n"
     ]
    }
   ],
   "source": [
    "#Run fairmlhealth bias detection for MLP \n",
    "\n",
    "mlp_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp,\n",
    "    prtc_attr=protected_attr_mlp,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(mlp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbf7eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_44d7a_row1_col0, #T_44d7a_row3_col0, #T_44d7a_row4_col0, #T_44d7a_row5_col0, #T_44d7a_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_44d7a\">\n",
       "  <caption>MLP Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_44d7a_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_44d7a_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_44d7a_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_44d7a_row0_col0\" class=\"data row0 col0\" >-0.0065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44d7a_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_44d7a_row1_col0\" class=\"data row1 col0\" >0.0267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44d7a_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_44d7a_row2_col0\" class=\"data row2 col0\" >1.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44d7a_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_44d7a_row3_col0\" class=\"data row3 col0\" >0.2183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44d7a_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_44d7a_row4_col0\" class=\"data row4 col0\" >-0.1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44d7a_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_44d7a_row5_col0\" class=\"data row5 col0\" >0.1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44d7a_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_44d7a_row6_col0\" class=\"data row6 col0\" >-0.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44d7a_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_44d7a_row7_col0\" class=\"data row7 col0\" >0.9143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44d7a_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_44d7a_row8_col0\" class=\"data row8 col0\" >-0.4712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44d7a_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_44d7a_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_44d7a_row9_col0\" class=\"data row9 col0\" >79.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22ac4e2e680>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for MLP\n",
    "styled_mlp = MyFlagger().apply_flag(\n",
    "    df=mlp_bias,\n",
    "    caption=\"MLP Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713caa5",
   "metadata": {},
   "source": [
    "## Gender Bias Detection Results for MLP Model  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics  \n",
    "\n",
    "- **AUC Difference (−0.0065)**: Minimal disparity, showing that ranking quality between genders is nearly equal.  \n",
    "- **Balanced Accuracy Difference (+0.0267)** and **Ratio (1.0337)**: Slightly favors females, who achieve marginally higher balanced accuracy.  \n",
    "- **Disparate Impact Ratio (0.2183)**: Far below the fairness guideline of 0.80–1.25, revealing **severe inequality in selection rates**, strongly disadvantaging females.  \n",
    "- **Equal Odds Difference (−0.1888)** and **Equal Odds Ratio (0.1420)**: Substantial disparity in error rates (TPR and FPR), with outcomes skewed toward males.  \n",
    "- **Positive Predictive Parity Difference (−0.0750)** and **Ratio (0.9143)**: Predictions for females are **less precise**, meaning their positive classifications are less trustworthy.  \n",
    "- **Statistical Parity Difference (−0.4712)**: Indicates a major shortfall in positive outcomes for females compared to males.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation  \n",
    "\n",
    "- The MLP model exhibits **systematic fairness concerns**, despite strong overall predictive capability.  \n",
    "- Females face significant disadvantages in:  \n",
    "  - **Selection rates** (very low compared to males).  \n",
    "  - **Error distribution**, with much worse equal odds performance.  \n",
    "  - **Precision**, as predictions are less reliable for females.  \n",
    "- Males benefit disproportionately, reflected in both statistical and predictive parity measures.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "\n",
    "The MLP model shows **marked gender bias in favor of males (privileged group)**.  \n",
    "- While AUC and balanced accuracy differences are relatively small, the **large gaps in selection rates, equal odds, and statistical parity** reveal **severe inequity**.  \n",
    "- This means the MLP model consistently provides **better opportunities and more reliable outcomes for males**, while **systematically disadvantaging females**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aa66097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - MLP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0267</td>\n",
       "      <td>0.9674</td>\n",
       "      <td>0.1888</td>\n",
       "      <td>7.040</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.0938</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>4.5808</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>1.2031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>1.0337</td>\n",
       "      <td>-0.1888</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>-0.4712</td>\n",
       "      <td>0.2183</td>\n",
       "      <td>-0.1354</td>\n",
       "      <td>0.8312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0          Sex             0                       -0.0267   \n",
       "1          Sex             1                        0.0267   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9674    0.1888      7.040     0.075     1.0938   \n",
       "1                   1.0337   -0.1888      0.142    -0.075     0.9143   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.4712           4.5808    0.1354     1.2031  \n",
       "1         -0.4712           0.2183   -0.1354     0.8312  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - MLP\")\n",
    "measure.bias(X_test, y_test, y_pred_mlp, features=['Sex'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576976f",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – MLP by Gender  \n",
    "\n",
    "This table presents **group-specific fairness metrics** for the MLP model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy  \n",
    "- **Females (0):** Balanced Accuracy Difference = **−0.0267**, Ratio = **0.9674**  \n",
    "- **Males (1):** Balanced Accuracy Difference = **+0.0267**, Ratio = **1.0337**  \n",
    "- ➝ Males have a slight advantage in balanced accuracy, while females perform somewhat worse.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)  \n",
    "- **Females (0):** FPR Difference = **+0.1888**, Ratio = **7.0400**  \n",
    "- **Males (1):** FPR Difference = **−0.1888**, Ratio = **0.1420**  \n",
    "- ➝ Females are **far more likely to receive false positives**, showing a strong disadvantage.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)  \n",
    "- **Females (0):** PPV Difference = **+0.0750**, Ratio = **1.0938**  \n",
    "- **Males (1):** PPV Difference = **−0.0750**, Ratio = **0.9143**  \n",
    "- ➝ Precision is slightly better for females, meaning when they are predicted positive, it is more often correct.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate  \n",
    "- **Females (0):** Selection Difference = **+0.4712**, Ratio = **4.5808**  \n",
    "- **Males (1):** Selection Difference = **−0.4712**, Ratio = **0.2183**  \n",
    "- ➝ The model **selects females at a much higher rate**, which may indicate over-prediction of positives for them.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)  \n",
    "- **Females (0):** TPR Difference = **+0.1354**, Ratio = **1.2031**  \n",
    "- **Males (1):** TPR Difference = **−0.1354**, Ratio = **0.8312**  \n",
    "- ➝ Females have higher sensitivity, meaning their true cases are more often detected than males’.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "- The MLP model **favors females in selection rate, precision, and sensitivity (TPR)**.  \n",
    "- However, this comes at the cost of a **much higher false positive rate for females**, which reduces fairness in error distribution.  \n",
    "- Males, while slightly disadvantaged in TPR and selection, experience **lower false positives**, making predictions more conservative for them.  \n",
    "\n",
    "Overall, the MLP introduces a **reverse bias**: it systematically **favors females over males**, but at the same time increases the **burden of false positives** for the female group.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbc2caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['Sex']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.8308</td>\n",
       "      <td>0.1463</td>\n",
       "      <td>—</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.7941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>—</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.8646</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>1</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.8370</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>—</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>0.8021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  184.0       0.5543           0.5054    0.8207   \n",
       "1           Sex             0   38.0       0.1579           0.1316    0.9211   \n",
       "2           Sex             1  146.0       0.6575           0.6027    0.7945   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.8308  0.1463      —      0.871   0.8968  0.7941  \n",
       "1    0.7273  0.0312      —      0.800   0.8646  0.6667  \n",
       "2    0.8370  0.2200      —      0.875   0.8710  0.8021  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_mlp = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_mlp = perf_table_mlp.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec7c2f",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – MLP by Gender  \n",
    "\n",
    "This table shows the **stratified performance metrics** of the MLP model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)  \n",
    "- **Accuracy (0.8207)** and **F1-score (0.8308)** indicate good but not perfect classification ability.  \n",
    "- **ROC AUC (0.8968)** reflects strong discriminatory power.  \n",
    "- **Precision (0.8710)** is high, showing predictions are generally reliable.  \n",
    "- **TPR (0.7941)** suggests reasonable sensitivity, though subgroup analysis reveals disparities.  \n",
    "- **Note**: PR AUC is not available (“—”) due to subgroup calculation limits.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison  \n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.9211     | 0.7945   | Accuracy is higher for females, suggesting better classification for this group. |\n",
    "| **F1-Score**  | 0.7273     | 0.8370   | Males benefit from a stronger balance between precision and recall. |\n",
    "| **FPR**       | 0.0312     | 0.2200   | Females experience far fewer false positives compared to males. |\n",
    "| **Precision** | 0.8000     | 0.8750   | Predictions are more reliable for males than for females. |\n",
    "| **ROC AUC**   | 0.8646     | 0.8710   | Both groups show strong ranking performance, with males slightly ahead. |\n",
    "| **TPR**       | 0.6667     | 0.8021   | Females are more likely to be missed (lower sensitivity). |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation  \n",
    "- **Females (unprivileged)**:  \n",
    "  - Benefit from **higher accuracy** and **much lower false positive rates** (3.12%).  \n",
    "  - However, they suffer from a **lower recall/TPR (66.7%)**, meaning more missed true cases.  \n",
    "  - Precision (0.8000) is weaker, making positive predictions less trustworthy.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Achieve **better F1 and TPR**, showing stronger overall detection ability.  \n",
    "  - However, they experience **substantially higher false positive rates (22%)**, which may lead to over-diagnosis.  \n",
    "  - Predictions are more precise (0.8750), reducing incorrect positives.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "The MLP model exhibits **mixed gender disparities**:  \n",
    "- Females gain **higher accuracy and fewer false alarms**, but at the cost of **lower sensitivity** (missed true cases).  \n",
    "- Males benefit from **higher recall and F1-score**, but face a **greater false positive burden**.  \n",
    "\n",
    "This indicates that the MLP is **not uniformly fair**: it balances error differently across genders, with each group facing different trade-offs.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379dbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6667\n",
      "  False Positive Rate (FPR): 0.0312\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.8021\n",
      "  False Positive Rate (FPR): 0.2200\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_mlp == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_mlp == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fea8d1",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – MLP Model  \n",
    "\n",
    "To further assess fairness, the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** are compared between the unprivileged (female) and privileged (male) groups.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Results by Gender Group  \n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.6667  | 0.0312  |\n",
    "| Privileged (Male)      | 0.8021  | 0.2200  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation  \n",
    "\n",
    "- The **True Positive Rate (TPR)** is higher for males (80.21%) than for females (66.67%).  \n",
    "  - This means males are **more likely to be correctly identified** when they have CVD, while females face a greater risk of missed diagnoses.  \n",
    "\n",
    "- The **False Positive Rate (FPR)** is much lower for females (3.12%) compared to males (22.00%).  \n",
    "  - This indicates that females are **less likely to be incorrectly flagged** as having CVD, while males face a substantial risk of false alarms.  \n",
    "\n",
    "- These results show a **trade-off in error distribution**:  \n",
    "  - Females experience **higher under-detection (low TPR)** but fewer false positives.  \n",
    "  - Males benefit from **higher sensitivity (high TPR)** but at the cost of a much larger false positive rate.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Summary  \n",
    "\n",
    "The MLP model creates **different fairness concerns** across genders:  \n",
    "- **Females (unprivileged group)** are more often missed (lower TPR).  \n",
    "- **Males (privileged group)** are more frequently misclassified as positive (higher FPR).  \n",
    "\n",
    "This highlights that the model’s **error balance is uneven**, with each gender disadvantaged in different ways.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
