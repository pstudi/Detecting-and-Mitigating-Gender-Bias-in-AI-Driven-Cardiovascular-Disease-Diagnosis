{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288d499e",
   "metadata": {},
   "source": [
    "## Bias Mitigation using Fairlearn - Heart Failure Prediction Dataset (Source: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e647717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>130.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>140.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>105.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
       "0   33    0              3      100.0        246.0          0           0   \n",
       "1   48    0              1      120.0        284.0          0           0   \n",
       "2   49    0              3      130.0        269.0          0           0   \n",
       "3   62    0              3      140.0        268.0          0           2   \n",
       "4   38    0              3      105.0        236.0          1           0   \n",
       "\n",
       "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n",
       "0  150.0               1      1.0         1             1  \n",
       "1  120.0               0      0.0         2             0  \n",
       "2  163.0               0      0.0         2             0  \n",
       "3  160.0               0      3.6         0             1  \n",
       "4  166.0               0      2.8         2             1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load preprocessed data \n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"./data_subsets/train_25M_75F.csv\")\n",
    "\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")\n",
    "\n",
    "#check out the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5330b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y_test is a Series (not a DataFrame with 1 column)\n",
    "y_test = y_test.squeeze(\"columns\")\n",
    "\n",
    "# Define target and sensitive column names\n",
    "TARGET = \"HeartDisease\"\n",
    "SENSITIVE = \"Sex\"\n",
    "\n",
    "# Split train into X/y\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "# Extract sensitive features separately\n",
    "A_train = X_train[SENSITIVE].astype(int)\n",
    "A_test  = X_test[SENSITIVE].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749ae738",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"HeartDisease\"\n",
    "SENSITIVE = \"Sex\"   # 1 = Male, 0 = Female\n",
    "\n",
    "categorical_cols = ['Sex','ChestPainType','FastingBS','RestingECG','ExerciseAngina','ST_Slope']\n",
    "continuous_cols  = ['Age','RestingBP','Cholesterol','MaxHR','Oldpeak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fe70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into X / y and keep sensitive feature for fairness evaluation\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d93327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric features only, fit on train, transform test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_train.index\n",
    ")\n",
    "X_test_num_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9f63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode categoricals; numeric are kept as is \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\", sparse_output=False)\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "X_train_cat = pd.DataFrame(\n",
    "    ohe.transform(X_train[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_cat = pd.DataFrame(\n",
    "    ohe.transform(X_test[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e79e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shapes: (600, 18) (184, 18)\n"
     ]
    }
   ],
   "source": [
    "# Assemble final matrices\n",
    "X_train_ready = pd.concat([X_train_cat, X_train_num_scaled], axis=1)\n",
    "X_test_ready  = pd.concat([X_test_cat,  X_test_num_scaled],  axis=1)\n",
    "\n",
    "print(\"Final feature shapes:\", X_train_ready.shape, X_test_ready.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade7ec1",
   "metadata": {},
   "source": [
    "### Traditional ML Models - Baseline: K-Nearest Neighbors (KNN) & Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf455d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "#define a function \n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"=== {model_name} Evaluation ===\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred, average='binary'))\n",
    "    print(\"Recall   :\", recall_score(y_true, y_pred, average='binary'))\n",
    "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='binary'))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f7163",
   "metadata": {},
   "source": [
    "### PCA-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7c94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned PCA+KNN, no mitigation) ===\n",
      "=== KNN (best params) Evaluation ===\n",
      "Accuracy : 0.8315217391304348\n",
      "Precision: 0.9080459770114943\n",
      "Recall   : 0.7745098039215687\n",
      "F1 Score : 0.8359788359788359\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.83        82\n",
      "           1       0.91      0.77      0.84       102\n",
      "\n",
      "    accuracy                           0.83       184\n",
      "   macro avg       0.84      0.84      0.83       184\n",
      "weighted avg       0.84      0.83      0.83       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[74  8]\n",
      " [23 79]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "#1) PCA + KNN pipeline (on one-hot encoded + scaled features)\n",
    "pca_knn = Pipeline([\n",
    "    ('pca', PCA(n_components=0.95, random_state=42)),  # keep 95% variance\n",
    "    ('knn', KNeighborsClassifier(\n",
    "        n_neighbors=15, metric='manhattan', weights='distance'\n",
    "    ))\n",
    "])\n",
    "\n",
    "pca_knn.fit(X_train_ready, y_train)\n",
    "\n",
    "# Inspect PCA details\n",
    "n_comp = pca_knn.named_steps['pca'].n_components_\n",
    "expl_var = pca_knn.named_steps['pca'].explained_variance_ratio_.sum()\n",
    "\n",
    "print(\"=== Baseline (tuned PCA+KNN, no mitigation) ===\")\n",
    "# 2) Evaluate \n",
    "y_pred_pca_knn = pca_knn.predict(X_test_ready)\n",
    "probs_pca_knn = pca_knn.predict_proba(X_test_ready)[:, 1]\n",
    "  \n",
    "evaluate_model(y_test, y_pred_pca_knn, \"KNN (best params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34404c3",
   "metadata": {},
   "source": [
    "### Post-Processing -  KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f8d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned PCA+KNN) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.03125  0.666667       0.131579  0.921053\n",
      "1    0.781250  0.14000  0.781250       0.561644  0.808219\n",
      "Accuracy: 0.8315 | DP diff: 0.4301 | EO diff: 0.1146\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.03125  0.666667       0.131579  0.921053\n",
      "1    0.781250  0.14000  0.781250       0.561644  0.808219\n",
      "Accuracy: 0.8315 | DP diff: 0.4301 | EO diff: 0.1146\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.03125  0.666667       0.131579  0.921053\n",
      "1    0.781250  0.14000  0.781250       0.561644  0.808219\n",
      "Accuracy: 0.8315 | DP diff: 0.4301 | EO diff: 0.1146\n"
     ]
    }
   ],
   "source": [
    "# Demographic Parity post-processing for your tuned PCA+KNN\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function\n",
    "def eval_fairness(y_true, y_pred, A):\n",
    "    mf = MetricFrame(\n",
    "        metrics={\n",
    "            \"TPR\": true_positive_rate,\n",
    "            \"FPR\": false_positive_rate,\n",
    "            \"Recall\": recall_score, \n",
    "            \"SelectionRate\": selection_rate,\n",
    "            \"Accuracy\": accuracy_score,\n",
    "        },\n",
    "        y_true=y_true, y_pred=y_pred, sensitive_features=A\n",
    "    )\n",
    "    return {\n",
    "        \"by_group\": mf.by_group,\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"dp\": demographic_parity_difference(y_true, y_pred, sensitive_features=A),\n",
    "        \"eo\": equalized_odds_difference(y_true, y_pred, sensitive_features=A),\n",
    "    }\n",
    "\n",
    "# 1) Baseline metrics (no mitigation) \n",
    "pca_knn.fit(X_train_ready, y_train)\n",
    "y_base = pca_knn.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (tuned PCA+KNN) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing with DEMOGRAPHIC PARITY\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Post-processing with EQUALIZED ODDS\n",
    "post_eod = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True,                               \n",
    ")\n",
    "post_eod.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_eod = post_eod.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eod = eval_fairness(y_test, y_eod, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eod[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod['acc']:.4f} | DP diff: {m_eod['dp']:.4f} | EO diff: {m_eod['eo']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d0918",
   "metadata": {},
   "source": [
    "### Bias Mitigation Results: PCA+KNN – Post-Processing\n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Model                    | Accuracy | DP diff | EO diff | Notes                                                             |\n",
    "|--------------------------|:--------:|:-------:|:-------:|-------------------------------------------------------------------|\n",
    "| PCA+KNN Baseline (tuned) | 0.8315   | 0.4301  | 0.1146  | Large DP gap; moderate EO gap                                     |\n",
    "| PCA+KNN + PP (DP)        | 0.8315   | 0.4301  | 0.1146  | **Identical to baseline** → post-processing not applied/effective |\n",
    "| PCA+KNN + PP (EO)        | 0.8315   | 0.4301  | 0.1146  | **Identical to baseline** → post-processing not applied/effective |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- Baseline shows **strong outcome disparity** (DP ≈ 0.43) and **non-trivial error-rate gap** (EO ≈ 0.115).\n",
    "- Both post-processing runs show **no change**, suggesting scores weren’t used or mitigation wasn’t applied at inference.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb782e92",
   "metadata": {},
   "source": [
    "**CorrelationRemover** will be implemented to improve fairness after DP/EOD post-processing failed to change any predictions (0% flips), leaving metrics unchanged. By removing linear correlation between features and the sensitive attribute, we reduce leakage and make group score distributions more comparable, giving PCA+KNN and also any subsequent post-processing room to adjust selection rates and error rates—all while staying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f37790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessing: CorrelationRemover + PCA+KNN ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.500000  0.125  0.500000       0.184211  0.815789\n",
      "1    0.802083  0.200  0.802083       0.595890  0.801370\n",
      "Accuracy: 0.8043 | DP diff: 0.4117 | EO diff: 0.3021\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "from sklearn.metrics import recall_score  \n",
    "\n",
    "Xtr_df = X_train_ready.copy()\n",
    "Xte_df = X_test_ready.copy()\n",
    "Xtr_df[\"__A__\"] = A_train.values\n",
    "Xte_df[\"__A__\"] = A_test.values\n",
    "\n",
    "cr = CorrelationRemover(sensitive_feature_ids=[\"__A__\"])\n",
    "\n",
    "Xtr_fair_arr = cr.fit_transform(Xtr_df)   # shape: (n_samples, n_features - 1)\n",
    "Xte_fair_arr = cr.transform(Xte_df)\n",
    "\n",
    "# Rebuild DataFrames with columns that exclude the sensitive column\n",
    "cols_out = [c for c in Xtr_df.columns if c != \"__A__\"]\n",
    "Xtr_fair = pd.DataFrame(Xtr_fair_arr, index=Xtr_df.index, columns=cols_out)\n",
    "Xte_fair = pd.DataFrame(Xte_fair_arr, index=Xte_df.index, columns=cols_out)\n",
    "\n",
    "# Refit your PCA+KNN\n",
    "pca_knn.fit(Xtr_fair, y_train)\n",
    "y_cr = pca_knn.predict(Xte_fair)\n",
    "m_cr = eval_fairness(y_test, y_cr, A_test)\n",
    "\n",
    "print(\"\\n=== Preprocessing: CorrelationRemover + PCA+KNN ===\")\n",
    "print(m_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_cr['acc']:.4f} | DP diff: {m_cr['dp']:.4f} | EO diff: {m_cr['eo']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc7847",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "- Accuracy **0.8043** (↓ vs baseline **0.8315**, −2.72 pts).\n",
    "- **DP diff = 0.4117**: selection rates **Female 0.184** vs **Male 0.596** (~**3.23×** higher for males) → under-selection of females.\n",
    "- **EO diff = 0.3021** (worse than baseline **0.1146**):\n",
    "  - **TPR:** Female **0.500** vs Male **0.802** (gap **0.302**) → more missed positives among females.\n",
    "  - **FPR:** Female **0.125** vs Male **0.200** (gap **0.075**) → more false alarms among males.\n",
    "- Group accuracy: **Female 0.816** vs **Male 0.801** (gap ~**0.014**, smaller than baseline).\n",
    "\n",
    "CorrelationRemover slightly improves **DP** and narrows **FPR/accuracy gaps**, but **worsens EO** by amplifying the **TPR disparity**—increasing undertreatment risk for **females** and overtreatment risk for **males**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79e2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed vs CR baseline (DP):  0.000%\n",
      "Changed vs CR baseline (eOD): 0.000%\n",
      "\n",
      "=== Post-CR (DP) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.500000  0.125  0.500000       0.184211  0.815789\n",
      "1    0.802083  0.200  0.802083       0.595890  0.801370\n",
      "Accuracy: 0.8043 | DP diff: 0.4117 | EO diff: 0.3021\n",
      "\n",
      "=== Post-CR (eOD) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.500000  0.125  0.500000       0.184211  0.815789\n",
      "1    0.802083  0.200  0.802083       0.595890  0.801370\n",
      "Accuracy: 0.8043 | DP diff: 0.4117 | EO diff: 0.3021\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Demographic Parity on top of the CorrelationRemover\n",
    "post_dp_cr = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    objective=\"accuracy_score\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    prefit=True\n",
    ")\n",
    "post_dp_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)  # ideally fit on a validation split\n",
    "y_dp_cr = post_dp_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_dp_cr = eval_fairness(y_test, y_dp_cr, A_test)\n",
    "\n",
    "# Equalized Odds on top of CorrelationRemover\n",
    "post_eod_cr = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    objective=\"accuracy_score\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    prefit=True\n",
    ")\n",
    "post_eod_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)  # ideally fit on a validation split\n",
    "y_eod_cr = post_eod_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_eod_cr = eval_fairness(y_test, y_eod_cr, A_test)\n",
    "\n",
    "# Check changes vs. CR baseline predictions (y_cr)\n",
    "import numpy as np\n",
    "print(f\"Changed vs CR baseline (DP):  {np.mean(y_dp_cr  != y_cr):.3%}\")\n",
    "print(f\"Changed vs CR baseline (eOD): {np.mean(y_eod_cr != y_cr):.3%}\")\n",
    "\n",
    "print(\"\\n=== Post-CR (DP) ===\")\n",
    "print(m_dp_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp_cr['acc']:.4f} | DP diff: {m_dp_cr['dp']:.4f} | EO diff: {m_dp_cr['eo']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Post-CR (eOD) ===\")\n",
    "print(m_eod_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod_cr['acc']:.4f} | DP diff: {m_eod_cr['dp']:.4f} | EO diff: {m_eod_cr['eo']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0af743",
   "metadata": {},
   "source": [
    "**Interpretation:**  \n",
    "- **No effect:** Post-CR **DP** and **eOD** post-processing changed **0%** — outputs identical to the CR baseline.  \n",
    "- **Disparities persist:** **DP diff 0.4117** (Female sel. **0.184** vs Male **0.596**), **EO diff 0.3021** (TPR **0.500** vs **0.802**; FPR **0.125** vs **0.200**).  \n",
    "- **Accuracy:** **0.8043**, unchanged.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074237d",
   "metadata": {},
   "source": [
    "### Bias mitigation comparison (PCA+KNN)  \n",
    "\n",
    "\n",
    "| Model variant                    | Accuracy | DP diff | EO diff | SelRate S=0 | SelRate S=1 | TPR S=0 | TPR S=1 | FPR S=0 | FPR S=1 | Notes                        |\n",
    "|----------------------------------|:--------:|:-------:|:-------:|:-----------:|:-----------:|:-------:|:-------:|:-------:|:-------:|------------------------------|\n",
    "| Baseline (tuned PCA+KNN)         | 0.8315   | 0.4301  | 0.1146  | 0.1316      | 0.5616      | 0.6667  | 0.7813  | 0.0313  | 0.1400  | Reference                    |\n",
    "| Post-processing (DP constraint)  | 0.8315   | 0.4301  | 0.1146  | 0.1316      | 0.5616      | 0.6667  | 0.7813  | 0.0313  | 0.1400  | **Flips vs baseline: 0%**    |\n",
    "| Post-processing (EO constraint)  | 0.8315   | 0.4301  | 0.1146  | 0.1316      | 0.5616      | 0.6667  | 0.7813  | 0.0313  | 0.1400  | **Flips vs baseline: 0%**    |\n",
    "| CorrelationRemover + PCA+KNN     | 0.8043   | 0.4117  | 0.3021  | 0.1842      | 0.5959      | 0.5000  | 0.8021  | 0.1250  | 0.2000  | New baseline after CR        |\n",
    "| Post-CR (DP constraint)          | 0.8043   | 0.4117  | 0.3021  | 0.1842      | 0.5959      | 0.5000  | 0.8021  | 0.1250  | 0.2000  | **Flips vs CR baseline: 0%** |\n",
    "| Post-CR (EO constraint)          | 0.8043   | 0.4117  | 0.3021  | 0.1842      | 0.5959      | 0.5000  | 0.8021  | 0.1250  | 0.2000  | **Flips vs CR baseline: 0%** |\n",
    "\n",
    "**Takeaway:** Post-processing caused **no label changes** in either setting. Applying **CorrelationRemover** slightly reduced **DP** (0.4301→0.4117) but **worsened EO** (0.1146→0.3021) and lowered accuracy (0.8315→0.8043), with males selected far more often than females.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941cf3a",
   "metadata": {},
   "source": [
    "### Alternative Tuned & Pruned Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66f6420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage A — Best DT params: {'class_weight': {0: 1, 1: 4}, 'criterion': 'entropy', 'max_depth': 4, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Stage A — Best CV Recall: 0.9733\n",
      "Stage B — Best ccp_alpha: 0.000000 | CV Recall: 0.9733\n",
      "=== Alternative Tuned & Pruned Decision Tree Evaluation ===\n",
      "Accuracy : 0.8260869565217391\n",
      "Precision: 0.7966101694915254\n",
      "Recall   : 0.9215686274509803\n",
      "F1 Score : 0.8545454545454545\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.78        82\n",
      "           1       0.80      0.92      0.85       102\n",
      "\n",
      "    accuracy                           0.83       184\n",
      "   macro avg       0.84      0.81      0.82       184\n",
      "weighted avg       0.83      0.83      0.82       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[58 24]\n",
      " [ 8 94]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative DT tuning focused on higher recall\n",
    "# Changes vs previous:\n",
    "#  - Remove calibration (predict uses raw tree probs at 0.5)\n",
    "#  - Tune class_weight (heavier positive weights allowed)\n",
    "#  - Broaden depth a bit but keep regularization via min_samples_* and tiny impurity decrease\n",
    "#  - Prune only with very small ccp_alphas to avoid killing recall\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Simpler-but-expressive trees + tuned class weights\n",
    "base_dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "param_grid_simple = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],                  # add \"log_loss\" if your sklearn supports it\n",
    "    \"max_depth\": [4, 5, 6, 7, 8, 9, 10],               # a bit deeper to help recall\n",
    "    \"min_samples_split\": [5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6],\n",
    "    \"min_impurity_decrease\": [0.0, 1e-4, 1e-3],\n",
    "    \"class_weight\": [\"balanced\", {0:1,1:2}, {0:1,1:3}, {0:1,1:4}],  # stronger push toward positives\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "\n",
    "grid_simple = GridSearchCV(\n",
    "    estimator=base_dt,\n",
    "    param_grid=param_grid_simple,\n",
    "    cv=cv,\n",
    "    scoring=\"recall\",      # prioritize sensitivity for class 1\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True\n",
    ")\n",
    "grid_simple.fit(X_train_ready, y_train)\n",
    "\n",
    "best_params = grid_simple.best_params_\n",
    "print(\"Stage A — Best DT params:\", best_params)\n",
    "print(\"Stage A — Best CV Recall:\", round(grid_simple.best_score_, 4))\n",
    "\n",
    "# Train a zero-pruned model with best params to get the pruning path\n",
    "dt0 = DecisionTreeClassifier(random_state=42, **best_params, ccp_alpha=0.0).fit(X_train_ready, y_train)\n",
    "\n",
    "\n",
    "# Stage B — Gentle cost-complexity pruning (favor small alphas)\n",
    "path = dt0.cost_complexity_pruning_path(X_train_ready, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "# Focus on tiny alphas only + 0.0 to avoid big recall loss\n",
    "small_slice = ccp_alphas[: min(30, len(ccp_alphas))]  # first 30 values are typically the smallest\n",
    "candidate_alphas = np.unique(np.r_[0.0, small_slice])\n",
    "\n",
    "cv_scores = []\n",
    "for alpha in candidate_alphas:\n",
    "    dt_alpha = DecisionTreeClassifier(random_state=42, **best_params, ccp_alpha=alpha)\n",
    "    rec = cross_val_score(dt_alpha, X_train_ready, y_train, cv=cv, scoring=\"recall\", n_jobs=-1).mean()\n",
    "    cv_scores.append((alpha, rec))\n",
    "\n",
    "best_alpha, best_cv_recall = max(cv_scores, key=lambda x: x[1])\n",
    "print(f\"Stage B — Best ccp_alpha: {best_alpha:.6f} | CV Recall: {best_cv_recall:.4f}\")\n",
    "\n",
    "alt_best_dt = DecisionTreeClassifier(random_state=42, **best_params, ccp_alpha=best_alpha).fit(X_train_ready, y_train)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "y_pred = alt_best_dt.predict(X_test_ready)               \n",
    "y_prob = alt_best_dt.predict_proba(X_test_ready)[:, 1]   \n",
    "\n",
    "evaluate_model(y_test, y_pred, \"Alternative Tuned & Pruned Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd1d09",
   "metadata": {},
   "source": [
    "### Bias Mitigation DT: Inprocessing - Exponentiated Gradient Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "680a1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Tuned DT) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.28125  0.833333       0.368421  0.736842\n",
      "1    0.927083  0.30000  0.927083       0.712329  0.849315\n",
      "Accuracy: 0.8261 | DP diff: 0.3439 | EO diff: 0.0938\n",
      "\n",
      "=== In-processing: EG (Equalized Odds) ===\n",
      "          TPR   FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                   \n",
      "0    0.833333  0.25  0.833333       0.342105  0.763158\n",
      "1    0.906250  0.28  0.906250       0.691781  0.842466\n",
      "Accuracy: 0.8261 | DP diff: 0.3497 | EO diff: 0.0729\n",
      "\n",
      "=== In-processing: EG (Demographic Parity) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.28125  0.833333       0.368421  0.736842\n",
      "1    0.864583  0.24000  0.864583       0.650685  0.828767\n",
      "Accuracy: 0.8098 | DP diff: 0.2823 | EO diff: 0.0413\n",
      "\n",
      "=== Decision Tree: Baseline vs In-processing (EG) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.8261</td>\n",
       "      <td>0.3439</td>\n",
       "      <td>0.0938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.8261</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.0729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.2823</td>\n",
       "      <td>0.0413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)    0.8261   0.3439   0.0938\n",
       "1         DT + EG (EO)    0.8261   0.3497   0.0729\n",
       "2         DT + EG (DP)    0.8098   0.2823   0.0413"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-processing mitigation for tuned Decision Tree\n",
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 0) Baseline: tuned DT without mitigation (for comparison)\n",
    "y_pred_dt_base = alt_best_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_pred_dt_base, A_test)\n",
    "print(\"=== Baseline (Tuned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Exponentiated Gradient with Equalized Odds\n",
    "eg_eo = ExponentiatedGradient(\n",
    "    estimator=clone(alt_best_dt),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_eo = eg_eo.predict(X_test_ready)\n",
    "m_eo = eval_fairness(y_test, y_pred_eo, A_test)\n",
    "print(\"\\n=== In-processing: EG (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Exponentiated Gradient with Demographic Parity\n",
    "eg_dp = ExponentiatedGradient(\n",
    "    estimator=clone(alt_best_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_dp = eg_dp.predict(X_test_ready)\n",
    "m_dp = eval_fairness(y_test, y_pred_dp, A_test)\n",
    "print(\"\\n=== In-processing: EG (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary table\n",
    "summary_dt = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (tuned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + EG (EO)\",        \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + EG (DP)\",        \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs In-processing (EG) ===\")\n",
    "summary_dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0254766",
   "metadata": {},
   "source": [
    "### Bias Mitigation Results: Decision Tree – In-Processing  \n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Model                   | Accuracy | DP diff | EO diff | Notes                                                                 |\n",
    "|-------------------------|:--------:|:-------:|:-------:|------------------------------------------------------------------------|\n",
    "| **DT Baseline (tuned)** | 0.8261   | 0.3439  | 0.0938  | Moderate DP disparity; small EO gap. |\n",
    "| **DT + EG (EO)**        | 0.8261   | 0.3497  | 0.0729  | Accuracy =; **DP worsens slightly**; **EO improves modestly**. |\n",
    "| **DT + EG (DP)**        | 0.8098   | 0.2823  | 0.0413  | Accuracy ↓ (−0.0163); **DP improves**; **EO improves strongly**. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- The **baseline DT** shows a **moderate DP gap (~0.34)** and a **small EO gap (~0.09)**.  \n",
    "- **EG (EO constraint):** keeps accuracy the same, **reduces EO** (0.0938 → 0.0729), but **slightly increases DP**.  \n",
    "- **EG (DP constraint):** delivers the **largest fairness gains** — both **DP ↓** (0.344 → 0.282) and **EO ↓** (0.094 → 0.041), though accuracy drops by ~1.6 pp.  \n",
    "\n",
    "**Conclusion:**  \n",
    "- **EG (DP)** offers the **best fairness improvements overall** (both DP and EO reduced) with only a small accuracy cost.  \n",
    "- **EG (EO)** provides a minor EO benefit without hurting accuracy, but DP slightly worsens.  \n",
    "- The **baseline** is already relatively fair on EO; mitigation mainly helps if **DP reduction** is prioritized.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87407025",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: In-processing: GridSearch Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c97e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing: GridSearch (Equalized Odds) ===\n",
      "          TPR   FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                   \n",
      "0    0.666667  0.25  0.666667       0.315789  0.736842\n",
      "1    0.718750  0.24  0.718750       0.554795  0.732877\n",
      "Accuracy: 0.7337 | DP diff: 0.2390 | EO diff: 0.0521\n",
      "\n",
      "=== In-processing: GridSearch (Demographic Parity) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.21875  0.833333       0.315789  0.789474\n",
      "1    0.916667  0.40000  0.916667       0.739726  0.808219\n",
      "Accuracy: 0.8043 | DP diff: 0.4239 | EO diff: 0.1813\n",
      "\n",
      "=== Decision Tree: Baseline vs EG vs GS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.8261</td>\n",
       "      <td>0.3439</td>\n",
       "      <td>0.0938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.8261</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.0729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.2823</td>\n",
       "      <td>0.0413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT + GS (EO)</td>\n",
       "      <td>0.7337</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.0521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT + GS (DP)</td>\n",
       "      <td>0.8043</td>\n",
       "      <td>0.4239</td>\n",
       "      <td>0.1813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)    0.8261   0.3439   0.0938\n",
       "1         DT + EG (EO)    0.8261   0.3497   0.0729\n",
       "2         DT + EG (DP)    0.8098   0.2823   0.0413\n",
       "3         DT + GS (EO)    0.7337   0.2390   0.0521\n",
       "4         DT + GS (DP)    0.8043   0.4239   0.1813"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds\n",
    "gs_eo = GridSearch(\n",
    "    estimator=clone(alt_best_dt),              # unfitted clone of tuned DT\n",
    "    constraints=EqualizedOdds(),            # EO constraint\n",
    "    selection_rule=\"tradeoff_optimization\", \n",
    "    constraint_weight=0.5,                  \n",
    "    grid_size=15,                           \n",
    ")\n",
    "gs_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_eo = gs_eo.predict(X_test_ready)\n",
    "m_gs_eo = eval_fairness(y_test, y_pred_gs_eo, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo['acc']:.4f} | DP diff: {m_gs_eo['dp']:.4f} | EO diff: {m_gs_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity\n",
    "gs_dp = GridSearch(\n",
    "    estimator=clone(alt_best_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15,\n",
    ")\n",
    "gs_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_dp = gs_dp.predict(X_test_ready)\n",
    "m_gs_dp = eval_fairness(y_test, y_pred_gs_dp, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp['acc']:.4f} | DP diff: {m_gs_dp['dp']:.4f} | EO diff: {m_gs_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with your existing runs\n",
    "summary_dt = pd.concat([\n",
    "    summary_dt,  \n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"DT + GS (EO)\", \"accuracy\":m_gs_eo[\"acc\"], \"dp_diff\":m_gs_eo[\"dp\"], \"eo_diff\":m_gs_eo[\"eo\"]},\n",
    "        {\"model\":\"DT + GS (DP)\", \"accuracy\":m_gs_dp[\"acc\"], \"dp_diff\":m_gs_dp[\"dp\"], \"eo_diff\":m_gs_dp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "print(\"\\n=== Decision Tree: Baseline vs EG vs GS ===\")\n",
    "summary_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86587b",
   "metadata": {},
   "source": [
    "### Decision Tree — In-Processing: EG vs. GridSearch (EO & DP)\n",
    "\n",
    "#### Summary of results\n",
    "| Model                    | Accuracy | DP diff | EO diff | Notes |\n",
    "|--------------------------|:--------:|:-------:|:-------:|------|\n",
    "| **DT Baseline (tuned)**  | 0.8261   | 0.3439  | 0.0938  | Moderate DP gap; small EO gap. |\n",
    "| **DT + EG (EO)**         | 0.8261   | 0.3497  | 0.0729  | **Acc =** baseline; **EO improves modestly**; DP slightly worse. |\n",
    "| **DT + EG (DP)**         | 0.8098   | 0.2823  | 0.0413  | **Acc ↓ ~1.6 pp**; **both DP and EO improve** vs baseline. |\n",
    "| **DT + GS (EO)**         | 0.7337   | **0.2390** | 0.0521 | **Lowest DP** & low EO, but **accuracy drops sharply**. |\n",
    "| **DT + GS (DP)**         | 0.8043   | 0.4239  | 0.1813  | **Acc ↓**; **DP and EO worsen**. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **EG (DP)** gives the **cleanest fairness gains** here: it **reduces both DP and EO** (0.3439→0.2823; 0.0938→0.0413) with a **small accuracy trade-off**.\n",
    "- **EG (EO)** delivers a **modest EO improvement** at **no accuracy cost**, but nudges DP upward a bit.\n",
    "- **GS (EO)** attains the **lowest DP** and a low EO but at the expense of a **large accuracy drop (to 0.7337)**—not a practical choice.\n",
    "- **GS (DP)** is **counterproductive**, degrading both fairness metrics and accuracy.\n",
    "\n",
    "**Summary:**  \n",
    "If you can tolerate a slight accuracy decrease, **DT + EG(DP)** is the best fairness–utility trade-off. If accuracy must remain unchanged, **DT + EG(EO)** offers a smaller EO gain with near-baseline performance. Avoid **GS(DP)**, and treat **GS(EO)** as impractical due to its accuracy hit.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15838f4",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: Post-processing: Threshold Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "873f1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned DT) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.28125  0.833333       0.368421  0.736842\n",
      "1    0.927083  0.30000  0.927083       0.712329  0.849315\n",
      "Accuracy: 0.8261 | DP diff: 0.3439 | EO diff: 0.0938\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "          TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.333333  0.125  0.333333       0.157895  0.789474\n",
      "1    0.625000  0.160  0.625000       0.465753  0.698630\n",
      "Accuracy: 0.7174 | DP diff: 0.3079 | EO diff: 0.2917\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.15625  0.666667       0.236842  0.815789\n",
      "1    0.822917  0.18000  0.822917       0.602740  0.821918\n",
      "Accuracy: 0.8207 | DP diff: 0.3659 | EO diff: 0.1562\n",
      "\n",
      "=== Decision Tree: Baseline vs Post-processing ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.8261</td>\n",
       "      <td>0.3439</td>\n",
       "      <td>0.0938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + Post (EO)</td>\n",
       "      <td>0.7174</td>\n",
       "      <td>0.3079</td>\n",
       "      <td>0.2917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + Post (DP)</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.3659</td>\n",
       "      <td>0.1562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)    0.8261   0.3439   0.0938\n",
       "1       DT + Post (EO)    0.7174   0.3079   0.2917\n",
       "2       DT + Post (DP)    0.8207   0.3659   0.1562"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "#Baseline for mitigation: fixed tuned DT\n",
    "alt_best_dt.fit(X_train_ready, y_train)\n",
    "y_base = alt_best_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "print(\"=== Baseline (tuned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "#Post-processing: Equalized Odds\n",
    "post_eo = ThresholdOptimizer(\n",
    "    estimator=alt_best_dt,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_eo = post_eo.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eo = eval_fairness(y_test, y_eo, A_test)\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# Post-processing: Demographic Parity\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=alt_best_dt,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# create summary table \n",
    "summary = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (tuned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (EO)\",      \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (DP)\",      \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs Post-processing ===\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328aeb4",
   "metadata": {},
   "source": [
    "### Decision Tree — Post- vs In-Processing \n",
    "\n",
    "#### Combined Results\n",
    "\n",
    "| Model / Method          | Accuracy | DP diff | EO diff | Notes (vs. baseline 0.8261 / 0.3439 / 0.0938) |\n",
    "|-------------------------|:--------:|:-------:|:-------:|-----------------------------------------------|\n",
    "| **Baseline (Tuned DT)** | 0.8261   | 0.3439  | 0.0938  | Reference                                      |\n",
    "| **Post (EO)**           | 0.7174   | 0.3079  | 0.2917  | Accuracy ↓ sharply (−0.11); DP ↓ slightly; **EO worsens strongly** |\n",
    "| **Post (DP)**           | 0.8207   | 0.3659  | 0.1562  | Accuracy ≈ baseline; DP ↑ slightly; **EO worsens** |\n",
    "| **EG (EO)**             | 0.8261   | 0.3497  | 0.0729  | Accuracy = baseline; DP ↑ slightly; **EO improves modestly** |\n",
    "| **EG (DP)**             | 0.8098   | 0.2823  | 0.0413  | Accuracy ↓ (~−1.6 pp); **both DP and EO improve** vs baseline |\n",
    "| **GS (EO)**             | 0.7337   | **0.2390** | 0.0521 | Accuracy ↓ heavily; **lowest DP and EO**, but poor utility overall |\n",
    "| **GS (DP)**             | 0.8043   | 0.4239  | 0.1813  | Accuracy ↓ moderately; DP & EO both worsen vs baseline |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline DT** shows a **moderate DP gap (~0.34)** and a **small EO gap (~0.09)**.  \n",
    "- **Post (EO):** slight DP gain but EO disparity more than triples, accuracy collapses → **not usable**.  \n",
    "- **Post (DP):** keeps accuracy near baseline but worsens both DP and EO → **ineffective**.  \n",
    "- **EG (EO):** yields a **modest EO improvement** with baseline accuracy; DP worsens slightly.  \n",
    "- **EG (DP):** actually improves both DP and EO but at a small accuracy cost (~−1.6 pp).  \n",
    "- **GS (EO):** achieves the **lowest DP and EO** but accuracy falls too much (0.7337) → impractical.  \n",
    "- **GS (DP):** harms both fairness metrics and reduces accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "- If **EO parity** is the main target, **EG (DP)** is surprisingly the best compromise: **both DP and EO improve**, with only a small accuracy drop.  \n",
    "- **EG (EO)** is safer if accuracy must remain at baseline, offering modest EO gains.  \n",
    "- **Post-processing methods** do not yield useful fairness–utility trade-offs in this setup.  \n",
    "- **GS (EO)** looks good on fairness metrics but its accuracy collapse rules it out for clinical use.  \n",
    "- Overall, **baseline DT remains competitive**, with only incremental improvements possible through EG-based approaches.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbe2d4",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81ec0da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n",
      "Best RF params: {'class_weight': None, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
      "Best CV F1: 0.9566666666666667\n",
      "=== Tuned Random Forest Evaluation ===\n",
      "Accuracy : 0.842391304347826\n",
      "Precision: 0.9010989010989011\n",
      "Recall   : 0.803921568627451\n",
      "F1 Score : 0.8497409326424871\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83        82\n",
      "           1       0.90      0.80      0.85       102\n",
      "\n",
      "    accuracy                           0.84       184\n",
      "   macro avg       0.84      0.85      0.84       184\n",
      "weighted avg       0.85      0.84      0.84       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[73  9]\n",
      " [20 82]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest: hyperparameter tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 1) GridSearchCV over impactful RF params\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [None, 8, 12, 16],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.8],  # 0.8 = 80% of features\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"recall\",          \n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "grid.fit(X_train_ready, y_train)\n",
    "best_rf = grid.best_estimator_\n",
    "print(\"Best RF params:\", grid.best_params_)\n",
    "print(\"Best CV F1:\", grid.best_score_)\n",
    "\n",
    "# 2) Evaluate best RF \n",
    "y_pred = best_rf.predict(X_test_ready)\n",
    "y_prob = best_rf.predict_proba(X_test_ready)[:, 1]\n",
    "evaluate_model(y_test, y_pred, \"Tuned Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daad4ed",
   "metadata": {},
   "source": [
    "### Bias Mitgation RF: In-processing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1199f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.03125  0.833333       0.157895  0.947368\n",
      "1    0.791667  0.14000  0.791667       0.568493  0.815068\n",
      "Accuracy: 0.8424 | DP diff: 0.4106 | EO diff: 0.1088\n",
      "\n",
      "=== In-processing RF: EG (Equalized Odds) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.03125  0.833333       0.157895  0.947368\n",
      "1    0.791667  0.14000  0.791667       0.568493  0.815068\n",
      "Accuracy: 0.8424 | DP diff: 0.4106 | EO diff: 0.1088\n",
      "\n",
      "=== In-processing RF: EG (Demographic Parity) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.03125  0.833333       0.157895  0.947368\n",
      "1    0.791667  0.14000  0.791667       0.568493  0.815068\n",
      "Accuracy: 0.8424 | DP diff: 0.4106 | EO diff: 0.1088\n",
      "\n",
      "=== Random Forest: Baseline vs In-processing (EG) ===\n",
      "          model  accuracy  dp_diff  eo_diff\n",
      "0   RF Baseline    0.8424   0.4106   0.1088\n",
      "1  RF + EG (EO)    0.8424   0.4106   0.1088\n",
      "2  RF + EG (DP)    0.8424   0.4106   0.1088\n"
     ]
    }
   ],
   "source": [
    "# 0) Baseline Random Forest\n",
    "best_rf = RandomForestClassifier(random_state=42)\n",
    "best_rf.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_rf_base = best_rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_pred_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "#1) EG with Equalized Odds\n",
    "eg_eo_rf = ExponentiatedGradient(\n",
    "    estimator=clone(best_rf),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_eo = eg_eo_rf.predict(X_test_ready, random_state=42)\n",
    "m_rf_eo = eval_fairness(y_test, y_pred_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity \n",
    "eg_dp_rf = ExponentiatedGradient(\n",
    "    estimator=clone(best_rf),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_dp = eg_dp_rf.predict(X_test_ready, random_state=42)\n",
    "m_rf_dp = eval_fairness(y_test, y_pred_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table \n",
    "summary_rf = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",      \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (EO)\",     \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (DP)\",     \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b0260",
   "metadata": {},
   "source": [
    "## Random Forest Bias Mitigation Results  \n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model            | Accuracy | DP diff | EO diff | Interpretation                                 |\n",
    "|------------------|:--------:|:-------:|:-------:|-----------------------------------------------|\n",
    "| **RF Baseline**  | 0.8424   | 0.4106  | 0.1088  | Good accuracy; **large DP gap** persists.      |\n",
    "| **RF + EG (EO)** | 0.8424   | 0.4106  | 0.1088  | **No change** vs baseline → EO constraint had no effect. |\n",
    "| **RF + EG (DP)** | 0.8424   | 0.4106  | 0.1088  | **No change** vs baseline → DP constraint had no effect. |\n",
    "\n",
    "### Key Points\n",
    "- Selection rates: **0.158 (F)** vs **0.568 (M)** → ~**3.6×** higher for males (**DP 0.4106**).\n",
    "- Error rates: **TPR** 0.833 (F) vs 0.792 (M); **FPR** 0.031 (F) vs 0.140 (M) → **EO 0.1088**.\n",
    "- In-processing EG (EO/DP) produced **0% movement**—likely constraints not binding or the optimizer chose the baseline point on the fairness–accuracy frontier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c181c7",
   "metadata": {},
   "source": [
    "### Bias Mitigation: RF: In-processing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4e11367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method  weight       acc        dp    eo\n",
      "5  RF + GS (DP)    0.00  0.836957  0.470079  0.16\n",
      "6  RF + GS (DP)    0.25  0.836957  0.470079  0.16\n",
      "7  RF + GS (DP)    0.50  0.836957  0.470079  0.16\n",
      "8  RF + GS (DP)    0.75  0.836957  0.470079  0.16\n",
      "9  RF + GS (DP)    1.00  0.836957  0.470079  0.16\n",
      "0  RF + GS (EO)    0.00  0.831522  0.476929  0.18\n",
      "1  RF + GS (EO)    0.25  0.831522  0.476929  0.18\n",
      "2  RF + GS (EO)    0.50  0.831522  0.476929  0.18\n",
      "3  RF + GS (EO)    0.75  0.831522  0.476929  0.18\n",
      "4  RF + GS (EO)    1.00  0.831522  0.476929  0.18\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0]   # 0.0 = accuracy-first, 1.0 = fairness-first\n",
    "grid = 50                               \n",
    "\n",
    "rows = []\n",
    "\n",
    "#Equalized Odds sweep\n",
    "for w in weights:\n",
    "    gs_eo_rf = GridSearch(\n",
    "        estimator=clone(best_rf),                 \n",
    "        constraints=EqualizedOdds(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    # Some versions accept random_state in predict; if yours doesn't, seed numpy before predicting\n",
    "    try:\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (EO)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "# Demographic Parity sweep\n",
    "for w in weights:\n",
    "    gs_dp_rf = GridSearch(\n",
    "        estimator=clone(best_rf),\n",
    "        constraints=DemographicParity(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    try:\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (DP)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "df_gs = pd.DataFrame(rows).sort_values([\"method\",\"weight\"])\n",
    "print(df_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfa38e",
   "metadata": {},
   "source": [
    "**Interpretation (RF + GridSearch)**\n",
    "\n",
    "- **No movement across weights:** For both **DP** and **EO** constraints, changing the weight from **0 → 1** yields the **same model** and metrics.\n",
    "- **Worse than RF baseline:**  \n",
    "  - **Accuracy:** 0.837 (DP) / 0.832 (EO) vs baseline 0.842 (↓ ~0.5–1.0 pts)  \n",
    "  - **DP diff:** ~**0.47** vs baseline **0.41** (↑ → fairness gap widened)  \n",
    "  - **EO diff:** **0.16–0.18** vs baseline **0.109** (↑ → error-rate disparity worsened)\n",
    "\n",
    "**Takeaway:** GridSearch did **not** explore the fairness–accuracy frontier as expected. Instead, it converged to a single corner solution that is **less fair and slightly less accurate** than the baseline RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2117d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     i       acc        dp        eo\n",
      "0    0  0.456522  0.052632  0.333333\n",
      "1    1  0.456522  0.052632  0.333333\n",
      "2    2  0.456522  0.052632  0.333333\n",
      "3    3  0.456522  0.052632  0.333333\n",
      "4    4  0.456522  0.052632  0.333333\n",
      "5    5  0.456522  0.052632  0.333333\n",
      "6    6  0.456522  0.052632  0.333333\n",
      "7    7  0.456522  0.052632  0.333333\n",
      "8    8  0.347826  0.297765  0.760000\n",
      "9    9  0.456522  0.052632  0.333333\n",
      "10  10  0.456522  0.052632  0.333333\n",
      "11  11  0.461957  0.078947  0.500000\n",
      "12  12  0.456522  0.052632  0.333333\n",
      "13  13  0.456522  0.052632  0.333333\n",
      "14  14  0.375000  0.285148  0.740000\n",
      "15  15  0.347826  0.325162  0.800000\n",
      "16  16  0.679348  0.363014  0.760000\n",
      "17  17  0.831522  0.476929  0.180000\n",
      "18  18  0.847826  0.497477  0.180000\n",
      "19  19  0.820652  0.410598  0.148750\n",
      "20  20  0.836957  0.397981  0.097500\n",
      "21  21  0.842391  0.424297  0.128750\n",
      "22  22  0.695652  0.736842  0.843750\n",
      "23  23  0.717391  0.842105  0.968750\n",
      "24  24  0.717391  0.842105  0.968750\n",
      "25  25  0.673913  0.383562  0.780000\n",
      "26  26  0.679348  0.363014  0.760000\n",
      "27  27  0.831522  0.476929  0.180000\n",
      "28  28  0.831522  0.430065  0.160000\n",
      "29  29  0.842391  0.391132  0.077500\n",
      "30  30  0.842391  0.410598  0.108750\n",
      "31  31  0.836957  0.384283  0.077500\n",
      "32  32  0.711957  0.815789  0.937500\n",
      "33  33  0.706522  0.842105  0.937500\n",
      "34  34  0.711957  0.868421  0.968750\n",
      "35  35  0.711957  0.868421  0.968750\n",
      "36  36  0.679348  0.363014  0.760000\n",
      "37  37  0.842391  0.430065  0.140000\n",
      "38  38  0.831522  0.463230  0.160000\n",
      "39  39  0.842391  0.410598  0.108750\n",
      "40  40  0.847826  0.403749  0.088750\n",
      "41  41  0.836957  0.436914  0.125000\n",
      "42  42  0.706522  0.842105  0.937500\n",
      "43  43  0.701087  0.815789  0.906250\n",
      "44  44  0.701087  0.815789  0.906250\n",
      "45  45  0.836957  0.403749  0.108750\n",
      "46  46  0.820652  0.377433  0.097500\n",
      "47  47  0.826087  0.370584  0.077500\n",
      "48  48  0.815217  0.423216  0.128750\n",
      "49  49  0.847826  0.442682  0.114583\n",
      "     i       acc        dp        eo\n",
      "0    0  0.456522  0.052632  0.333333\n",
      "1    1  0.456522  0.052632  0.333333\n",
      "2    2  0.456522  0.052632  0.333333\n",
      "3    3  0.456522  0.052632  0.333333\n",
      "4    4  0.456522  0.052632  0.333333\n",
      "5    5  0.456522  0.052632  0.333333\n",
      "6    6  0.456522  0.052632  0.333333\n",
      "7    7  0.456522  0.052632  0.333333\n",
      "8    8  0.456522  0.052632  0.333333\n",
      "9    9  0.456522  0.052632  0.333333\n",
      "10  10  0.456522  0.052632  0.333333\n",
      "11  11  0.456522  0.052632  0.333333\n",
      "12  12  0.456522  0.052632  0.333333\n",
      "13  13  0.836957  0.470079  0.160000\n",
      "14  14  0.847826  0.464311  0.156250\n",
      "15  15  0.820652  0.410598  0.148750\n",
      "16  16  0.820652  0.396900  0.128750\n",
      "17  17  0.842391  0.443764  0.160000\n",
      "18  18  0.836957  0.470079  0.302083\n",
      "19  19  0.842391  0.463230  0.140000\n",
      "20  20  0.836957  0.403749  0.108750\n",
      "21  21  0.836957  0.384283  0.077500\n",
      "22  22  0.842391  0.410598  0.108750\n",
      "23  23  0.831522  0.410598  0.128750\n",
      "24  24  0.826087  0.356885  0.072917\n",
      "25  25  0.842391  0.410598  0.108750\n",
      "26  26  0.836957  0.417448  0.128750\n",
      "27  27  0.831522  0.410598  0.125000\n",
      "28  28  0.826087  0.384283  0.097500\n",
      "29  29  0.847826  0.431146  0.128750\n",
      "30  30  0.836957  0.403749  0.108750\n",
      "31  31  0.831522  0.391132  0.097500\n",
      "32  32  0.836957  0.384283  0.077500\n",
      "33  33  0.842391  0.416366  0.114583\n",
      "34  34  0.820652  0.482696  0.270833\n",
      "35  35  0.836957  0.436914  0.125000\n",
      "36  36  0.826087  0.436914  0.128750\n",
      "37  37  0.842391  0.449531  0.291667\n",
      "38  38  0.711957  0.868421  0.968750\n",
      "39  39  0.711957  0.868421  0.968750\n",
      "40  40  0.706522  0.842105  0.937500\n",
      "41  41  0.706522  0.842105  0.937500\n",
      "42  42  0.706522  0.842105  0.937500\n",
      "43  43  0.711957  0.815789  0.937500\n",
      "44  44  0.701087  0.815789  0.906250\n",
      "45  45  0.706522  0.789474  0.906250\n",
      "46  46  0.711957  0.815789  0.937500\n",
      "47  47  0.706522  0.842105  0.937500\n",
      "48  48  0.701087  0.815789  0.906250\n",
      "49  49  0.701087  0.815789  0.906250\n"
     ]
    }
   ],
   "source": [
    "# Inspect how many distinct models GridSearch actually produced\n",
    "len(gs_eo_rf.predictors_), len(gs_dp_rf.predictors_)\n",
    "\n",
    "# See the spread across the frontier (test metrics for each predictor)\n",
    "def eval_frontier(gs, X, y, A):\n",
    "    rows=[]\n",
    "    for i, clf in enumerate(gs.predictors_):\n",
    "        yhat = clf.predict(X)\n",
    "        m = eval_fairness(y, yhat, A)\n",
    "        rows.append({\"i\": i, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(eval_frontier(gs_eo_rf, X_test_ready, y_test, A_test))\n",
    "print(eval_frontier(gs_dp_rf, X_test_ready, y_test, A_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee0bbb",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**What’s in the tables:** Each index `i` corresponds to one GridSearch candidate along the fairness–accuracy frontier.  \n",
    "- Many low-index models (e.g., `i=0–12`) collapse to weak classifiers (Acc ≈0.4565, EO ≈0.333) → not useful.  \n",
    "- Mid-range indices (`i=38–49`) contain the strongest trade-offs.  \n",
    "\n",
    "#### Strong candidates\n",
    "- **Best overall (↑Acc, ↓DP, ↓EO):**  \n",
    "  - `i=40` → **Acc 0.8478**, **DP 0.4037**, **EO 0.0888**.  \n",
    "  - Improves both fairness metrics compared to baseline while slightly increasing accuracy.\n",
    "\n",
    "- **Lowest EO gap at near-baseline accuracy:**  \n",
    "  - `i=29` → **Acc 0.8424**, **DP 0.3911**, **EO 0.0775**.  \n",
    "  - Strong option if minimizing error-rate disparity is the main goal.\n",
    "\n",
    "- **Lower DP with modest accuracy trade-off:**  \n",
    "  - `i=24` (2nd table) → **Acc 0.8261**, **DP 0.3569**, **EO 0.0729**.  \n",
    "  - `i=47` (1st table) → **Acc 0.8261**, **DP 0.3706**, **EO 0.0775**.  \n",
    "  - Both reduce selection-rate gap more aggressively, but cost ~1.5–2 points of accuracy.\n",
    "\n",
    "- **Balanced trade-off (both fairness metrics improved, small Acc dip):**  \n",
    "  - `i=31` (1st table) → **Acc 0.8370**, **DP 0.3843**, **EO 0.0775**.  \n",
    "  - A middle ground if you want improvements in DP & EO without losing much accuracy.\n",
    "\n",
    "**Takeaway:**  \n",
    "- If **maximizing overall performance with fairness gains** → choose **`i=40`**.  \n",
    "- If **error-rate parity (EO)** is top priority → choose **`i=29`**.  \n",
    "- If **reducing demographic disparity (DP)** is more important, and you can accept a modest accuracy loss → choose **`i=24`** or **`i=47`**.  \n",
    "- **`i=31`** offers a well-balanced compromise.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5477483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RF + GS (EO): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (EO)] i=40\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.03125  0.833333       0.157895  0.947368\n",
      "1    0.791667  0.12000  0.791667       0.561644  0.821918\n",
      "Accuracy: 0.8478 | DP diff: 0.4037 | EO diff: 0.0887\n",
      "\n",
      "[RF + GS (EO)] i=29\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.833333  0.0625  0.833333       0.184211  0.921053\n",
      "1    0.802083  0.1400  0.802083       0.575342  0.821918\n",
      "Accuracy: 0.8424 | DP diff: 0.3911 | EO diff: 0.0775\n",
      "\n",
      "[RF + GS (EO)] i=31\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.833333  0.0625  0.833333       0.184211  0.921053\n",
      "1    0.791667  0.1400  0.791667       0.568493  0.815068\n",
      "Accuracy: 0.8370 | DP diff: 0.3843 | EO diff: 0.0775\n",
      "\n",
      "--- Summary (RF + GS (EO)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "1  29    0.8424   0.3911   0.0775\n",
      "2  31    0.8370   0.3843   0.0775\n",
      "0  40    0.8478   0.4037   0.0888\n",
      "\n",
      "=== RF + GS (DP): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (DP)] i=40\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.666667  0.0625  0.666667       0.157895  0.894737\n",
      "1    1.000000  1.0000  1.000000       1.000000  0.657534\n",
      "Accuracy: 0.7065 | DP diff: 0.8421 | EO diff: 0.9375\n",
      "\n",
      "[RF + GS (DP)] i=29\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.03125  0.833333       0.157895  0.947368\n",
      "1    0.812500  0.16000  0.812500       0.589041  0.821918\n",
      "Accuracy: 0.8478 | DP diff: 0.4311 | EO diff: 0.1288\n",
      "\n",
      "[RF + GS (DP)] i=31\n",
      "          TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                     \n",
      "0    0.833333  0.0625  0.833333       0.184211  0.921053\n",
      "1    0.791667  0.1600  0.791667       0.575342  0.808219\n",
      "Accuracy: 0.8315 | DP diff: 0.3911 | EO diff: 0.0975\n",
      "\n",
      "--- Summary (RF + GS (DP)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "1  29    0.8478   0.4311   0.1288\n",
      "2  31    0.8315   0.3911   0.0975\n",
      "0  40    0.7065   0.8421   0.9375\n"
     ]
    }
   ],
   "source": [
    "# Show results for the specific frontier models \n",
    "# for both RF GridSearch runs (EO- and DP-constrained).\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "indices = [40,29,31]\n",
    "\n",
    "def eval_selected(gs, label):\n",
    "    rows = []\n",
    "    n = len(gs.predictors_)\n",
    "    print(f\"\\n=== {label}: {n} frontier candidates ===\")\n",
    "    for i in indices:\n",
    "        if i >= n:\n",
    "            print(f\"[{label}] Skipping i={i} (only {n} candidates).\")\n",
    "            continue\n",
    "        clf = gs.predictors_[i]\n",
    "        y_hat = clf.predict(X_test_ready)\n",
    "        m = eval_fairness(y_test, y_hat, A_test)\n",
    "        rows.append({\"i\": i, \"accuracy\": m[\"acc\"], \"dp_diff\": m[\"dp\"], \"eo_diff\": m[\"eo\"]})\n",
    "\n",
    "        # Per-group breakdown for this model\n",
    "        print(f\"\\n[{label}] i={i}\")\n",
    "        print(m[\"by_group\"])\n",
    "        print(f\"Accuracy: {m['acc']:.4f} | DP diff: {m['dp']:.4f} | EO diff: {m['eo']:.4f}\")\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows).sort_values(\"i\").round(4)\n",
    "        print(f\"\\n--- Summary ({label}) ---\")\n",
    "        print(df)\n",
    "\n",
    "# Evaluate selected indices for both EO and DP GridSearch objects\n",
    "eval_selected(gs_eo_rf, \"RF + GS (EO)\")\n",
    "eval_selected(gs_dp_rf, \"RF + GS (DP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675feefc",
   "metadata": {},
   "source": [
    "### Interpretation (RF + GridSearch)\n",
    "\n",
    "#### Equalized Odds (EO)\n",
    "- **Best overall (accuracy + fairness):**  \n",
    "  - `i=40` → **Acc 0.8478**, **DP 0.4037**, **EO 0.0888**.  \n",
    "    - Slight accuracy gain over baseline, EO gap reduced, DP gap still moderate.  \n",
    "- **Lowest EO gap (with small accuracy trade-off):**  \n",
    "  - `i=29` → **Acc 0.8424**, **DP 0.3911**, **EO 0.0775**.  \n",
    "    - Very good fairness improvement (EO close to zero), accuracy only slightly below baseline.  \n",
    "- **Balanced candidate:**  \n",
    "  - `i=31` → **Acc 0.8370**, **DP 0.3843**, **EO 0.0775**.  \n",
    "    - Both fairness metrics improved, but accuracy ~0.5% lower.\n",
    "\n",
    "**Takeaway (EO):**  \n",
    "- If **accuracy + stability** matter most → pick `i=40`.  \n",
    "- If **minimizing EO disparity** is the top goal → pick `i=29`.  \n",
    "- If a **balanced trade-off** is desired → `i=31` is solid.\n",
    "\n",
    "---\n",
    "\n",
    "#### Demographic Parity (DP)\n",
    "- **Unstable/poor solution:**  \n",
    "  - `i=40` → **Acc 0.7065**, **DP 0.8421**, **EO 0.9375**.  \n",
    "    - Very low accuracy and extreme fairness gaps — discard.  \n",
    "- **High accuracy but fairness not great:**  \n",
    "  - `i=29` → **Acc 0.8478**, **DP 0.4311**, **EO 0.1288**.  \n",
    "    - Accuracy similar to EO frontier, but fairness weaker (DP gap larger, EO gap higher).  \n",
    "- **Best DP candidate:**  \n",
    "  - `i=31` → **Acc 0.8315**, **DP 0.3911**, **EO 0.0975**.  \n",
    "    - Both fairness metrics improved with only a small drop in accuracy.\n",
    "\n",
    "**Takeaway (DP):**  \n",
    "- Ignore unstable `i=40`.  \n",
    "- `i=29` achieves high accuracy but fairness is weaker.  \n",
    "- **Best balance is `i=31`**, which delivers meaningful fairness gains with a modest accuracy trade-off.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Summary\n",
    "- **EO-constrained GridSearch** provided more reliable frontier candidates than **DP-constrained**.  \n",
    "- **Recommended picks:**  \n",
    "  - `i=40` (EO) → highest accuracy, fairness improved.  \n",
    "  - `i=29` (EO) → lowest EO disparity, accuracy near baseline.  \n",
    "  - `i=31` (DP) → best DP-driven balance, but accuracy slightly lower.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001876d",
   "metadata": {},
   "source": [
    "### Bias Mitigation RF: Post-processing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8238f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.833333  0.03125  0.833333       0.157895  0.947368\n",
      "1    0.791667  0.14000  0.791667       0.568493  0.815068\n",
      "Accuracy: 0.8424 | DP diff: 0.4106 | EO diff: 0.1088\n",
      "\n",
      "=== RF + Post-processing (Equalized Odds) ===\n",
      "          TPR   FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                   \n",
      "0    0.666667  0.00  0.666667       0.105263  0.947368\n",
      "1    0.791667  0.14  0.791667       0.568493  0.815068\n",
      "Accuracy: 0.8424 | DP diff: 0.4632 | EO diff: 0.1400\n",
      "\n",
      "=== RF + Post-processing (Demographic Parity) ===\n",
      "          TPR   FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                   \n",
      "0    0.666667  0.00  0.666667       0.105263  0.947368\n",
      "1    0.791667  0.14  0.791667       0.568493  0.815068\n",
      "Accuracy: 0.8424 | DP diff: 0.4632 | EO diff: 0.1400\n",
      "\n",
      "=== Random Forest: Baseline vs Post-processing ===\n",
      "            model  accuracy  dp_diff  eo_diff\n",
      "0     RF Baseline    0.8424   0.4106   0.1088\n",
      "1  RF + Post (EO)    0.8424   0.4632   0.1400\n",
      "2  RF + Post (DP)    0.8424   0.4632   0.1400\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# 0) Baseline RF \n",
    "best_rf.fit(X_train_ready, y_train)\n",
    "y_rf_base = best_rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds \n",
    "post_rf_eo = ThresholdOptimizer(\n",
    "    estimator=best_rf,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_eo = post_rf_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_rf_eo = eval_fairness(y_test, y_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity \n",
    "post_rf_dp = ThresholdOptimizer(\n",
    "    estimator=best_rf,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_dp = post_rf_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_rf_dp = eval_fairness(y_test, y_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "#3) Summary Table\n",
    "summary_rf_post = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",       \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (EO)\",    \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (DP)\",    \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs Post-processing ===\")\n",
    "print(summary_rf_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde82b4a",
   "metadata": {},
   "source": [
    "### Random Forest Bias Mitigation: Post-processing: Threshold Optimizer\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Interpretation                                    |\n",
    "|---------------------|:--------:|:-------:|:-------:|---------------------------------------------------|\n",
    "| **RF Baseline**     | 0.8424   | 0.4106  | 0.1088  | Solid accuracy; sizeable DP gap; moderate EO gap. |\n",
    "| **RF + Post (EO)**  | 0.8424   | 0.4632  | 0.1400  | Accuracy unchanged; **DP worsens**; **EO worsens**. |\n",
    "| **RF + Post (DP)**  | 0.8424   | 0.4632  | 0.1400  | Identical to EO result → no fairness gain.        |\n",
    "\n",
    "### Summary:\n",
    "- **Accuracy** remained flat (0.8424) across baseline and post-processing.  \n",
    "- **Demographic Parity difference** increased from **0.4106 → 0.4632** → gap widened.  \n",
    "- **Equalized Odds difference** also worsened from **0.1088 → 0.1400**.  \n",
    "- Both EO and DP post-processing collapsed to the **same adjusted solution**, offering **no fairness improvement**.\n",
    "\n",
    "**Takeaway:** For RF, this post-processing configuration **failed to improve fairness** and instead made disparities worse while leaving accuracy unchanged.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2c505",
   "metadata": {},
   "source": [
    "### Deep Learning - Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b0d5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required library \n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4cbac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== (Adam + EarlyStopping) Evaluation ===\n",
      "Accuracy : 0.8206521739130435\n",
      "Precision: 0.8709677419354839\n",
      "Recall   : 0.7941176470588235\n",
      "F1 Score : 0.8307692307692308\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81        82\n",
      "           1       0.87      0.79      0.83       102\n",
      "\n",
      "    accuracy                           0.82       184\n",
      "   macro avg       0.82      0.82      0.82       184\n",
      "weighted avg       0.83      0.82      0.82       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[70 12]\n",
      " [21 81]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adam + Early Stopping \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "adammlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),   # slightly smaller/deeper can help\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=1e-3,       # smaller step can stabilize\n",
    "    alpha=1e-3,                    # L2 regularization to reduce overfitting\n",
    "    batch_size=32,\n",
    "    max_iter=1000,                 # increased max_iter\n",
    "    early_stopping=True,           # use a validation split internally\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=25,          \n",
    "    tol=1e-4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "adammlp.fit(X_train_ready, y_train)  \n",
    "y_pred_mlp = adammlp.predict(X_test_ready)                     \n",
    "y_prob_mlp = adammlp.predict_proba(X_test_ready)[:, 1]         \n",
    "\n",
    "evaluate_model(y_test, y_pred_mlp, \"(Adam + EarlyStopping)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775454c",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "857cf027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.03125  0.666667       0.131579  0.921053\n",
      "1    0.802083  0.22000  0.802083       0.602740  0.794521\n",
      "Accuracy: 0.8207 | DP diff: 0.4712 | EO diff: 0.1888\n",
      "\n",
      "=== In-processing MLP: EG (Equalized Odds) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.03125  0.666667       0.131579  0.921053\n",
      "1    0.802083  0.22000  0.802083       0.602740  0.794521\n",
      "Accuracy: 0.8207 | DP diff: 0.4712 | EO diff: 0.1888\n",
      "\n",
      "=== In-processing MLP: EG (Demographic Parity) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.03125  0.666667       0.131579  0.921053\n",
      "1    0.812500  0.20000  0.812500       0.602740  0.808219\n",
      "Accuracy: 0.8315 | DP diff: 0.4712 | EO diff: 0.1688\n",
      "\n",
      "=== MLP: Baseline vs In-processing (EG) ===\n",
      "           model  accuracy  dp_diff  eo_diff\n",
      "0   MLP Baseline    0.8207   0.4712   0.1888\n",
      "1  MLP + EG (EO)    0.8207   0.4712   0.1888\n",
      "2  MLP + EG (DP)    0.8315   0.4712   0.1688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Baseline MLP (seeded for reproducibility)\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=1e-3,\n",
    "    alpha=1e-3,\n",
    "    batch_size=32,\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=25,\n",
    "    tol=1e-4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_mlp_base = mlp.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_pred_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) EG with Equalized Odds\n",
    "eg_eo_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(mlp),   # inherits random_state=42\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "# Prefer predict(..., random_state=42) if supported; otherwise fall back without global seeds\n",
    "try:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_eo = eval_fairness(y_test, y_pred_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP: EG (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity\n",
    "eg_dp_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(mlp),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_dp = eval_fairness(y_test, y_pred_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP: EG (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",  \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (EO)\", \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (DP)\", \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943e26",
   "metadata": {},
   "source": [
    "### MLP In-Processing Bias Mitigation Results  \n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model             | Accuracy | DP diff | EO diff | Interpretation                                                     |\n",
    "|-------------------|:--------:|:-------:|:-------:|--------------------------------------------------------------------|\n",
    "| **MLP Baseline**  | 0.8207   | 0.4712  | 0.1888  | Good accuracy; **large DP gap** and **moderate EO**.               |\n",
    "| **MLP + EG (EO)** | 0.8207   | 0.4712  | 0.1888  | **Identical to baseline** → EO constraint had **no effect**.       |\n",
    "| **MLP + EG (DP)** | 0.8315   | 0.4712  | 0.1688  | **Accuracy +1.1 pts**; **EO improves slightly**; **DP unchanged**. |\n",
    "\n",
    "### Summary:\n",
    "- **Selection disparity persists:** Female sel. **0.132** vs Male **0.603** → **DP = 0.471** (~**4.6×** higher for males) across all runs.\n",
    "- **EG (EO)** did not move metrics — constraints likely **not binding**.\n",
    "- **EG (DP)** nudged **EO down** (0.1888 → **0.1688**) and **accuracy up** (0.8207 → **0.8315**), but **did not reduce DP**.\n",
    "\n",
    "**Takeaway:** With current settings, MLP **resists DP reduction** via in-processing. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11de87",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1b9ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing MLP: GridSearch (Equalized Odds) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.03125  0.666667       0.131579  0.921053\n",
      "1    0.802083  0.22000  0.802083       0.602740  0.794521\n",
      "Accuracy: 0.8207 | DP diff: 0.4712 | EO diff: 0.1888\n",
      "\n",
      "=== In-processing MLP: GridSearch (Demographic Parity) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.03125  0.666667       0.131579  0.921053\n",
      "1    0.802083  0.22000  0.802083       0.602740  0.794521\n",
      "Accuracy: 0.8207 | DP diff: 0.4712 | EO diff: 0.1888\n",
      "\n",
      "=== MLP: Baseline vs EG vs GS ===\n",
      "           model  accuracy  dp_diff  eo_diff\n",
      "0   MLP Baseline    0.8207   0.4712   0.1888\n",
      "1  MLP + EG (EO)    0.8207   0.4712   0.1888\n",
      "2  MLP + EG (DP)    0.8315   0.4712   0.1688\n",
      "3  MLP + GS (EO)    0.8207   0.4712   0.1888\n",
      "4  MLP + GS (DP)    0.8207   0.4712   0.1888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds (MLP)\n",
    "gs_eo_mlp = GridSearch(\n",
    "    estimator=clone(adammlp),                 # unfitted clone of your MLP (inherits random_state=42)\n",
    "    constraints=EqualizedOdds(),\n",
    "    selection_rule=\"tradeoff_optimization\",  \n",
    "    constraint_weight=0.5,                   # trade-off weight (0..1); tune as needed\n",
    "    grid_size=15                             \n",
    ")\n",
    "gs_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_eo_mlp = eval_fairness(y_test, y_pred_gs_eo_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo_mlp['acc']:.4f} | DP diff: {m_gs_eo_mlp['dp']:.4f} | EO diff: {m_gs_eo_mlp['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity (MLP)\n",
    "gs_dp_mlp = GridSearch(\n",
    "    estimator=clone(adammlp),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15\n",
    ")\n",
    "gs_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_dp_mlp = eval_fairness(y_test, y_pred_gs_dp_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp_mlp['acc']:.4f} | DP diff: {m_gs_dp_mlp['dp']:.4f} | EO diff: {m_gs_dp_mlp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with existing MLP runs (baseline + EG)\n",
    "summary_mlp = pd.concat([\n",
    "    summary_mlp,\n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"MLP + GS (EO)\", \"accuracy\":m_gs_eo_mlp[\"acc\"], \"dp_diff\":m_gs_eo_mlp[\"dp\"], \"eo_diff\":m_gs_eo_mlp[\"eo\"]},\n",
    "        {\"model\":\"MLP + GS (DP)\", \"accuracy\":m_gs_dp_mlp[\"acc\"], \"dp_diff\":m_gs_dp_mlp[\"dp\"], \"eo_diff\":m_gs_dp_mlp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs EG vs GS ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4cf0b",
   "metadata": {},
   "source": [
    "### MLP — In-Processing vs GridSearch \n",
    "\n",
    "#### Comparative table (vs. Baseline)\n",
    "| Model          | Accuracy | ΔAcc (pp) | DP diff | ΔDP | EO diff | ΔEO  | Notes                           |\n",
    "|----------------|:--------:|:---------:|:-------:|:---:|:-------:|:----:|----------------------------------|\n",
    "| Baseline (MLP) | 0.8207   |   –       | 0.4712  |  –  | 0.1888  |  –   | Reference                        |\n",
    "| EG (EO)        | 0.8207   |  +0.00    | 0.4712  | 0.00| 0.1888  | 0.00 | **No change** vs baseline        |\n",
    "| EG (DP)        | 0.8315   | **+1.08** | 0.4712  | 0.00| 0.1688  | **−0.02** | Better EO & accuracy; DP unchanged |\n",
    "| GS (EO)        | 0.8207   |  +0.00    | 0.4712  | 0.00| 0.1888  | 0.00 | **No change** vs baseline        |\n",
    "| GS (DP)        | 0.8207   |  +0.00    | 0.4712  | 0.00| 0.1888  | 0.00 | **No change** vs baseline        |\n",
    "\n",
    "#### Interpretation\n",
    "- **Only EG (DP)** moved the needle: **EO improved** (0.1888 → **0.1688**) and **accuracy increased** (0.8207 → **0.8315**); **DP stayed high** at 0.4712.\n",
    "- **EG (EO) and both GS variants** produced **identical predictions** to baseline → constraints likely **not binding** or reweighting not affecting the MLP.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d29d4a",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Postprocessing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4591c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "          TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "Sex                                                      \n",
      "0    0.666667  0.03125  0.666667       0.131579  0.921053\n",
      "1    0.802083  0.22000  0.802083       0.602740  0.794521\n",
      "Accuracy: 0.8207 | DP diff: 0.4712 | EO diff: 0.1888\n",
      "\n",
      "=== MLP + Post-processing (Equalized Odds) ===\n",
      "         TPR      FPR   Recall  SelectionRate  Accuracy\n",
      "Sex                                                    \n",
      "0    0.50000  0.09375  0.50000       0.157895  0.842105\n",
      "1    0.84375  0.22000  0.84375       0.630137  0.821918\n",
      "Accuracy: 0.8261 | DP diff: 0.4722 | EO diff: 0.3438\n",
      "\n",
      "=== MLP + Post-processing (Demographic Parity) ===\n",
      "         TPR   FPR   Recall  SelectionRate  Accuracy\n",
      "Sex                                                 \n",
      "0    0.50000  0.00  0.50000       0.078947  0.921053\n",
      "1    0.84375  0.22  0.84375       0.630137  0.821918\n",
      "Accuracy: 0.8424 | DP diff: 0.5512 | EO diff: 0.3438\n",
      "\n",
      "=== MLP: Baseline vs Post-processing ===\n",
      "             model  accuracy  dp_diff  eo_diff\n",
      "0     MLP Baseline    0.8207   0.4712   0.1888\n",
      "1  MLP + Post (EO)    0.8261   0.4722   0.3438\n",
      "2  MLP + Post (DP)    0.8424   0.5512   0.3438\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Baseline MLP\n",
    "adammlp.fit(X_train_ready, y_train)\n",
    "y_mlp_base = adammlp.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds\n",
    "post_mlp_eo = ThresholdOptimizer(\n",
    "    estimator=adammlp,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_eo = post_mlp_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_eo = eval_fairness(y_test, y_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity\n",
    "post_mlp_dp = ThresholdOptimizer(\n",
    "    estimator=adammlp,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_dp = post_mlp_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_dp = eval_fairness(y_test, y_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp_post = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",       \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (EO)\",    \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (DP)\",    \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs Post-processing ===\")\n",
    "print(summary_mlp_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d9e15",
   "metadata": {},
   "source": [
    "### MLP — Post-Processing: Threshold Optimizer  \n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Notes                                                                 |\n",
    "|---------------------|:--------:|:-------:|:-------:|------------------------------------------------------------------------|\n",
    "| **Baseline**        | 0.8207   | 0.4712  | 0.1888  | Notable outcome disparity (DP) and moderate error-rate gap (EO).       |\n",
    "| **Post (EO)**       | 0.8261   | 0.4722  | 0.3438  | Acc **+0.54 pp**; **EO worsens** (+0.155); DP ~unchanged (+0.001).     |\n",
    "| **Post (DP)**       | 0.8424   | 0.5512  | 0.3438  | Acc **+2.17 pp**; **DP worsens** (+0.080); **EO worsens** (+0.155).    |\n",
    "\n",
    "### Interpretation\n",
    "- **Equalized Odds post-processing** slightly increases accuracy but **substantially increases EO** and leaves **DP unchanged/slightly worse**.  \n",
    "- **Demographic Parity post-processing** raises accuracy more, yet **worsens both DP and EO** (female selection drops to **0.079** while male stays high **0.630** → **DP ↑ to 0.551**).  \n",
    "- Both post-processing variants converge to **EO = 0.3438**, driven by a large **TPR gap** (Female **0.50** vs Male **0.844**).\n",
    "\n",
    "**Takeaway:** With current scores/settings, post-processing **degrades fairness** (EO—and for DP constraint also DP) despite small accuracy gains. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d93504",
   "metadata": {},
   "source": [
    "### Overall Comparison:\n",
    "\n",
    "### Overall Bias-Mitigation Comparison (AIF360) — Gender Bias in CVD Prediction\n",
    "\n",
    "**Metric keys:**  \n",
    "- **DP diff** (Demographic Parity difference): outcome-rate gap across genders (lower = fairer).  \n",
    "- **EO diff** (Equalized Odds difference): error-rate gap across genders (lower = fairer).  \n",
    "- **Accuracy**: utility measure (higher = better).  \n",
    "\n",
    "---\n",
    "\n",
    "### Aggregated Summary Across Models\n",
    "\n",
    "| Model / Technique         | Accuracy | DP diff | EO diff | Verdict |\n",
    "|---------------------------|:--------:|:-------:|:-------:|---------|\n",
    "| **PCA+KNN Baseline**      | 0.8315   | 0.4301  | 0.1146  | Reference; DP large, EO moderate |\n",
    "| KNN + Post (DP/EO)        | 0.8315   | 0.4301  | 0.1146  | ❌ No effect — 0% label flips |\n",
    "| KNN + CorrelationRemover  | 0.8043   | 0.4117  | 0.3021  | ❌ Acc ↓, EO worsens |\n",
    "| **Decision Tree Baseline**| 0.8261   | 0.3439  | 0.0938  | Reference; moderate DP, small EO |\n",
    "| DT + EG (EO)              | 0.8261   | 0.3497  | 0.0729  | ✅ Best DT if acc must hold — EO improves modestly |\n",
    "| DT + EG (DP)              | 0.8098   | 0.2823  | 0.0413  | ✅ Strongest DT: both DP & EO ↓; small acc cost |\n",
    "| DT + Post (EO/DP)         | 0.7174 / 0.8207 | 0.3079 / 0.3659 | 0.2917 / 0.1562 | ❌ Either acc ↓ or fairness worsens |\n",
    "| DT + GS (EO/DP)           | 0.7337 / 0.8043 | 0.2390 / 0.4239 | 0.0521 / 0.1813 | ❌ EO run has lowest DP/EO but acc collapse; DP run worsens fairness |\n",
    "| **Random Forest Baseline**| 0.8424   | 0.4106  | 0.1088  | Strong acc; large DP; moderate EO |\n",
    "| RF + EG (EO/DP)           | 0.8424   | 0.4106  | 0.1088  | ❌ No effect |\n",
    "| RF + GS (EO i=29)         | 0.8424   | 0.3911  | 0.0775  | ✅ Best EO fairness at baseline acc |\n",
    "| RF + GS (EO i=40)         | 0.8478   | 0.4037  | 0.0888  | ✅ Best accuracy with EO ↓; DP still moderate |\n",
    "| RF + GS (DP i=31)         | 0.8315   | 0.3911  | 0.0975  | ✅ Balanced DP + EO ↓; small acc cost |\n",
    "| RF + Post (EO/DP)         | 0.8424   | 0.4632  | 0.1400  | ❌ Fairness worsens, acc unchanged |\n",
    "| **MLP Baseline**          | 0.8207   | 0.4712  | 0.1888  | Reference; weakest baseline fairness |\n",
    "| MLP + EG (EO/DP)          | 0.8207 / 0.8315 | 0.4712 | 0.1888 / 0.1688 | ❌ Minimal/no gains |\n",
    "| MLP + GS (EO/DP)          | 0.8207   | 0.4712  | 0.1888  | ❌ No effect |\n",
    "| MLP + Post (EO/DP)        | 0.8261 / 0.8424 | 0.4722 / 0.5512 | 0.3438 / 0.3438 | ❌ EO diff worsened sharply |\n",
    "\n",
    "---\n",
    "\n",
    "### What Worked Best\n",
    "\n",
    "- **Decision Tree + EG (DP):** Both DP and EO improved (**0.344 → 0.282, 0.094 → 0.041**) with only −1.6 pp acc.  \n",
    "- **Decision Tree + EG (EO):** EO ↓ to 0.073 at baseline accuracy — safe improvement.  \n",
    "- **Random Forest + GS (EO i=29):** EO ↓ to 0.078 with DP ↓, accuracy stable — balanced choice.  \n",
    "- **Random Forest + GS (EO i=40):** Highest acc (0.848) with EO ↓ to 0.089.  \n",
    "- **Random Forest + GS (DP i=31):** DP ↓ and EO ↓ jointly, small acc drop.\n",
    "\n",
    "---\n",
    "\n",
    "### What Did Not Help\n",
    "\n",
    "- **PCA+KNN post-processing:** 0% label flips → no effect.  \n",
    "- **CorrelationRemover (KNN):** Hurt accuracy and worsened EO.  \n",
    "- **RF + EG (EO/DP):** Constraints not binding → baseline repeated.  \n",
    "- **RF + Post (EO/DP):** Fairness worsened, accuracy unchanged.  \n",
    "- **MLP EG/GS/Post:** Either no movement or worsened EO/DP.  \n",
    "\n",
    "---\n",
    "\n",
    "### Practical Implications for CVD Prediction\n",
    "\n",
    "- **If priority = EO parity (error-rate fairness):**  \n",
    "  - Use **DT + EG (DP)** (strongest EO ↓ with small acc cost), or  \n",
    "  - **RF + GS (EO i=29/40)** (stable/high accuracy with EO ↓).  \n",
    "\n",
    "- **If priority = DP parity (outcome-rate fairness):**  \n",
    "  - **DT + EG (DP)** (lowest DP), or  \n",
    "  - **RF + GS (DP i=31)** (balanced DP ↓ & EO ↓, small acc trade-off).  \n",
    "\n",
    "- **Avoid:** PCA+KNN fixes, RF Post/EG, and MLP interventions — none provided meaningful fairness gains.  \n",
    "\n",
    "---\n",
    "\n",
    "### Final Recommendation\n",
    "\n",
    "For fair and accurate gender-sensitive CVD prediction:  \n",
    "- **Primary choice:** **Random Forest + GS (EO i=29 or i=40)** for balanced EO ↓ with high accuracy.  \n",
    "- **Secondary:** **Decision Tree + EG (DP)** if interpretability and consistent DP+EO improvements matter.  \n",
    "- **Tertiary:** **RF + GS (DP i=31)** if DP parity is the key objective.  \n",
    "- **KNN & MLP:** No reliable fairness improvements — better left out in clinical deployment.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcc2d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
