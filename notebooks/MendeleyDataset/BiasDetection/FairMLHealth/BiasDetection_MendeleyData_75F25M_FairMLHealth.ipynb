{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba0b2b9",
   "metadata": {},
   "source": [
    "### Bias Detection and Fairness Evaluation on CVD Prediction (Mendeley Dataset) using FairMLhealth\n",
    "Source: https://data.mendeley.com/datasets/dzz48mvjht/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2c29a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load X_test set\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f57b21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup successful\n"
     ]
    }
   ],
   "source": [
    "import fairmlhealth\n",
    "import aif360\n",
    "print(\"Environment setup successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6f126c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fairmlhealth\n",
      "Version: 1.0.2\n",
      "Summary: Health-centered variation analysis\n",
      "Home-page: https://github.com/KenSciResearch/fairMLHealth\n",
      "Author: Christine Allen\n",
      "Author-email: ca.magallen@gmail.com\n",
      "License: \n",
      "Location: c:\\users\\patri\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: aif360, ipython, jupyter, numpy, pandas, requests, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#have a look at the details of fairmlhealth - especially the version\n",
    "!pip show fairmlhealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76186324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__fairness_metrics', '__file__', '__loader__', '__name__', '__package__', '__path__', '__preprocessing', '__spec__', '__utils', '__validation', 'measure', 'performance_metrics']\n"
     ]
    }
   ],
   "source": [
    "#have a look at the modules that are within fairmlhealth\n",
    "\n",
    "print(dir(fairmlhealth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44657ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load necessary modules \n",
    "\n",
    "#import module measure to use measure.summary for bias detection\n",
    "from fairmlhealth import measure\n",
    "\n",
    "#import module for investigation of individual cohorts \n",
    "from fairmlhealth.__utils import iterate_cohorts\n",
    "\n",
    "#import FairRanges to flag high values\n",
    "from fairmlhealth.__utils import FairRanges\n",
    "\n",
    "# Wrap the fairness summary function for cohort-wise analysis\n",
    "@iterate_cohorts\n",
    "def cohort_summary(**kwargs):\n",
    "    return measure.summary(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baee5d9",
   "metadata": {},
   "source": [
    "During the execution of FairMLHealth and AIF360, several runtime warnings were raised (e.g., “AdversarialDebiasing will be unavailable” due to the absence of TensorFlow, and deprecation warnings from the inFairness package regarding PyTorch’s functorch.vmap). These warnings do not affect the fairness metrics or results presented in this study, as the unavailable components were not used. To maintain clarity of output, the warnings were silenced programmatically, and the analysis was conducted without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "859ae2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"inFairness\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"AdversarialDebiasing will be unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a099295",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models - KNN & DT\n",
    "\n",
    "#### K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d817251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_knn  y_prob_knn\n",
      "0       0       0           1         0.6\n",
      "1       1       0           0         0.0\n",
      "2       1       1           1         0.6\n",
      "3       1       1           1         0.6\n",
      "4       1       0           0         0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "knn_df = pd.read_csv(\"MendeleyData_75F25M_KNN_predictions.csv\")\n",
    "\n",
    "print(knn_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "871aa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_knn = knn_df[\"y_true\"].values\n",
    "y_prob_knn = knn_df[\"y_prob_knn\"].values\n",
    "y_pred_knn = knn_df[\"y_pred_knn\"].values\n",
    "gender_knn = knn_df[\"gender\"].values\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_knn = gender_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ad78836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.0075\n",
      "               Balanced Accuracy Difference           -0.0608\n",
      "               Balanced Accuracy Ratio                 0.9327\n",
      "               Disparate Impact Ratio                  1.0390\n",
      "               Equal Odds Difference                   0.1062\n",
      "               Equal Odds Ratio                        2.1333\n",
      "               Positive Predictive Parity Difference  -0.0792\n",
      "               Positive Predictive Parity Ratio        0.9150\n",
      "               Statistical Parity Difference           0.0220\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    }
   ],
   "source": [
    "knn_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_true_knn,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn,\n",
    "    prtc_attr=protected_attr_knn,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,   # skip inconsistency metrics that cause NearestNeighbors error\n",
    "    skip_performance=True\n",
    ")\n",
    "\n",
    "print(knn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19ba3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Custom scenario oriented bounds\n",
    "\n",
    "custom_ranges = {\n",
    "    \"tpr diff\": (-0.03, 0.03),\n",
    "    \"fpr diff\": (-0.03, 0.03),\n",
    "    \"equal odds difference\": (-0.04, 0.04),\n",
    "    \"statistical parity difference\": (-0.05, 0.05),\n",
    "    \"disparate impact ratio\": (0.9, 1.1),\n",
    "    \"selection ratio\": (0.9, 1.1),\n",
    "    \"auc difference\": (-0.02, 0.02),\n",
    "    \"balanced accuracy difference\": (-0.02, 0.02),\n",
    "}\n",
    "\n",
    "bounds = FairRanges().load_fair_ranges(custom_ranges=custom_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f14d2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  restore Styler.set_precision to adjust the highlighting color in the styled table\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "Styler = type(pd.DataFrame({\"_\":[0]}).style)  \n",
    "\n",
    "if not hasattr(Styler, \"set_precision\"):\n",
    "    def _set_precision(self, precision=4):\n",
    "        try:\n",
    "            return self.format(precision=precision)\n",
    "        except TypeError:\n",
    "            return self.format(formatter=lambda x:\n",
    "                f\"{x:.{precision}g}\" if isinstance(x, (int, float, np.floating)) else x\n",
    "            )\n",
    "    setattr(Styler, \"set_precision\", _set_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90435472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_441cb_row1_col0, #T_441cb_row4_col0, #T_441cb_row5_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_441cb\">\n",
       "  <caption>KNN Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_441cb_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_441cb_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_441cb_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_441cb_row0_col0\" class=\"data row0 col0\" >-0.0075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441cb_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_441cb_row1_col0\" class=\"data row1 col0\" >-0.0608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441cb_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_441cb_row2_col0\" class=\"data row2 col0\" >0.9327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441cb_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_441cb_row3_col0\" class=\"data row3 col0\" >1.0390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441cb_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_441cb_row4_col0\" class=\"data row4 col0\" >0.1062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441cb_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_441cb_row5_col0\" class=\"data row5 col0\" >2.1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441cb_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_441cb_row6_col0\" class=\"data row6 col0\" >-0.0792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441cb_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_441cb_row7_col0\" class=\"data row7 col0\" >0.9150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441cb_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_441cb_row8_col0\" class=\"data row8 col0\" >0.0220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_441cb_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_441cb_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_441cb_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20cd998b730>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "from fairmlhealth.__utils import Flagger\n",
    "\n",
    "class MyFlagger(Flagger):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.flag_color = \"#491ee6\"   \n",
    "        self.flag_type = \"background-color\"\n",
    "\n",
    "styled_knn = MyFlagger().apply_flag(\n",
    "    df=knn_bias,\n",
    "    caption=\"KNN Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d3278",
   "metadata": {},
   "source": [
    "## Gender Bias Detection Results for KNN Model \n",
    "\n",
    "The fairness evaluation of the **KNN model** with respect to **gender** reveals the following insights:\n",
    "\n",
    "### 1. Group Fairness  \n",
    "- **AUC Difference (-0.0075)**: Very small, indicating that the model's ranking ability is nearly identical across genders.  \n",
    "- **Balanced Accuracy Difference (-0.0608)**: Suggests a noticeable gap in balanced accuracy, with one gender receiving less accurate predictions.  \n",
    "- **Balanced Accuracy Ratio (0.9327)**: Below the ideal value of 1, confirming reduced fairness in balanced accuracy.  \n",
    "- **Equal Odds Difference (0.1062)**: Indicates that the model’s error rates (false positives/false negatives) differ between genders, which is a fairness concern.  \n",
    "- **Equal Odds Ratio (2.1333)**: A high ratio, showing unequal treatment in predictive performance across groups.  \n",
    "- **Disparate Impact Ratio (1.0390)**: Close to 1, suggesting that the likelihood of receiving a positive prediction is fairly balanced across genders.  \n",
    "- **Positive Predictive Parity Difference (-0.0792)**: Negative difference shows lower predictive precision for one gender.  \n",
    "- **Positive Predictive Parity Ratio (0.9150)**: Below 1, confirming disparity in predictive precision.  \n",
    "- **Statistical Parity Difference (0.0220)**: Very small, meaning overall prediction rates are nearly balanced across groups.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- The **AUC** and **statistical parity** metrics suggest near-fair performance.  \n",
    "- However, **balanced accuracy difference** and **equal odds difference** highlight substantial fairness gaps, meaning the model treats genders unequally in terms of predictive errors.  \n",
    "- The imbalance in the dataset (77% privileged class) likely contributes to these disparities.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0eeae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - KNN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>1.0722</td>\n",
       "      <td>-0.1062</td>\n",
       "      <td>0.4688</td>\n",
       "      <td>0.0792</td>\n",
       "      <td>1.093</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.9625</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>1.0174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0608</td>\n",
       "      <td>0.9327</td>\n",
       "      <td>0.1062</td>\n",
       "      <td>2.1333</td>\n",
       "      <td>-0.0792</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1.0390</td>\n",
       "      <td>-0.0154</td>\n",
       "      <td>0.9829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                        0.0608   \n",
       "1       gender             1                       -0.0608   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0722   -0.1062     0.4688    0.0792      1.093   \n",
       "1                   0.9327    0.1062     2.1333   -0.0792      0.915   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          -0.022           0.9625    0.0154     1.0174  \n",
       "1           0.022           1.0390   -0.0154     0.9829  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - KNN\")\n",
    "measure.bias(X_test, y_test, y_pred_knn, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05764be",
   "metadata": {},
   "source": [
    "## Stratified Fairness Metrics by Gender\n",
    "\n",
    "The table provides group-specific fairness metrics for the **KNN model**, separated by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Female (0): 0.0608 higher than baseline** (Balanced Accuracy Ratio = 1.0722)  \n",
    "- **Male (1): 0.0608 lower than baseline** (Balanced Accuracy Ratio = 0.9327)  \n",
    "➡️ The model is **more balanced for females** than for males.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0): FPR Diff = -0.1062, FPR Ratio = 0.4688**  \n",
    "- **Males (1): FPR Diff = 0.1062, FPR Ratio = 2.1333**  \n",
    "➡️ **Females have a much higher false positive rate**, meaning they are more often incorrectly flagged as positive cases compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0): PPV Diff = 0.0792, PPV Ratio = 1.093**  \n",
    "- **Males (1): PPV Diff = -0.0792, PPV Ratio = 0.915**  \n",
    "➡️ Precision is **better for females** (when predicted positive, it is more likely to be correct) but **worse for males**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0): Selection Ratio = 0.9625**  \n",
    "- **Males (1): Selection Ratio = 1.0390**  \n",
    "Males are selected for positive predictions slightly more often than females.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0): TPR Diff = 0.0154, TPR Ratio = 1.0174**  \n",
    "- **Males (1): TPR Diff = -0.0154, TPR Ratio = 0.9829**  \n",
    "➡️ Sensitivity is very similar, with females slightly advantaged.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged)**: suffer from **higher false positive rates** but benefit from **higher precision and slightly higher TPR**.  \n",
    "- **Males (privileged)**: have **lower false positives** but **lower precision**.  \n",
    "- Overall, the model’s fairness trade-off shows a **systematic disadvantage for females in specificity (FPR)**, even though they gain slightly in precision and sensitivity.  \n",
    "\n",
    "This imbalance reflects why fairness metrics such as **Equal Odds Difference (0.1062)** and **Equal Odds Ratio (2.1333)** flagged disparities in the earlier evaluation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fdeff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.8900</td>\n",
       "      <td>0.9043</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.9123</td>\n",
       "      <td>0.9342</td>\n",
       "      <td>0.8966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>0.8478</td>\n",
       "      <td>0.8679</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.3725</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>0.9279</td>\n",
       "      <td>0.8846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5649</td>\n",
       "      <td>0.9026</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>0.3695</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>0.9354</td>\n",
       "      <td>0.9000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.5700    0.8900   \n",
       "1        gender             0   46.0       0.5652           0.5870    0.8478   \n",
       "2        gender             1  154.0       0.5844           0.5649    0.9026   \n",
       "\n",
       "   F1-Score     FPR  PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9043  0.1190  0.3706     0.9123   0.9342  0.8966  \n",
       "1    0.8679  0.2000  0.3725     0.8519   0.9279  0.8846  \n",
       "2    0.9153  0.0938  0.3695     0.9310   0.9354  0.9000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fairmlhealth import measure\n",
    "import pandas as pd\n",
    "from IPython.display import display  \n",
    "\n",
    "# convert gender into DataFrame with a clear column name to get a nice table as output\n",
    "gender_df = pd.DataFrame({\"gender\": X_test[\"gender\"].astype(int)})\n",
    "\n",
    "\n",
    "# Get the stratified table\n",
    "perf_table_knn = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn\n",
    ")\n",
    "\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e845f8a",
   "metadata": {},
   "source": [
    "## FairMLHealth Stratified Bias Analysis (KNN by Gender)\n",
    "\n",
    "This table reports the **stratified performance of the KNN model** across gender subgroups, with  \n",
    "- **0 = Female**  \n",
    "- **1 = Male**  \n",
    "---\n",
    "\n",
    "### 1. Overall Performance\n",
    "- **Accuracy (0.890)** and **F1-score (0.9043)** indicate strong overall model performance.  \n",
    "- **ROC AUC (0.9342)** confirms the model’s ability to distinguish between CVD and non-CVD cases across genders.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Observations |\n",
    "|---------------|------------|----------|--------------|\n",
    "| Accuracy      | **0.8478** | **0.9026** | Accuracy is lower for females |\n",
    "| F1-Score      | 0.8679     | 0.9153   | Worse for females |\n",
    "| False Positive Rate (FPR) | **0.2000** | **0.0938** | Females suffer double the false positives |\n",
    "| Precision     | 0.8519     | 0.9310   | Predictions are less reliable for females |\n",
    "| ROC AUC       | 0.9279     | 0.9354   | Similar across genders |\n",
    "| True Positive Rate (TPR) | 0.8846     | 0.9000   | Slightly lower for females |\n",
    "| Observations  | 46 (23%)   | 154 (77%) | Male-dominant test sample |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Female patients (minority in the test set)** experience **systematic disadvantage**:  \n",
    "  - Lower accuracy and F1-score.  \n",
    "  - **False positives are twice as common** compared to males (20% vs. 9.4%).  \n",
    "  - Precision is significantly lower, meaning when the model predicts CVD for females, it is less often correct.  \n",
    "\n",
    "- **Male patients** benefit from higher accuracy, F1, and precision.  \n",
    "\n",
    "- Despite these disparities, **ROC AUC values are similar**, showing that the model distinguishes positive and negative cases fairly consistently, but **error distribution is biased against females**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The KNN model, although strong overall, is **less fair to female patients**. It tends to over-predict CVD for women, leading to more false positives, while men receive more accurate and reliable predictions. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5db1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.8846\n",
      "  False Positive Rate (FPR): 0.2000\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9000\n",
      "  False Positive Rate (FPR): 0.0938\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#group specific error analysis\n",
    "\n",
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_knn == 0)  # unprivileged group (female)\n",
    "male_mask   = (protected_attr_knn == 1)  # privileged group (male)\n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba4143",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis\n",
    "\n",
    "To further examine fairness at the subgroup level, we compared the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.8846  | 0.2000  |\n",
    "| Privileged (Male)      | 0.9000  | 0.0938  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- The **privileged group (male)** shows a slightly higher **TPR (90.00%)** than the unprivileged group (88.46%), meaning the model is marginally better at correctly identifying positives for males.  \n",
    "- However, the **FPR is more than twice as high for females (20.00%) compared to males (9.38%)**, indicating that women are more likely to be incorrectly flagged as having CVD.  \n",
    "- This imbalance suggests that, while overall sensitivity is fairly similar, the **specificity of the model disproportionately disadvantages females**, as they suffer from a higher rate of false positives.  \n",
    "- These subgroup disparities align with the fairness metrics, where the **Equal Odds Difference and Ratio highlight unequal error distributions** between genders.  \n",
    "- In summary, the results reveal a **systematic disadvantage for the unprivileged group (females)**, underscoring the importance of applying **fairness mitigation techniques** to reduce bias in error rates.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb317ca",
   "metadata": {},
   "source": [
    "### Decision Tree - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2adbaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_dt  y_prob_dt\n",
      "0       0       0          0   0.000000\n",
      "1       1       0          0   0.000000\n",
      "2       1       1          1   0.960486\n",
      "3       1       1          1   0.960486\n",
      "4       1       0          0   0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "dt_df = pd.read_csv(\"MendeleyData_75F25M_DT_classweightedtuned_predictions.csv\")\n",
    "\n",
    "print(dt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06530601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract common columns\n",
    "y_true_dt = dt_df[\"y_true\"].values\n",
    "y_prob_dt = dt_df[\"y_prob_dt\"].values\n",
    "y_pred_dt = dt_df[\"y_pred_dt\"].values\n",
    "gender_dt = dt_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_dt = gender_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Decision Tree Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.1194\n",
      "               Balanced Accuracy Difference           -0.0404\n",
      "               Balanced Accuracy Ratio                 0.9552\n",
      "               Disparate Impact Ratio                  0.9972\n",
      "               Equal Odds Difference                   0.0594\n",
      "               Equal Odds Ratio                        1.4222\n",
      "               Positive Predictive Parity Difference  -0.0471\n",
      "               Positive Predictive Parity Ratio        0.9479\n",
      "               Statistical Parity Difference          -0.0017\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Gender Bias Report\n",
    "print(\"\\nDecision Tree Gender Bias Report\")\n",
    "\n",
    "dt_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt,\n",
    "    prtc_attr=protected_attr_dt,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,  \n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(dt_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afc2d88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ab27d_row0_col0, #T_ab27d_row1_col0, #T_ab27d_row4_col0, #T_ab27d_row5_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ab27d\">\n",
       "  <caption>Decision Tree Fairness (Gender) — Custom Clinical Bounds</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ab27d_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ab27d_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_ab27d_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_ab27d_row0_col0\" class=\"data row0 col0\" >-0.1194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab27d_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_ab27d_row1_col0\" class=\"data row1 col0\" >-0.0404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab27d_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_ab27d_row2_col0\" class=\"data row2 col0\" >0.9552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab27d_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_ab27d_row3_col0\" class=\"data row3 col0\" >0.9972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab27d_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_ab27d_row4_col0\" class=\"data row4 col0\" >0.0594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab27d_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_ab27d_row5_col0\" class=\"data row5 col0\" >1.4222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab27d_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_ab27d_row6_col0\" class=\"data row6 col0\" >-0.0471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab27d_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_ab27d_row7_col0\" class=\"data row7 col0\" >0.9479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab27d_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_ab27d_row8_col0\" class=\"data row8 col0\" >-0.0017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab27d_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_ab27d_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_ab27d_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20cd998b790>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "styled_dt = MyFlagger().apply_flag(\n",
    "    df=dt_bias,\n",
    "    caption=\"Decision Tree Fairness (Gender) — Custom Clinical Bounds\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1c7f5",
   "metadata": {},
   "source": [
    "## Fairness Metrics Interpretation for Decision Tree \n",
    "\n",
    "The fairness evaluation of the **Decision Tree model** with respect to gender provides the following insights:\n",
    "\n",
    "### 1. Group Fairness\n",
    "- **AUC Difference (-0.1194)**: Indicates a noticeable disparity in ranking quality between males and females, with the model performing worse for one group.  \n",
    "- **Balanced Accuracy Difference (-0.0404)**: Suggests that balanced accuracy is moderately higher for one gender.  \n",
    "- **Balanced Accuracy Ratio (0.9552)**: Below the ideal value of 1, confirming reduced fairness across groups.  \n",
    "- **Equal Odds Difference (0.0594)**: Shows that error rates (false positives/false negatives) differ, though the difference is smaller than what was observed for KNN.  \n",
    "- **Equal Odds Ratio (1.4222)**: A ratio greater than 1, highlighting unequal treatment across genders in predictive errors.  \n",
    "- **Disparate Impact Ratio (0.9972)**: Very close to 1, meaning the overall likelihood of receiving a positive prediction is almost equal between males and females.  \n",
    "- **Positive Predictive Parity Difference (-0.0471)**: Suggests that predictive precision is somewhat lower for one gender.  \n",
    "- **Positive Predictive Parity Ratio (0.9479)**: Less than 1, confirming unequal predictive reliability.  \n",
    "- **Statistical Parity Difference (-0.0017)**: Close to zero, meaning overall prediction rates are nearly identical across genders.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Decision Tree model exhibits **relatively small but consistent fairness disparities** across genders:  \n",
    "- It shows stronger group differences in **AUC** (ranking ability) and **balanced accuracy**, suggesting uneven predictive performance.  \n",
    "- **Equal Odds metrics** point to an imbalance in error rates, though less pronounced than in KNN.  \n",
    "- At the same time, **statistical and disparate impact measures are near ideal**, indicating that the overall distribution of predictions is balanced across genders.  \n",
    "\n",
    "**Overall**, the Decision Tree is *fairer than KNN in terms of statistical parity*, but it still struggles with **error rate fairness**, particularly in ranking ability and predictive precision, which disadvantages the unprivileged group (females).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "631b7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - DT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>1.0469</td>\n",
       "      <td>-0.0594</td>\n",
       "      <td>0.7031</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>1.0550</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>1.0028</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>1.0231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0404</td>\n",
       "      <td>0.9552</td>\n",
       "      <td>0.0594</td>\n",
       "      <td>1.4222</td>\n",
       "      <td>-0.0471</td>\n",
       "      <td>0.9479</td>\n",
       "      <td>-0.0017</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>-0.0214</td>\n",
       "      <td>0.9774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                        0.0404   \n",
       "1       gender             1                       -0.0404   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0469   -0.0594     0.7031    0.0471     1.0550   \n",
       "1                   0.9552    0.0594     1.4222   -0.0471     0.9479   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0017           1.0028    0.0214     1.0231  \n",
       "1         -0.0017           0.9972   -0.0214     0.9774  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - DT\")\n",
    "measure.bias(X_test, y_test, y_pred_dt, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa9add",
   "metadata": {},
   "source": [
    "## Stratified Fairness Metrics by Gender – Decision Tree\n",
    "\n",
    "The table provides subgroup-specific fairness metrics for the **Decision Tree model**, separated by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0): +0.0404 (Ratio = 1.0469)** → Higher balanced accuracy compared to the baseline.  \n",
    "- **Males (1): -0.0404 (Ratio = 0.9552)** → Lower balanced accuracy relative to the baseline.  \n",
    "➡️ The model achieves slightly **better balanced accuracy for females**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0): FPR Diff = -0.0594, FPR Ratio = 0.7031**  \n",
    "- **Males (1): FPR Diff = +0.0594, FPR Ratio = 1.4222**  \n",
    "➡️ Females have a **higher false positive rate** than males, though the disparity is smaller than observed in the KNN model.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0): PPV Diff = +0.0471, PPV Ratio = 1.0550**  \n",
    "- **Males (1): PPV Diff = -0.0471, PPV Ratio = 0.9479**  \n",
    "➡️ Precision is **slightly better for females**, meaning predictions for them are somewhat more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0): Selection Ratio = 1.0028**  \n",
    "- **Males (1): Selection Ratio = 0.9972**  \n",
    "➡️ The model selects females and males for positive predictions at almost identical rates.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0): TPR Diff = +0.0214, TPR Ratio = 1.0231**  \n",
    "- **Males (1): TPR Diff = -0.0214, TPR Ratio = 0.9774**  \n",
    "➡️ Sensitivity is slightly higher for females, meaning they are marginally more likely to be correctly identified as positive cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged)**: show slightly **better balanced accuracy, precision, and sensitivity**, but still experience a **higher false positive rate** than males.  \n",
    "- **Males (privileged)**: benefit from lower false positives but show somewhat lower precision and sensitivity.  \n",
    "- Overall, disparities exist but are **less pronounced than in KNN**, suggesting that the **Decision Tree provides a more balanced treatment across genders**, even if females still face a higher risk of false positives.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acccc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.6100</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.9160</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8934</td>\n",
       "      <td>0.9276</td>\n",
       "      <td>0.9397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>0.8365</td>\n",
       "      <td>0.9231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.6104</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>0.9239</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.3924</td>\n",
       "      <td>0.9043</td>\n",
       "      <td>0.9559</td>\n",
       "      <td>0.9444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.6100    0.9000   \n",
       "1        gender             0   46.0       0.5652           0.6087    0.8696   \n",
       "2        gender             1  154.0       0.5844           0.6104    0.9091   \n",
       "\n",
       "   F1-Score     FPR  PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9160  0.1548       —     0.8934   0.9276  0.9397  \n",
       "1    0.8889  0.2000       —     0.8571   0.8365  0.9231  \n",
       "2    0.9239  0.1406  0.3924     0.9043   0.9559  0.9444  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table for DT\n",
    "perf_table_dt = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_dt = perf_table_dt.fillna(\"—\")\n",
    "\n",
    "# Display pretty table\n",
    "display(perf_table_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936a469",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Decision Tree by Gender\n",
    "\n",
    "This table presents the **stratified performance metrics** of the Decision Tree model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9000)** and **F1-score (0.9160)** indicate strong general performance.  \n",
    "- **ROC AUC (0.9276)** shows good discriminatory power.  \n",
    "- **Precision (0.8934)** and **TPR (0.9397)** confirm a good balance between sensitivity and predictive reliability.  \n",
    "- **Note**: For some subgroups, **PR AUC is reported as NaN** because the subgroup sample size did not allow calculation of a meaningful precision–recall curve. This does not affect the validity of the other metrics (Accuracy, F1, ROC AUC, etc.), which remain comparable across groups.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8696     | 0.9091   | Accuracy is lower for females. |\n",
    "| **F1-Score**  | 0.8889     | 0.9239   | Model performs better for males. |\n",
    "| **FPR**       | 0.2000     | 0.1406   | Females experience more false positives. |\n",
    "| **Precision** | 0.8571     | 0.9043   | Predictions are more reliable for males. |\n",
    "| **ROC AUC**   | 0.8365     | 0.9559   | Substantial gap; males benefit from much stronger ranking performance. |\n",
    "| **TPR**       | 0.9231     | 0.9444   | Slightly higher sensitivity for males. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Lower accuracy, F1-score, and precision compared to males.  \n",
    "  - **Much lower ROC AUC (0.8365 vs. 0.9559)** → the model discriminates less effectively between positive and negative cases for females.  \n",
    "  - Higher false positive rate (20% vs. 14%), meaning more healthy females are incorrectly predicted as having CVD.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Benefit from higher accuracy, F1, and precision.  \n",
    "  - Stronger ROC AUC and lower FPR → predictions are both more accurate and more reliable.  \n",
    "  - Slightly higher sensitivity (TPR), showing males are marginally more likely to be correctly identified when they have CVD.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Decision Tree model demonstrates **systematic disadvantages for females**. They face **more false positives**, **weaker precision**, and **much poorer discriminatory power (ROC AUC)** compared to males.  \n",
    "Although sensitivity (TPR) is relatively balanced, the disparities in precision and AUC highlight a fairness issue where **males consistently receive more favorable predictive performance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53556439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.9231\n",
      "  False Positive Rate (FPR): 0.2000\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9444\n",
      "  False Positive Rate (FPR): 0.1406\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_dt == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_dt == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97580f6",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Decision Tree\n",
    "\n",
    "This section analyzes the classification performance of the Decision Tree model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Unprivileged (female = 0)    | 0.9231  | 0.2000  |\n",
    "| Privileged (male = 1)        | 0.9444  | 0.1406  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR)** is high and comparable across genders:  \n",
    "  - Females (unprivileged): **92.31%**  \n",
    "  - Males (privileged): **94.44%**  \n",
    "  - This indicates that the model identifies positive CVD cases with similar effectiveness for both groups.  \n",
    "\n",
    "- **False Positive Rate (FPR)** reveals more variation:  \n",
    "  - Females: **20.00%**  \n",
    "  - Males: **14.06%**  \n",
    "  - Females are therefore **more frequently misclassified as having CVD** when they do not, which reflects a disadvantage for the unprivileged group.  \n",
    "\n",
    "#### Implications\n",
    "\n",
    "- The **Decision Tree achieves balanced sensitivity (TPR)** across genders, but the **higher FPR for females** suggests a bias in specificity.  \n",
    "- These differences contribute to fairness metrics such as the **Equal Odds Difference (0.0594)** and **Equal Odds Ratio (1.4222)** observed earlier.  \n",
    "- In summary, while the Decision Tree performs consistently in detecting true positives across genders, it still places a **systematic burden on females** by producing more false positives in this group.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5168d",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aaa2a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_rf  y_prob\n",
      "0       0       0          0    0.23\n",
      "1       1       0          0    0.14\n",
      "2       1       1          1    0.96\n",
      "3       1       1          1    0.81\n",
      "4       1       0          0    0.02\n"
     ]
    }
   ],
   "source": [
    "rf_df = pd.read_csv(\"MendeleyData_75F25M_RF_predictions.csv\")\n",
    "print(rf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "278af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_rf = rf_df[\"y_true\"].values\n",
    "y_pred_rf = rf_df[\"y_pred_rf\"].values\n",
    "y_prob_rf = rf_df[\"y_prob\"].values\n",
    "gender_rf = rf_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_rf = gender_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "689f0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Random Forest Gender Bias Report\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0201\n",
      "               Balanced Accuracy Difference            0.0641\n",
      "               Balanced Accuracy Ratio                 1.0703\n",
      "               Disparate Impact Ratio                  1.0511\n",
      "               Equal Odds Difference                   0.1000\n",
      "               Equal Odds Ratio                        0.6400\n",
      "               Positive Predictive Parity Difference   0.0211\n",
      "               Positive Predictive Parity Ratio        1.0224\n",
      "               Statistical Parity Difference           0.0285\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Gender Bias Report\n",
    "print(\"\\n Random Forest Gender Bias Report\")\n",
    "\n",
    "rf_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf,\n",
    "    prtc_attr=protected_attr_rf,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(rf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7a8e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_81f96_row0_col0, #T_81f96_row1_col0, #T_81f96_row4_col0, #T_81f96_row5_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_81f96\">\n",
       "  <caption>Random Forest Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_81f96_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_81f96_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_81f96_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_81f96_row0_col0\" class=\"data row0 col0\" >0.0201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f96_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_81f96_row1_col0\" class=\"data row1 col0\" >0.0641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f96_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_81f96_row2_col0\" class=\"data row2 col0\" >1.0703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f96_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_81f96_row3_col0\" class=\"data row3 col0\" >1.0511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f96_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_81f96_row4_col0\" class=\"data row4 col0\" >0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f96_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_81f96_row5_col0\" class=\"data row5 col0\" >0.6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f96_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_81f96_row6_col0\" class=\"data row6 col0\" >0.0211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f96_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_81f96_row7_col0\" class=\"data row7 col0\" >1.0224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f96_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_81f96_row8_col0\" class=\"data row8 col0\" >0.0285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f96_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_81f96_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_81f96_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20cbc0b2650>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for Random Forest\n",
    "styled_rf = MyFlagger().apply_flag(\n",
    "    df=rf_bias,\n",
    "    caption=\"Random Forest Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7c650",
   "metadata": {},
   "source": [
    "## Fairness Metrics Interpretation for Random Forest (Gender)\n",
    "\n",
    "The fairness evaluation of the **Random Forest model** with respect to gender provides the following insights:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness\n",
    "- **AUC Difference (0.0201)**: Very small, suggesting the model’s ranking ability is almost equal across genders.  \n",
    "- **Balanced Accuracy Difference (0.0641)**: Indicates that balanced accuracy is moderately higher for one group, favoring fairness gaps.  \n",
    "- **Balanced Accuracy Ratio (1.0703)**: Slightly above 1, showing a modest imbalance in balanced accuracy between genders.  \n",
    "- **Disparate Impact Ratio (1.0511)**: Close to 1, meaning that the overall rate of positive predictions is fairly balanced across groups.  \n",
    "- **Equal Odds Difference (0.1000)**: Suggests a noticeable disparity in error rates (false positives and false negatives) between genders.  \n",
    "- **Equal Odds Ratio (0.6400)**: Below 1, reinforcing that one gender experiences fewer errors than the other.  \n",
    "- **Positive Predictive Parity Difference (0.0211)**: Very small, indicating predictive precision is nearly equal across genders.  \n",
    "- **Positive Predictive Parity Ratio (1.0224)**: Close to 1, confirming similar reliability in positive predictions.  \n",
    "- **Statistical Parity Difference (0.0285)**: Small but positive, meaning one group receives slightly more positive predictions than the other.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Random Forest model demonstrates **good overall fairness** compared to the KNN and Decision Tree models:  \n",
    "- **Ranking ability (AUC)** and **precision (PPV)** are nearly identical across genders.  \n",
    "- **Positive prediction distribution** (statistical parity and disparate impact) is also well balanced.  \n",
    "- However, **Equal Odds metrics (difference = 0.1000, ratio = 0.64)** reveal disparities in error rates, meaning one gender (likely females) is still more affected by false positives or false negatives.  \n",
    "\n",
    "**Overall**, the Random Forest performs consistently across genders in most fairness metrics but still shows **residual bias in error distribution**, indicating room for improvement in ensuring equal treatment.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47479386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - RF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>0.9343</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>1.5625</td>\n",
       "      <td>-0.0211</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>-0.0285</td>\n",
       "      <td>0.9514</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>1.0703</td>\n",
       "      <td>-0.0281</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>1.0224</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>1.0511</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.1111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0641   \n",
       "1       gender             1                        0.0641   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9343    0.0281     1.5625   -0.0211     0.9781   \n",
       "1                   1.0703   -0.0281     0.6400    0.0211     1.0224   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0         -0.0285           0.9514      -0.1     0.9000  \n",
       "1          0.0285           1.0511       0.1     1.1111  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - RF\")\n",
    "measure.bias(X_test, y_test, y_pred_rf, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be772b16",
   "metadata": {},
   "source": [
    "## Stratified Fairness Metrics by Gender – Random Forest\n",
    "\n",
    "The table presents subgroup-specific fairness metrics for the **Random Forest model**, separated by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0): -0.0641, Ratio = 0.9343** → Balanced accuracy is lower for females.  \n",
    "- **Males (1): +0.0641, Ratio = 1.0703** → Males benefit from higher balanced accuracy.  \n",
    "➡️ The model performs more reliably for males.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0): FPR Diff = 0.0281, Ratio = 1.5625** → Females have a higher false positive rate.  \n",
    "- **Males (1): FPR Diff = -0.0281, Ratio = 0.6400** → Males experience fewer false positives.  \n",
    "➡️ This indicates a **specificity disadvantage for females**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0): PPV Diff = -0.0211, Ratio = 0.9781** → Precision is slightly lower.  \n",
    "- **Males (1): PPV Diff = +0.0211, Ratio = 1.0224** → Precision is slightly higher.  \n",
    "➡️ Predictions are **more reliable for males**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0): Selection Ratio = 0.9514**  \n",
    "- **Males (1): Selection Ratio = 1.0511**  \n",
    "➡️ Males are selected for positive predictions slightly more often.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0): TPR Diff = -0.1000, Ratio = 0.9000** → Sensitivity is lower.  \n",
    "- **Males (1): TPR Diff = +0.1000, Ratio = 1.1111** → Sensitivity is higher.  \n",
    "➡️ The model is **better at correctly identifying positive cases for males**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged)**: Disadvantaged across multiple metrics — **lower balanced accuracy, higher false positives, weaker precision, and lower sensitivity**.  \n",
    "- **Males (privileged)**: Benefit from **higher accuracy, fewer false positives, more reliable predictions, and better sensitivity**.  \n",
    "\n",
    "Overall, the Random Forest introduces a **systematic bias favoring males**. While group differences are not extreme, the **consistent performance gap across key metrics** indicates fairness concerns that should be addressed through mitigation strategies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6573254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.9345</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9469</td>\n",
       "      <td>0.9852</td>\n",
       "      <td>0.9224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>0.9783</td>\n",
       "      <td>0.9811</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.4348</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5584</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>0.9205</td>\n",
       "      <td>0.0781</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9419</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.9000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.5650    0.9250   \n",
       "1        gender             0   46.0       0.5652           0.5870    0.9783   \n",
       "2        gender             1  154.0       0.5844           0.5584    0.9091   \n",
       "\n",
       "   F1-Score     FPR  PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9345  0.0714       —     0.9469   0.9852  0.9224  \n",
       "1    0.9811  0.0500  0.4348     0.9630   1.0000  1.0000  \n",
       "2    0.9205  0.0781       —     0.9419   0.9799  0.9000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_rf = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_rf = perf_table_rf.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de72316",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Random Forest by Gender\n",
    "\n",
    "This table presents the **stratified performance metrics** of the Random Forest model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9250)** and **F1-score (0.9345)** demonstrate strong overall performance.  \n",
    "- **ROC AUC (0.9852)** indicates excellent discriminatory power.  \n",
    "- **Precision (0.9469)** and **TPR (0.9224)** confirm the model balances sensitivity with predictive reliability.  \n",
    "- **Note**: For some subgroups, **PR AUC is reported as “—”** because the subgroup sample size did not allow calculation of a meaningful precision–recall curve. This does not affect the validity of the other metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.9783     | 0.9091   | Accuracy is higher for females. |\n",
    "| **F1-Score**  | 0.9811     | 0.9205   | Stronger F1 performance for females. |\n",
    "| **FPR**       | 0.0500     | 0.0781   | Females experience fewer false positives. |\n",
    "| **Precision** | 0.9630     | 0.9419   | Predictions are more reliable for females. |\n",
    "| **ROC AUC**   | 1.0000     | 0.9799   | Females achieve near-perfect discrimination. |\n",
    "| **TPR**       | 1.0000     | 0.9000   | Females are perfectly identified when they have CVD. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Benefit from **higher accuracy, F1, precision, and sensitivity (TPR)** compared to males.  \n",
    "  - **ROC AUC = 1.0000**, suggesting near-perfect discrimination between positive and negative cases.  \n",
    "  - Lower false positive rate (5% vs. 7.8%), meaning fewer healthy females are misclassified.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Performance remains strong, but consistently **lower than for females** across most metrics.  \n",
    "  - Higher false positive rate and slightly weaker sensitivity, indicating less reliable predictions compared to females.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Random Forest model appears to perform **better for females** across nearly all metrics.  \n",
    "While overall performance is excellent for both groups, females receive systematically **more favorable outcomes** (higher F1, higher precision, lower FPR, and perfect TPR/ROC AUC), suggesting that this model may be biased **in favor of females** rather than against them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a52946a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 1.0000\n",
      "  False Positive Rate (FPR): 0.0500\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9000\n",
      "  False Positive Rate (FPR): 0.0781\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_rf == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_rf == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06d248",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Random Forest\n",
    "\n",
    "This section presents the performance of the Random Forest model across gender groups, focusing on **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Unprivileged (female = 0)    | 1.0000  | 0.0500  |\n",
    "| Privileged (male = 1)        | 0.9000  | 0.0781  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR)** is **perfect for females (100%)**, compared to **90.00% for males**.  \n",
    "  - This means the model successfully identifies all true positive cases among females, but misses a small proportion of cases among males.\n",
    "\n",
    "- **False Positive Rate (FPR)** is **lower for females (5.00%)** than for males (7.81%).  \n",
    "  - This indicates that males are more likely to receive **false alarms** compared to females.\n",
    "\n",
    "#### Implications\n",
    "\n",
    "- The model exhibits a **gender-based disparity**:  \n",
    "  - It is both **more sensitive** (higher TPR) and **more specific** (lower FPR) for females,  \n",
    "  - while males face slightly worse outcomes on both measures.\n",
    "\n",
    "- This asymmetry suggests that the Random Forest model may be **biased in favor of females**. In practice, this means females are almost always correctly detected when they have CVD and face fewer false positives, whereas males are at a **double disadvantage** (slightly higher missed cases and more false alarms).\n",
    "\n",
    "- Depending on the clinical use case, these imbalances could have **important consequences**: males may experience both a higher risk of underdiagnosis and more unnecessary follow-ups, raising fairness concerns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136de6",
   "metadata": {},
   "source": [
    "### Deep Learning Model - Feed Forward Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3c69d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_lbfgs  y_prob_lbfgs\n",
      "0       0       0             0      0.012395\n",
      "1       1       0             0      0.000004\n",
      "2       1       1             1      1.000000\n",
      "3       1       1             1      0.999969\n",
      "4       1       0             0      0.000014\n"
     ]
    }
   ],
   "source": [
    "mlp_df = pd.read_csv(\"MendeleyData_75F25M_MLP_lbfgs_predictions.csv\")\n",
    "print(mlp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f65a498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns \n",
    "y_true_mlp = mlp_df[\"y_true\"].values \n",
    "y_prob_mlp = mlp_df[\"y_prob_lbfgs\"].values\n",
    "y_pred_mlp = mlp_df[\"y_pred_lbfgs\"].values\n",
    "gender_mlp = mlp_df[\"gender\"].values \n",
    "\n",
    "# Use gender_mlp as the protected attribute\n",
    "protected_attr_mlp = gender_mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6969bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.0123\n",
      "               Balanced Accuracy Difference            0.0084\n",
      "               Balanced Accuracy Ratio                 1.0093\n",
      "               Disparate Impact Ratio                  1.0005\n",
      "               Equal Odds Difference                   0.0231\n",
      "               Equal Odds Ratio                        1.0667\n",
      "               Positive Predictive Parity Difference  -0.0080\n",
      "               Positive Predictive Parity Ratio        0.9915\n",
      "               Statistical Parity Difference           0.0003\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    }
   ],
   "source": [
    "#Run fairmlhealth bias detection for MLP \n",
    "\n",
    "mlp_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp,\n",
    "    prtc_attr=protected_attr_mlp,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(mlp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cbf7eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b6dbb\">\n",
       "  <caption>MLP Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b6dbb_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b6dbb_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_b6dbb_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_b6dbb_row0_col0\" class=\"data row0 col0\" >-0.0123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6dbb_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_b6dbb_row1_col0\" class=\"data row1 col0\" >0.0084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6dbb_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_b6dbb_row2_col0\" class=\"data row2 col0\" >1.0093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6dbb_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_b6dbb_row3_col0\" class=\"data row3 col0\" >1.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6dbb_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_b6dbb_row4_col0\" class=\"data row4 col0\" >0.0231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6dbb_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_b6dbb_row5_col0\" class=\"data row5 col0\" >1.0667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6dbb_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_b6dbb_row6_col0\" class=\"data row6 col0\" >-0.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6dbb_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_b6dbb_row7_col0\" class=\"data row7 col0\" >0.9915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6dbb_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_b6dbb_row8_col0\" class=\"data row8 col0\" >0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6dbb_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_b6dbb_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_b6dbb_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20cd999d3f0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for MLP\n",
    "styled_mlp = MyFlagger().apply_flag(\n",
    "    df=mlp_bias,\n",
    "    caption=\"MLP Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713caa5",
   "metadata": {},
   "source": [
    "## Fairness Evaluation – MLP by Gender\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (-0.0123)**: The area under the ROC curve is slightly higher for males than females, but the difference is minimal (< 0.02).  \n",
    "- **Balanced Accuracy Difference (0.0084)** and **Ratio (1.0093)**: Balanced accuracy is almost identical across genders, with females performing slightly worse.  \n",
    "- **Disparate Impact Ratio (1.0005)**: Essentially perfect parity in selection rates between males and females.  \n",
    "- **Equal Odds Difference (0.0231)** and **Ratio (1.0667)**: Indicates small disparities in error rates (TPR/FPR) between groups, with males slightly favored.  \n",
    "- **Positive Predictive Parity Difference (-0.0080)** and **Ratio (0.9915)**: Predictive reliability of positive classifications is very similar across genders.  \n",
    "- **Statistical Parity Difference (0.0003)**: Selection rates are almost exactly equal for males and females.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "\n",
    "- The MLP shows **very small fairness disparities** across all metrics.  \n",
    "- Differences in **AUC, balanced accuracy, and predictive parity** are negligible, suggesting the model performs similarly for both genders.  \n",
    "- **Equal odds difference (0.0231)** is the only metric showing a small gap, meaning error rates differ slightly, but still within modest bounds.  \n",
    "- The **disparate impact ratio (1.0005)** and **statistical parity difference (0.0003)** confirm that males and females are selected at nearly identical rates.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "The MLP model demonstrates **high fairness with respect to gender**.  \n",
    "While males (privileged group) appear to have a **slight advantage** in terms of error rates and AUC, the overall differences are **minimal and unlikely to represent significant bias**. This model is more balanced compared to others that exhibited stronger gender disparities.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7aa66097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - MLP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0084</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>-0.0063</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.0086</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>-0.0231</td>\n",
       "      <td>0.9750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>1.0093</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>1.0667</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>1.0005</td>\n",
       "      <td>0.0231</td>\n",
       "      <td>1.0256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0084   \n",
       "1       gender             1                        0.0084   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9908   -0.0063     0.9375     0.008     1.0086   \n",
       "1                   1.0093    0.0063     1.0667    -0.008     0.9915   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0         -0.0003           0.9995   -0.0231     0.9750  \n",
       "1          0.0003           1.0005    0.0231     1.0256  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - MLP\")\n",
    "measure.bias(X_test, y_test, y_pred_mlp, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576976f",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – MLP by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the MLP model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Difference**: Females = -0.0084, Males = 0.0084.  \n",
    "- **Ratio**: Females = 0.9908, Males = 1.0093.  \n",
    "- ➝ Balanced accuracy is **nearly identical** across genders, with a negligible advantage for males.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Difference**: Females = -0.0063, Males = 0.0063.  \n",
    "- **Ratio**: Females = 0.9375, Males = 1.0667.  \n",
    "- ➝ Females have a **slightly lower false positive rate**, while males face marginally more false alarms.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Difference**: Females = 0.008, Males = -0.008.  \n",
    "- **Ratio**: Females = 1.0086, Males = 0.9915.  \n",
    "- ➝ Precision is **slightly higher for females**, meaning predictions of CVD for females are marginally more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Difference**: Females = -0.0003, Males = 0.0003.  \n",
    "- **Ratio**: Females = 0.9995, Males = 1.0005.  \n",
    "- ➝ Selection rates are essentially **equal across genders**, showing no meaningful disparity.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Difference**: Females = -0.0231, Males = 0.0231.  \n",
    "- **Ratio**: Females = 0.9750, Males = 1.0256.  \n",
    "- ➝ Males have a **slight advantage in sensitivity**, being more likely to be correctly identified when they truly have CVD.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- The MLP model demonstrates **high parity across gender groups**.  \n",
    "- Most fairness differences are **very small (<0.02)** and ratios remain close to **1.0**, well within common fairness tolerance thresholds.  \n",
    "- Small tendencies:\n",
    "  - **Females** benefit from slightly higher **precision** and lower **false positive rates**.  \n",
    "  - **Males** benefit from slightly higher **sensitivity (TPR)**.  \n",
    "- Overall, the disparities are **minor** and unlikely to indicate systematic gender bias in this model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fbc2caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9292</td>\n",
       "      <td>0.9698</td>\n",
       "      <td>0.9052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.9130</td>\n",
       "      <td>0.9231</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9231</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>0.9231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5649</td>\n",
       "      <td>0.9026</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.9000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.5650    0.9050   \n",
       "1        gender             0   46.0       0.5652           0.5652    0.9130   \n",
       "2        gender             1  154.0       0.5844           0.5649    0.9026   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9170  0.0952      —     0.9292   0.9698  0.9052  \n",
       "1    0.9231  0.1000      —     0.9231   0.9577  0.9231  \n",
       "2    0.9153  0.0938      —     0.9310   0.9700  0.9000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_mlp = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_mlp = perf_table_mlp.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec7c2f",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – MLP by Gender\n",
    "\n",
    "This table shows the **stratified performance metrics** of the MLP model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9050)** and **F1-score (0.9170)** indicate strong general performance.  \n",
    "- **ROC AUC (0.9698)** shows excellent discriminatory ability between positive and negative cases.  \n",
    "- **Precision (0.9292)** and **TPR (0.9052)** suggest a good balance between predictive reliability and sensitivity.  \n",
    "- **Note**: PR AUC is reported as “—” because the subgroup sample size did not allow calculation of a meaningful precision–recall curve.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.9130     | 0.9026   | Accuracy is slightly higher for females. |\n",
    "| **F1-Score**  | 0.9231     | 0.9153   | Females perform marginally better. |\n",
    "| **FPR**       | 0.1000     | 0.0938   | Males experience slightly fewer false positives. |\n",
    "| **Precision** | 0.9231     | 0.9310   | Predictions are slightly more reliable for males. |\n",
    "| **ROC AUC**   | 0.9577     | 0.9700   | Males benefit from a small advantage in ranking performance. |\n",
    "| **TPR**       | 0.9231     | 0.9000   | Females are somewhat more likely to be correctly identified when they have CVD. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Achieve higher accuracy and F1-score compared to males.  \n",
    "  - Benefit from higher sensitivity (TPR = 0.9231), meaning fewer missed positive cases.  \n",
    "  - However, they face a slightly higher false positive rate (10% vs. 9.38%), leading to more false alarms.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Have slightly stronger precision and ROC AUC, indicating more reliable predictions and better ranking of outcomes.  \n",
    "  - Lower false positive rate suggests healthier males are less likely to be misclassified.  \n",
    "  - Slightly weaker sensitivity compared to females, meaning a few more missed positive cases.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The MLP model achieves **balanced performance across genders**.  \n",
    "- **Females** benefit from stronger recall (TPR) and overall accuracy.  \n",
    "- **Males** benefit from slightly higher precision and ROC AUC, as well as fewer false positives.  \n",
    "The disparities are **minor**, suggesting that the MLP model is **relatively fair across gender groups** without strong systematic bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "379dbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.9231\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9000\n",
      "  False Positive Rate (FPR): 0.0938\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_mlp == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_mlp == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fea8d1",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – MLP Model\n",
    "\n",
    "This section breaks down the classification performance of the MLP model across gender groups, using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Unprivileged (female = 0)    | 0.9231  | 0.1000  |\n",
    "| Privileged (male = 1)        | 0.9000  | 0.0938  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR)** is slightly higher for females (92.31%) compared to males (90.00%).  \n",
    "  - This means the model is **better at correctly identifying true positive cases for females**.  \n",
    "\n",
    "- **False Positive Rate (FPR)** is marginally higher for females (10.00%) than for males (9.38%).  \n",
    "  - This suggests females are **slightly more likely to receive false alarms** compared to males.  \n",
    "\n",
    "#### Implications\n",
    "\n",
    "- The MLP model shows a **mild trade-off across gender groups**:  \n",
    "  - **Females (unprivileged)**: benefit from better sensitivity (higher TPR) but face a small increase in false positives.  \n",
    "  - **Males (privileged)**: experience fewer false positives but at the cost of lower sensitivity.  \n",
    "\n",
    "- These asymmetries contribute to the fairness metrics (e.g., **Equal Odds Difference = 0.0231** and **Equal Odds Ratio = 1.0667**), which capture small but noticeable disparities in error rates.  \n",
    "\n",
    "#### Recommendation\n",
    "\n",
    "- While the differences are **minor overall**, the model tends to favor females in terms of sensitivity, while males receive slightly more reliable specificity.  \n",
    "- Depending on clinical priorities (avoiding missed cases vs. minimizing false alarms), these imbalances could be relevant in evaluating fairness.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
