{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba0b2b9",
   "metadata": {},
   "source": [
    "### Bias Detection and Fairness Evaluation on CVD Prediction (Mendeley Dataset) using FairMLhealth\n",
    "Source: https://data.mendeley.com/datasets/dzz48mvjht/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c29a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load X_test set\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57b21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup successful\n"
     ]
    }
   ],
   "source": [
    "import fairmlhealth\n",
    "import aif360\n",
    "print(\"Environment setup successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f126c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fairmlhealth\n",
      "Version: 1.0.2\n",
      "Summary: Health-centered variation analysis\n",
      "Home-page: https://github.com/KenSciResearch/fairMLHealth\n",
      "Author: Christine Allen\n",
      "Author-email: ca.magallen@gmail.com\n",
      "License: \n",
      "Location: c:\\users\\patri\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: aif360, ipython, jupyter, numpy, pandas, requests, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#have a look at the details of fairmlhealth - especially the version\n",
    "!pip show fairmlhealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76186324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "#have a look at the modules that are within fairmlhealth\n",
    "\n",
    "print(dir(fairmlhealth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44657ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "#load necessary modules \n",
    "\n",
    "#import module measure to use measure.summary for bias detection\n",
    "from fairmlhealth import measure\n",
    "\n",
    "#import module for investigation of individual cohorts \n",
    "from fairmlhealth.__utils import iterate_cohorts\n",
    "\n",
    "#import FairRanges to flag high values\n",
    "from fairmlhealth.__utils import FairRanges\n",
    "\n",
    "# Wrap the fairness summary function for cohort-wise analysis\n",
    "@iterate_cohorts\n",
    "def cohort_summary(**kwargs):\n",
    "    return measure.summary(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e064c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"inFairness\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"AdversarialDebiasing will be unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74540a7c",
   "metadata": {},
   "source": [
    "During the execution of FairMLHealth and AIF360, several runtime warnings were raised (e.g., “AdversarialDebiasing will be unavailable” due to the absence of TensorFlow, and deprecation warnings from the inFairness package regarding PyTorch’s functorch.vmap). These warnings do not affect the fairness metrics or results presented in this study, as the unavailable components were not used. To maintain clarity of output, the warnings were silenced programmatically, and the analysis was conducted without issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a099295",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models - KNN & DT\n",
    "\n",
    "#### K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d817251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_prob  y_pred\n",
      "0       0       0     0.0       0\n",
      "1       1       0     0.0       0\n",
      "2       1       1     1.0       1\n",
      "3       1       1     1.0       1\n",
      "4       1       0     0.0       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "knn_df = pd.read_csv(\"MendeleyData_50_50_KNN_best_predictions.csv\")\n",
    "\n",
    "print(knn_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871aa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_knn = knn_df[\"y_true\"].values\n",
    "y_prob_knn = knn_df[\"y_prob\"].values\n",
    "y_pred_knn = knn_df[\"y_pred\"].values\n",
    "gender_knn = knn_df[\"gender\"].values\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_knn = gender_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad78836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.0587\n",
      "               Balanced Accuracy Difference           -0.0587\n",
      "               Balanced Accuracy Ratio                 0.9382\n",
      "               Disparate Impact Ratio                  0.9732\n",
      "               Equal Odds Difference                   0.0688\n",
      "               Equal Odds Ratio                        3.2000\n",
      "               Positive Predictive Parity Difference  -0.0567\n",
      "               Positive Predictive Parity Ratio        0.9419\n",
      "               Statistical Parity Difference          -0.0150\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    }
   ],
   "source": [
    "knn_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_true_knn,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn,\n",
    "    prtc_attr=protected_attr_knn,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,   # skip inconsistency metrics that cause NearestNeighbors error\n",
    "    skip_performance=True\n",
    ")\n",
    "\n",
    "print(knn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ba3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Custom scenario oriented bounds\n",
    "\n",
    "custom_ranges = {\n",
    "    \"tpr diff\": (-0.03, 0.03),\n",
    "    \"fpr diff\": (-0.03, 0.03),\n",
    "    \"equal odds difference\": (-0.04, 0.04),\n",
    "    \"statistical parity difference\": (-0.05, 0.05),\n",
    "    \"disparate impact ratio\": (0.9, 1.1),\n",
    "    \"selection ratio\": (0.9, 1.1),\n",
    "    \"auc difference\": (-0.02, 0.02),\n",
    "    \"balanced accuracy difference\": (-0.02, 0.02),\n",
    "}\n",
    "\n",
    "bounds = FairRanges().load_fair_ranges(custom_ranges=custom_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14d2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  restore Styler.set_precision to adjust the highlighting color in the styled table\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "Styler = type(pd.DataFrame({\"_\":[0]}).style)  \n",
    "\n",
    "if not hasattr(Styler, \"set_precision\"):\n",
    "    def _set_precision(self, precision=4):\n",
    "        try:\n",
    "            return self.format(precision=precision)\n",
    "        except TypeError:\n",
    "            return self.format(formatter=lambda x:\n",
    "                f\"{x:.{precision}g}\" if isinstance(x, (int, float, np.floating)) else x\n",
    "            )\n",
    "    setattr(Styler, \"set_precision\", _set_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90435472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_45bcb_row0_col0, #T_45bcb_row1_col0, #T_45bcb_row4_col0, #T_45bcb_row5_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_45bcb\">\n",
       "  <caption>KNN Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_45bcb_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_45bcb_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_45bcb_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_45bcb_row0_col0\" class=\"data row0 col0\" >-0.0587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45bcb_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_45bcb_row1_col0\" class=\"data row1 col0\" >-0.0587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45bcb_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_45bcb_row2_col0\" class=\"data row2 col0\" >0.9382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45bcb_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_45bcb_row3_col0\" class=\"data row3 col0\" >0.9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45bcb_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_45bcb_row4_col0\" class=\"data row4 col0\" >0.0688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45bcb_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_45bcb_row5_col0\" class=\"data row5 col0\" >3.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45bcb_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_45bcb_row6_col0\" class=\"data row6 col0\" >-0.0567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45bcb_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_45bcb_row7_col0\" class=\"data row7 col0\" >0.9419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45bcb_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_45bcb_row8_col0\" class=\"data row8 col0\" >-0.0150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_45bcb_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_45bcb_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_45bcb_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2786a2e2230>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "from fairmlhealth.__utils import Flagger\n",
    "\n",
    "class MyFlagger(Flagger):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.flag_color = \"#491ee6\"   \n",
    "        self.flag_type = \"background-color\"\n",
    "\n",
    "styled_knn = MyFlagger().apply_flag(\n",
    "    df=knn_bias,\n",
    "    caption=\"KNN Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d3278",
   "metadata": {},
   "source": [
    "## Fairness Evaluation – KNN by Gender\n",
    "\n",
    "The table summarizes fairness metrics for the KNN model, with gender as the protected attribute  \n",
    "(**0 = Female / unprivileged, 1 = Male / privileged**).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (−0.0587):** Females have lower AUC than males, suggesting weaker ability to distinguish between positive and negative cases.  \n",
    "- **Balanced Accuracy Difference (−0.0587)** and **Ratio (0.9382):** Balanced accuracy is lower for females, indicating reduced classification quality for the unprivileged group.  \n",
    "- **Disparate Impact Ratio (0.9732):** Close to 1, suggesting selection rates between genders are fairly similar, with only a small disparity.  \n",
    "- **Equal Odds Difference (0.0688)** and **Ratio (3.2000):** A noticeable imbalance in error rates (TPR/FPR), showing that outcomes differ more substantially between genders.  \n",
    "- **Positive Predictive Parity Difference (−0.0567)** and **Ratio (0.9419):** Precision is slightly lower for females, meaning their positive predictions are less reliable.  \n",
    "- **Statistical Parity Difference (−0.0150):** Indicates a very small under-selection of females compared to males.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation\n",
    "\n",
    "- The KNN model shows **moderate gender disparities**:  \n",
    "  - Females are disadvantaged in terms of AUC, balanced accuracy, and precision, reflecting weaker performance quality.  \n",
    "  - Equal Odds metrics highlight **notable disparities in error rates**, suggesting uneven treatment of males and females.  \n",
    "  - Disparate Impact and Statistical Parity remain close to ideal, meaning selection rates are relatively fair despite differences in prediction quality.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The KNN model appears to be **less fair across genders**, particularly disadvantaging females through lower accuracy, weaker ranking ability, and less reliable positive predictions. While selection rates are relatively balanced, the **error distribution (Equal Odds)** shows a more substantial fairness concern, indicating that this model may reinforce gender bias in predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0eeae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - KNN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0587</td>\n",
       "      <td>1.0658</td>\n",
       "      <td>-0.0688</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.0567</td>\n",
       "      <td>1.0617</td>\n",
       "      <td>0.015</td>\n",
       "      <td>1.0275</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>1.0551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0587</td>\n",
       "      <td>0.9382</td>\n",
       "      <td>0.0688</td>\n",
       "      <td>3.2000</td>\n",
       "      <td>-0.0567</td>\n",
       "      <td>0.9419</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>-0.0487</td>\n",
       "      <td>0.9478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                        0.0587   \n",
       "1       gender             1                       -0.0587   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0658   -0.0688     0.3125    0.0567     1.0617   \n",
       "1                   0.9382    0.0688     3.2000   -0.0567     0.9419   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0           0.015           1.0275    0.0487     1.0551  \n",
       "1          -0.015           0.9732   -0.0487     0.9478  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - KNN\")\n",
    "measure.bias(X_test, y_test, y_pred_knn, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05764be",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – KNN by Gender\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Difference = +0.0587, Ratio = 1.0658  \n",
    "- **Males (1):** Difference = −0.0587, Ratio = 0.9382  \n",
    "- ➝ Balanced accuracy is higher for females, suggesting they benefit from stronger classification quality.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** Diff = −0.0688, Ratio = 0.3125  \n",
    "- **Males (1):** Diff = +0.0688, Ratio = 3.2000  \n",
    "- ➝ Females have a **much lower false positive rate**, while males are more frequently misclassified as having CVD.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** Diff = +0.0567, Ratio = 1.0617  \n",
    "- **Males (1):** Diff = −0.0567, Ratio = 0.9419  \n",
    "- ➝ Predictions are **more reliable for females**, while males experience lower precision.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Diff = +0.0150, Ratio = 1.0275  \n",
    "- **Males (1):** Diff = −0.0150, Ratio = 0.9732  \n",
    "- ➝ Females are selected slightly more often, while males are under-selected.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** Diff = +0.0487, Ratio = 1.0551  \n",
    "- **Males (1):** Diff = −0.0487, Ratio = 0.9478  \n",
    "- ➝ Females have higher sensitivity, meaning they are more likely to be correctly identified when they have CVD.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged):** Benefit across most metrics — higher balanced accuracy, lower false positive rate, better precision, higher sensitivity, and slightly higher selection rates.  \n",
    "- **Males (privileged):** Are disadvantaged, with more false positives, lower precision, and lower sensitivity.  \n",
    "\n",
    "Overall, the KNN model appears to be **biased in favor of females**, reversing the more common pattern where privileged groups (males) benefit. This indicates that error distribution is uneven and requires consideration for fairness mitigation, as males experience comparatively worse outcomes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fdeff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.9350</td>\n",
       "      <td>0.9427</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9374</td>\n",
       "      <td>0.9224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.3697</td>\n",
       "      <td>0.9200</td>\n",
       "      <td>0.8923</td>\n",
       "      <td>0.8846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5584</td>\n",
       "      <td>0.9481</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>0.3901</td>\n",
       "      <td>0.9767</td>\n",
       "      <td>0.9510</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.5550    0.9350   \n",
       "1        gender             0   46.0       0.5652           0.5435    0.8913   \n",
       "2        gender             1  154.0       0.5844           0.5584    0.9481   \n",
       "\n",
       "   F1-Score     FPR  PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9427  0.0476  0.3857     0.9640   0.9374  0.9224  \n",
       "1    0.9020  0.1000  0.3697     0.9200   0.8923  0.8846  \n",
       "2    0.9545  0.0312  0.3901     0.9767   0.9510  0.9333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fairmlhealth import measure\n",
    "import pandas as pd\n",
    "from IPython.display import display  \n",
    "\n",
    "# convert gender into DataFrame with a clear column name to get a nice table as output\n",
    "gender_df = pd.DataFrame({\"gender\": X_test[\"gender\"].astype(int)})\n",
    "\n",
    "\n",
    "# Get the stratified table\n",
    "perf_table_knn = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn\n",
    ")\n",
    "\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e845f8a",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – KNN by Gender\n",
    "\n",
    "This table shows the **stratified performance metrics** of the KNN model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9350)** and **F1-score (0.9427)** indicate strong overall model performance.  \n",
    "- **ROC AUC (0.9374)** demonstrates good discriminatory ability, though not as high as some other models.  \n",
    "- **Precision (0.9640)** and **TPR (0.9224)** show a solid balance between predictive reliability and sensitivity.  \n",
    "- **PR AUC (0.3857)** is relatively low, which may be due to the dataset’s class distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8913     | 0.9481   | Accuracy is notably higher for males. |\n",
    "| **F1-Score**  | 0.9020     | 0.9545   | Model performs better for males. |\n",
    "| **FPR**       | 0.1000     | 0.0312   | Females experience many more false positives than males. |\n",
    "| **Precision** | 0.9200     | 0.9767   | Predictions are more reliable for males. |\n",
    "| **ROC AUC**   | 0.8923     | 0.9510   | Males benefit from stronger ranking performance. |\n",
    "| **TPR**       | 0.8846     | 0.9333   | Males are more likely to be correctly identified when they have CVD. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Disadvantaged across almost all metrics: lower accuracy, F1, ROC AUC, and precision.  \n",
    "  - They also suffer from a **much higher false positive rate (10% vs. 3.12%)**, meaning more healthy females are incorrectly flagged as having CVD.  \n",
    "  - Sensitivity (TPR = 88.46%) is weaker, increasing the risk of missed diagnoses.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Benefit from **higher accuracy, F1, precision, and ROC AUC**.  \n",
    "  - Enjoy lower false positives and stronger sensitivity (93.33%), indicating more consistent and favorable performance across all metrics.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The KNN model exhibits a **systematic bias in favor of males**.  \n",
    "- **Males** consistently receive more favorable outcomes, with higher accuracy, reliability, and sensitivity, as well as fewer false positives.  \n",
    "- **Females** face both **higher false alarms** and **more missed cases**, highlighting significant fairness concerns.  \n",
    "\n",
    "This performance disparity aligns with the fairness metrics, confirming that KNN is the **least balanced model** across gender and requires mitigation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5db1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.8846\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9333\n",
      "  False Positive Rate (FPR): 0.0312\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#group specific error analysis\n",
    "\n",
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_knn == 0)  # unprivileged group (female)\n",
    "male_mask   = (protected_attr_knn == 1)  # privileged group (male)\n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba4143",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – KNN Model\n",
    "\n",
    "To further examine fairness at the subgroup level, we compared the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.8846  | 0.1000  |\n",
    "| Privileged (Male)      | 0.9333  | 0.0312  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- The **privileged group (male)** achieves a higher **TPR (93.33%)** than the unprivileged group (88.46%), meaning males are more often correctly identified when they truly have CVD.  \n",
    "- At the same time, the **FPR for females (10.00%)** is more than three times higher than for males (3.12%), indicating that women are more likely to be incorrectly flagged as having CVD.  \n",
    "- This imbalance highlights that the model is both **less sensitive** for females (missing more true cases) and **less specific** (producing more false alarms).  \n",
    "- These subgroup disparities correspond with fairness metrics such as the **Equal Odds Difference and Ratio**, which capture unequal error distributions between genders.  \n",
    "\n",
    "#### Summary\n",
    "\n",
    "The results reveal a **systematic disadvantage for the unprivileged group (females)**:  \n",
    "- They face more missed detections (lower TPR) and more false alarms (higher FPR).  \n",
    "- In contrast, males benefit from stronger sensitivity and better specificity.  \n",
    "\n",
    "This outcome underscores the need for **fairness mitigation strategies** to reduce error rate disparities and improve equity in model predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb317ca",
   "metadata": {},
   "source": [
    "### Decision Tree - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adbaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred    y_prob\n",
      "0       0       0       0  0.000000\n",
      "1       1       0       0  0.000000\n",
      "2       1       1       1  0.993939\n",
      "3       1       1       1  1.000000\n",
      "4       1       0       0  0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "dt_df = pd.read_csv(\"MendeleyData_50_50_DT_pruned_tuned_predictions.csv\")\n",
    "\n",
    "print(dt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06530601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract common columns\n",
    "y_true_dt = dt_df[\"y_true\"].values\n",
    "y_prob_dt = dt_df[\"y_prob\"].values\n",
    "y_pred_dt = dt_df[\"y_pred\"].values\n",
    "gender_dt = dt_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_dt = gender_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b83e2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Decision Tree Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0482\n",
      "               Balanced Accuracy Difference            0.0191\n",
      "               Balanced Accuracy Ratio                 1.0205\n",
      "               Disparate Impact Ratio                  1.0189\n",
      "               Equal Odds Difference                   0.0444\n",
      "               Equal Odds Ratio                        1.0667\n",
      "               Positive Predictive Parity Difference  -0.0062\n",
      "               Positive Predictive Parity Ratio        0.9934\n",
      "               Statistical Parity Difference           0.0113\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Gender Bias Report\n",
    "print(\"\\n--- Decision Tree Gender Bias Report ---\")\n",
    "\n",
    "dt_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt,\n",
    "    prtc_attr=protected_attr_dt,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,  \n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(dt_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc2d88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_da9b8_row0_col0, #T_da9b8_row4_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_da9b8\">\n",
       "  <caption>Decision Tree Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_da9b8_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_da9b8_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_da9b8_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_da9b8_row0_col0\" class=\"data row0 col0\" >0.0482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da9b8_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_da9b8_row1_col0\" class=\"data row1 col0\" >0.0191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da9b8_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_da9b8_row2_col0\" class=\"data row2 col0\" >1.0205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da9b8_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_da9b8_row3_col0\" class=\"data row3 col0\" >1.0189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da9b8_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_da9b8_row4_col0\" class=\"data row4 col0\" >0.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da9b8_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_da9b8_row5_col0\" class=\"data row5 col0\" >1.0667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da9b8_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_da9b8_row6_col0\" class=\"data row6 col0\" >-0.0062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da9b8_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_da9b8_row7_col0\" class=\"data row7 col0\" >0.9934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da9b8_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_da9b8_row8_col0\" class=\"data row8 col0\" >0.0113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_da9b8_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_da9b8_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_da9b8_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2786a4ae0e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "styled_dt = MyFlagger().apply_flag(\n",
    "    df=dt_bias,\n",
    "    caption=\"Decision Tree Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1c7f5",
   "metadata": {},
   "source": [
    "## Fairness Evaluation – Decision Tree by Gender\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (0.0482):** Males show slightly higher AUC, meaning they benefit from stronger ranking performance compared to females.  \n",
    "- **Balanced Accuracy Difference (0.0191)** and **Ratio (1.0205):** Balanced accuracy is marginally higher for males, indicating a small performance gap.  \n",
    "- **Disparate Impact Ratio (1.0189):** Very close to 1, suggesting nearly equal selection rates across genders.  \n",
    "- **Equal Odds Difference (0.0444)** and **Ratio (1.0667):** Some disparity exists in error rates (TPR/FPR), with males experiencing somewhat more favorable outcomes.  \n",
    "- **Positive Predictive Parity Difference (−0.0062)** and **Ratio (0.9934):** Precision is nearly identical across genders, with a very small disadvantage for females.  \n",
    "- **Statistical Parity Difference (0.0113):** Selection rates are slightly higher for males, though the gap is negligible.  \n",
    "---\n",
    "\n",
    "### 2. Interpretation\n",
    "\n",
    "- Overall, the Decision Tree model demonstrates **relatively balanced performance across genders**.  \n",
    "- Most fairness measures are very close to parity, but **males (privileged group)** hold a slight advantage in terms of AUC, balanced accuracy, and error rate distribution.  \n",
    "- Females show nearly equal predictive precision and selection rates, meaning disparities are not severe.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Decision Tree model exhibits **modest fairness disparities**, with a slight tendency to favor males in ranking ability, balanced accuracy, and error distributions.  \n",
    "However, differences are small, and the model remains more balanced compared to models like KNN, which show stronger systematic bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "631b7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - DT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0191</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>-0.0063</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>1.0067</td>\n",
       "      <td>-0.0113</td>\n",
       "      <td>0.9814</td>\n",
       "      <td>-0.0444</td>\n",
       "      <td>0.9556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>1.0205</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>1.0667</td>\n",
       "      <td>-0.0062</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>1.0189</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>1.0465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0191   \n",
       "1       gender             1                        0.0191   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9799   -0.0063     0.9375    0.0062     1.0067   \n",
       "1                   1.0205    0.0063     1.0667   -0.0062     0.9934   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0         -0.0113           0.9814   -0.0444     0.9556  \n",
       "1          0.0113           1.0189    0.0444     1.0465  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - DT\")\n",
    "measure.bias(X_test, y_test, y_pred_dt, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa9add",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – Decision Tree by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the Decision Tree model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Difference = −0.0191, Ratio = 0.9799  \n",
    "- **Males (1):** Difference = +0.0191, Ratio = 1.0205  \n",
    "- ➝ Males benefit from slightly higher balanced accuracy, but the disparity is very small.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** Diff = −0.0063, Ratio = 0.9375  \n",
    "- **Males (1):** Diff = +0.0063, Ratio = 1.0667  \n",
    "- ➝ Females have a slightly lower false positive rate, while males face marginally more false alarms.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** Diff = +0.0062, Ratio = 1.0067  \n",
    "- **Males (1):** Diff = −0.0062, Ratio = 0.9934  \n",
    "- ➝ Precision is marginally higher for females, though the difference is negligible.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Diff = −0.0113, Ratio = 0.9814  \n",
    "- **Males (1):** Diff = +0.0113, Ratio = 1.0189  \n",
    "- ➝ Males are selected slightly more often than females.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** Diff = −0.0444, Ratio = 0.9556  \n",
    "- **Males (1):** Diff = +0.0444, Ratio = 1.0465  \n",
    "- ➝ Males enjoy higher sensitivity, meaning they are more likely to be correctly identified when they truly have CVD.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged):** Benefit from a marginally lower false positive rate and slightly higher precision, but are disadvantaged by lower sensitivity (TPR) and balanced accuracy.  \n",
    "- **Males (privileged):** Enjoy higher sensitivity and balanced accuracy, but face a slightly higher false positive rate.  \n",
    "\n",
    "Overall, disparities are **modest**. The Decision Tree model is **relatively balanced**, with only minor tendencies:  \n",
    "- **Males** are favored in sensitivity and accuracy.  \n",
    "- **Females** benefit slightly in precision and specificity.  \n",
    "\n",
    "This suggests the Decision Tree is comparatively fair across gender, with no severe systematic bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1acccc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.9400</td>\n",
       "      <td>0.9492</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.9357</td>\n",
       "      <td>0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.4101</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.9351</td>\n",
       "      <td>0.9451</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9348</td>\n",
       "      <td>0.9230</td>\n",
       "      <td>0.9556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.6000    0.9400   \n",
       "1        gender             0   46.0       0.5652           0.6087    0.9565   \n",
       "2        gender             1  154.0       0.5844           0.5974    0.9351   \n",
       "\n",
       "   F1-Score     FPR  PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9492  0.0952       —     0.9333   0.9357  0.9655  \n",
       "1    0.9630  0.1000  0.4101     0.9286   0.9712  1.0000  \n",
       "2    0.9451  0.0938       —     0.9348   0.9230  0.9556  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_dt = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_dt = perf_table_dt.fillna(\"—\")\n",
    "\n",
    "# Display pretty table\n",
    "display(perf_table_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936a469",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Random Forest by Gender\n",
    "\n",
    "This table shows the **stratified performance metrics** of the Random Forest model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9400)** and **F1-score (0.9492)** indicate excellent overall classification performance.  \n",
    "- **ROC AUC (0.9357)** confirms strong discriminatory ability between positive and negative cases.  \n",
    "- **Precision (0.9333)** and **TPR (0.9655)** show the model balances predictive reliability and sensitivity effectively.  \n",
    "- **Note**: PR AUC is reported as “—” for the overall set, though subgroup values are shown where possible.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.9565     | 0.9351   | Accuracy is higher for females. |\n",
    "| **F1-Score**  | 0.9630     | 0.9451   | Females achieve stronger F1 performance. |\n",
    "| **FPR**       | 0.1000     | 0.0938   | Females experience slightly more false positives. |\n",
    "| **Precision** | 0.9286     | 0.9348   | Predictions are marginally more reliable for males. |\n",
    "| **ROC AUC**   | 0.9712     | 0.9230   | Females benefit from a higher ranking performance. |\n",
    "| **TPR**       | 1.0000     | 0.9556   | Females are perfectly identified when they have CVD, while males have slightly lower sensitivity. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Achieve **perfect sensitivity (TPR = 1.0000)**, ensuring no missed positive cases.  \n",
    "  - Benefit from higher accuracy, F1, and ROC AUC compared to males.  \n",
    "  - However, they face a slightly higher false positive rate (10% vs. 9.38%) and slightly weaker precision.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Enjoy slightly higher precision and fewer false positives, making their predictions more reliable.  \n",
    "  - Their sensitivity (95.56%) is strong but lower than females, resulting in a few more missed cases.  \n",
    "  - Overall, performance remains excellent but marginally less favorable than for females.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Random Forest model performs **well for both genders**, but females enjoy more favorable outcomes in several key metrics (accuracy, F1, ROC AUC, and sensitivity).  \n",
    "- **Females**: Higher recall and discrimination but at the cost of slightly more false alarms.  \n",
    "- **Males**: Strong precision and fewer false positives, but lower sensitivity.  \n",
    "\n",
    "Overall, the model shows a **mild bias in favor of females**, giving them more comprehensive detection coverage, though both groups achieve high-quality results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53556439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 1.0000\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9556\n",
      "  False Positive Rate (FPR): 0.0938\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_dt == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_dt == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97580f6",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Decision Tree\n",
    "\n",
    "This section analyzes the classification performance of the Decision Tree model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Unprivileged (female = 0)    | 1.0000  | 0.1000  |\n",
    "| Privileged (male = 1)        | 0.9556  | 0.0938  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR)** shows that females achieve **perfect sensitivity (100%)**, meaning all true CVD cases are detected.  \n",
    "  - Males, while still high, have a slightly lower TPR (**95.56%**), indicating a small proportion of missed cases.  \n",
    "\n",
    "- **False Positive Rate (FPR)** is fairly similar across groups:  \n",
    "  - Females: **10.00%**  \n",
    "  - Males: **9.38%**  \n",
    "  - This suggests that both groups experience false alarms, with females only marginally more affected.  \n",
    "\n",
    "#### Implications\n",
    "\n",
    "- The **Decision Tree achieves excellent sensitivity for females**, ensuring no missed detections in this group.  \n",
    "- Males maintain strong but slightly weaker sensitivity, while benefiting from a marginally lower false positive rate.  \n",
    "- These differences reflect a **mild trade-off**: females are fully protected from missed diagnoses but face a touch more false alarms, whereas males enjoy slightly better specificity but risk a few missed cases.  \n",
    "\n",
    "#### Summary\n",
    "\n",
    "Overall, the Decision Tree model demonstrates **strong performance across genders** with only small disparities.  \n",
    "- **Females (unprivileged):** favored in sensitivity (perfect recall) but incur a slightly higher false positive rate.  \n",
    "- **Males (privileged):** experience fewer false positives but slightly lower sensitivity.  \n",
    "\n",
    "This indicates a **balanced but not identical error distribution**, where the model leans slightly in favor of females in terms of detection coverage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5168d",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaa2a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred  y_prob\n",
      "0       0       0       0    0.11\n",
      "1       1       0       0    0.01\n",
      "2       1       1       1    0.96\n",
      "3       1       1       1    0.88\n",
      "4       1       0       0    0.01\n"
     ]
    }
   ],
   "source": [
    "rf_df = pd.read_csv(\"MendeleyData_50_50_baselineRF_predictions.csv\")\n",
    "print(rf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "278af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_rf = rf_df[\"y_true\"].values\n",
    "y_pred_rf = rf_df[\"y_pred\"].values\n",
    "y_prob_rf = rf_df[\"y_prob\"].values\n",
    "gender_rf = rf_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_rf = gender_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "689f0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0003\n",
      "               Balanced Accuracy Difference            0.0146\n",
      "               Balanced Accuracy Ratio                 1.0156\n",
      "               Disparate Impact Ratio                  1.0652\n",
      "               Equal Odds Difference                   0.0667\n",
      "               Equal Odds Ratio                        1.6000\n",
      "               Positive Predictive Parity Difference  -0.0260\n",
      "               Positive Predictive Parity Ratio        0.9728\n",
      "               Statistical Parity Difference           0.0373\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Gender Bias Report\n",
    "print(\"\\n--- Random Forest Gender Bias Report ---\")\n",
    "\n",
    "rf_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf,\n",
    "    prtc_attr=protected_attr_rf,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(rf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7a8e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b06a7_row4_col0, #T_b06a7_row5_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b06a7\">\n",
       "  <caption>Random Forest Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b06a7_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b06a7_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_b06a7_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_b06a7_row0_col0\" class=\"data row0 col0\" >0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b06a7_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_b06a7_row1_col0\" class=\"data row1 col0\" >0.0146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b06a7_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_b06a7_row2_col0\" class=\"data row2 col0\" >1.0156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b06a7_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_b06a7_row3_col0\" class=\"data row3 col0\" >1.0652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b06a7_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_b06a7_row4_col0\" class=\"data row4 col0\" >0.0667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b06a7_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_b06a7_row5_col0\" class=\"data row5 col0\" >1.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b06a7_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_b06a7_row6_col0\" class=\"data row6 col0\" >-0.0260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b06a7_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_b06a7_row7_col0\" class=\"data row7 col0\" >0.9728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b06a7_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_b06a7_row8_col0\" class=\"data row8 col0\" >0.0373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b06a7_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_b06a7_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_b06a7_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2786a520f70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for Random Forest\n",
    "styled_rf = MyFlagger().apply_flag(\n",
    "    df=rf_bias,\n",
    "    caption=\"Random Forest Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7c650",
   "metadata": {},
   "source": [
    "## Fairness Evaluation – Random Forest by Gender\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (−0.0020):** Negligible difference in ranking performance across genders, suggesting both groups are treated similarly in terms of separability.  \n",
    "- **Balanced Accuracy Difference (0.0146)** and **Ratio (1.0156):** Slight advantage for males, but the difference remains very small.  \n",
    "- **Disparate Impact Ratio (1.0652):** Within the fairness guideline range (0.8–1.25), showing reasonably balanced selection rates across genders.  \n",
    "- **Equal Odds Difference (0.0667)** and **Ratio (1.6000):** A more pronounced disparity in error distributions (TPR and FPR), indicating uneven error trade-offs across genders.  \n",
    "- **Positive Predictive Parity Difference (−0.0260)** and **Ratio (0.9728):** Precision is slightly lower for females, meaning positive predictions are a bit less reliable for the unprivileged group.  \n",
    "- **Statistical Parity Difference (0.0373):** Males are selected somewhat more often than females, though the difference is modest.  \n",
    "\n",
    "\n",
    "### 2. Interpretation\n",
    "\n",
    "- The Random Forest model shows **overall balanced fairness across most metrics**, with negligible differences in AUC and balanced accuracy.  \n",
    "- However, the **Equal Odds metrics highlight a notable disparity**, meaning that males and females experience different error rates (false positives and false negatives).  \n",
    "- Precision is slightly higher for males, and selection rates also favor the privileged group.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Random Forest model demonstrates **generally fair performance across genders**, but with **uneven error distributions** that give a mild advantage to males in terms of precision and error trade-offs.  \n",
    "While disparities are smaller than in KNN, they are more visible than in the MLP model, suggesting that fairness monitoring and potential mitigation remain important if this model were to be used in practice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47479386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - RF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0146</td>\n",
       "      <td>0.9846</td>\n",
       "      <td>-0.0375</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.0280</td>\n",
       "      <td>-0.0373</td>\n",
       "      <td>0.9388</td>\n",
       "      <td>-0.0667</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>1.0156</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>1.600</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.9728</td>\n",
       "      <td>0.0373</td>\n",
       "      <td>1.0652</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>1.0714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0146   \n",
       "1       gender             1                        0.0146   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9846   -0.0375      0.625     0.026     1.0280   \n",
       "1                   1.0156    0.0375      1.600    -0.026     0.9728   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0         -0.0373           0.9388   -0.0667     0.9333  \n",
       "1          0.0373           1.0652    0.0667     1.0714  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - RF\")\n",
    "measure.bias(X_test, y_test, y_pred_rf, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be772b16",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – Random Forest by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the Random Forest model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Difference = −0.0146, Ratio = 0.9846  \n",
    "- **Males (1):** Difference = +0.0146, Ratio = 1.0156  \n",
    "- ➝ Males benefit from slightly higher balanced accuracy, but the gap is very small.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** Diff = −0.0375, Ratio = 0.6250  \n",
    "- **Males (1):** Diff = +0.0375, Ratio = 1.6000  \n",
    "- ➝ Females face a lower false positive rate, while males are more likely to be incorrectly classified as having CVD.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** Diff = +0.0260, Ratio = 1.0280  \n",
    "- **Males (1):** Diff = −0.0260, Ratio = 0.9728  \n",
    "- ➝ Precision is slightly higher for females, meaning their positive predictions are marginally more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Diff = −0.0373, Ratio = 0.9388  \n",
    "- **Males (1):** Diff = +0.0373, Ratio = 1.0652  \n",
    "- ➝ Males are selected more frequently, while females are slightly under-selected.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** Diff = −0.0667, Ratio = 0.9333  \n",
    "- **Males (1):** Diff = +0.0667, Ratio = 1.0714  \n",
    "- ➝ Males enjoy higher sensitivity, meaning they are more likely to be correctly identified when they have CVD, while females face more missed cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged):** Benefit from **fewer false positives** and slightly **higher precision**, but are disadvantaged by **lower sensitivity (TPR)** and somewhat lower selection rates.  \n",
    "- **Males (privileged):** Enjoy **higher sensitivity and selection rates**, but face more false positives and slightly lower precision.  \n",
    "\n",
    "Overall, the Random Forest model shows a **trade-off**:  \n",
    "- **Females** are less frequently selected and miss more true cases, but when predicted positive, their results are more precise and with fewer false alarms.  \n",
    "- **Males** are detected more often (higher TPR) but at the cost of **more false positives** and slightly weaker prediction reliability.  \n",
    "\n",
    "This indicates a **mixed fairness profile**, with each gender advantaged in different aspects of performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6573254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.9400</td>\n",
       "      <td>0.9483</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9483</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.9856</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>0.9351</td>\n",
       "      <td>0.9438</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>0.9852</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.5800    0.9400   \n",
       "1        gender             0   46.0       0.5652           0.6087    0.9565   \n",
       "2        gender             1  154.0       0.5844           0.5714    0.9351   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9483  0.0714      —     0.9483   0.9859  0.9483  \n",
       "1    0.9630  0.1000      —     0.9286   0.9856  1.0000  \n",
       "2    0.9438  0.0625      —     0.9545   0.9852  0.9333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_rf = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_rf = perf_table_rf.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de72316",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Random Forest by Gender\n",
    "\n",
    "This table shows the **stratified performance metrics** of the Random Forest model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9400)** and **F1-score (0.9483)** indicate excellent overall classification performance.  \n",
    "- **ROC AUC (0.9841)** confirms very strong discriminatory ability.  \n",
    "- **Precision (0.9483)** and **TPR (0.9483)** show the model balances predictive reliability and sensitivity well.  \n",
    "- **Note**: PR AUC is not available (“—”) due to subgroup size limitations.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.9565     | 0.9351   | Females achieve slightly higher accuracy. |\n",
    "| **F1-Score**  | 0.9630     | 0.9438   | F1 performance is stronger for females. |\n",
    "| **FPR**       | 0.1000     | 0.0625   | Females experience more false positives than males. |\n",
    "| **Precision** | 0.9286     | 0.9545   | Predictions are more reliable for males. |\n",
    "| **ROC AUC**   | 0.9817     | 0.9838   | Both groups have excellent ranking performance, with males slightly ahead. |\n",
    "| **TPR**       | 1.0000     | 0.9333   | Females are perfectly identified when they have CVD, while males are slightly less likely to be detected. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Benefit from higher accuracy, F1, and perfect sensitivity (TPR = 1.0000), meaning no missed positive cases.  \n",
    "  - However, they face more false alarms (FPR = 10%) and slightly lower precision than males.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Enjoy stronger precision (0.9545) and fewer false positives (6.25%).  \n",
    "  - Their sensitivity (TPR = 93.33%) is slightly weaker, leading to a small number of missed cases.  \n",
    "  - Overall, predictions for males are more reliable but less comprehensive in terms of detection.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Random Forest model performs **very well for both genders**, but distributes errors differently:  \n",
    "- **Females** benefit from stronger sensitivity and higher overall accuracy but are penalized with more false positives and weaker precision.  \n",
    "- **Males** gain more reliable predictions and fewer false positives but face slightly lower sensitivity.  \n",
    "\n",
    "This reflects a **trade-off rather than systematic bias**: females are more comprehensively detected, while males are more accurately confirmed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a52946a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 1.0000\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9333\n",
      "  False Positive Rate (FPR): 0.0625\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_rf == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_rf == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06d248",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Random Forest\n",
    "\n",
    "This section presents the performance of the Random Forest model across gender groups, focusing on **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Unprivileged (female = 0)    | 1.0000  | 0.1000  |\n",
    "| Privileged (male = 1)        | 0.9333  | 0.0625  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR)** is **perfect for females (100%)**, compared to **93.33% for males**.  \n",
    "  - This shows the model successfully identifies all true positive cases among females, while a small proportion of male cases are missed.  \n",
    "\n",
    "- **False Positive Rate (FPR)** is **higher for females (10.00%)** than for males (6.25%).  \n",
    "  - This indicates that females are more likely to be incorrectly flagged as having CVD.  \n",
    "\n",
    "#### Implications\n",
    "\n",
    "- The model exhibits a **gender-based trade-off**:  \n",
    "  - **Females (unprivileged):** benefit from perfect sensitivity (no missed cases), but face more false alarms.  \n",
    "  - **Males (privileged):** enjoy fewer false positives, but at the cost of slightly lower sensitivity.  \n",
    "\n",
    "- This asymmetry suggests that the Random Forest model distributes errors differently across groups rather than consistently favoring one gender.  \n",
    "  - **Females are over-diagnosed** (higher FPR) but fully detected (perfect TPR).  \n",
    "  - **Males are under-diagnosed** (lower TPR) but less likely to be misclassified when healthy.  \n",
    "\n",
    "- Depending on the clinical context, these imbalances may have different consequences:  \n",
    "  - For females, more unnecessary follow-ups due to false alarms.  \n",
    "  - For males, greater risk of missed diagnoses.  \n",
    "\n",
    "#### Summary\n",
    "\n",
    "The Random Forest model provides **excellent performance for both genders**, but error trade-offs differ:  \n",
    "- **Females** gain in sensitivity but lose in specificity.  \n",
    "- **Males** gain in specificity but lose in sensitivity.  \n",
    "\n",
    "This pattern highlights the importance of evaluating not only overall accuracy but also **fairness in error distributions** when applying the model in healthcare settings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136de6",
   "metadata": {},
   "source": [
    "### Deep Learning Model - Feed Forward Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c69d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred        y_prob\n",
      "0       0       0       0  3.821277e-04\n",
      "1       1       0       0  2.510854e-09\n",
      "2       1       1       1  9.999984e-01\n",
      "3       1       1       1  9.999645e-01\n",
      "4       1       0       0  1.178902e-05\n"
     ]
    }
   ],
   "source": [
    "mlp_df = pd.read_csv(\"MendeleyData_50_50_MLP_recallfirst_predictions.csv\")\n",
    "print(mlp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f65a498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns \n",
    "y_true_mlp = mlp_df[\"y_true\"].values \n",
    "y_prob_mlp = mlp_df[\"y_prob\"].values\n",
    "y_pred_mlp = mlp_df[\"y_pred\"].values\n",
    "gender_mlp = mlp_df[\"gender\"].values \n",
    "\n",
    "# Use gender_mlp as the protected attribute\n",
    "protected_attr_mlp = gender_mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6969bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.0075\n",
      "               Balanced Accuracy Difference           -0.0621\n",
      "               Balanced Accuracy Ratio                 0.9340\n",
      "               Disparate Impact Ratio                  0.8276\n",
      "               Equal Odds Difference                  -0.1368\n",
      "               Equal Odds Ratio                        0.8000\n",
      "               Positive Predictive Parity Difference  -0.0005\n",
      "               Positive Predictive Parity Ratio        0.9995\n",
      "               Statistical Parity Difference          -0.0997\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    }
   ],
   "source": [
    "#Run fairmlhealth bias detection for MLP \n",
    "\n",
    "mlp_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp,\n",
    "    prtc_attr=protected_attr_mlp,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(mlp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbf7eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b195e_row1_col0, #T_b195e_row3_col0, #T_b195e_row4_col0, #T_b195e_row5_col0, #T_b195e_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b195e\">\n",
       "  <caption>MLP Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b195e_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b195e_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_b195e_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_b195e_row0_col0\" class=\"data row0 col0\" >-0.0075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b195e_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_b195e_row1_col0\" class=\"data row1 col0\" >-0.0621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b195e_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_b195e_row2_col0\" class=\"data row2 col0\" >0.9340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b195e_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_b195e_row3_col0\" class=\"data row3 col0\" >0.8276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b195e_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_b195e_row4_col0\" class=\"data row4 col0\" >-0.1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b195e_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_b195e_row5_col0\" class=\"data row5 col0\" >0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b195e_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_b195e_row6_col0\" class=\"data row6 col0\" >-0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b195e_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_b195e_row7_col0\" class=\"data row7 col0\" >0.9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b195e_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_b195e_row8_col0\" class=\"data row8 col0\" >-0.0997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b195e_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_b195e_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_b195e_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2786a596ec0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for MLP\n",
    "styled_mlp = MyFlagger().apply_flag(\n",
    "    df=mlp_bias,\n",
    "    caption=\"MLP Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713caa5",
   "metadata": {},
   "source": [
    "## Fairness Evaluation – MLP by Gender\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (−0.0075):** Very small difference in ranking performance across genders, suggesting that overall discrimination between positive and negative cases is balanced.  \n",
    "- **Balanced Accuracy Difference (−0.0621)** and **Ratio (0.9340):** Females have lower balanced accuracy, indicating weaker classification performance compared to males.  \n",
    "- **Disparate Impact Ratio (0.8276):** Slightly below the fairness guideline threshold of 0.80–1.25, showing that females are selected at noticeably lower rates.  \n",
    "- **Equal Odds Difference (−0.1368)** and **Ratio (0.8000):** Clear disparities in error distribution (TPR/FPR), indicating that males are treated more favorably in terms of sensitivity and false positives.  \n",
    "- **Positive Predictive Parity Difference (−0.0005)** and **Ratio (0.9995):** Precision is nearly identical across genders, meaning predictive reliability of positive cases is consistent.  \n",
    "- **Statistical Parity Difference (−0.0997):** Females are significantly under-selected compared to males, suggesting unequal treatment in overall prediction outcomes.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation\n",
    "\n",
    "- The MLP model shows **mixed fairness results**:  \n",
    "  - On the positive side, **AUC difference is negligible** and **precision is nearly equal** across genders.  \n",
    "  - However, females face disadvantages in **balanced accuracy, selection rates, and error distributions (Equal Odds and Statistical Parity)**.  \n",
    "- The **Equal Odds metrics** in particular reveal that males benefit from more favorable sensitivity and false positive balances, while females are disadvantaged.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The MLP model demonstrates **unequal treatment across genders**, with a tendency to favor males (privileged group).  \n",
    "While precision is balanced and AUC differences are minimal, females face **lower balanced accuracy, reduced selection rates, and less favorable error distributions**.  \n",
    "This suggests that without mitigation, the MLP model risks reinforcing bias against the unprivileged group (females).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aa66097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - MLP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>1.0707</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1.0005</td>\n",
       "      <td>0.0997</td>\n",
       "      <td>1.2084</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>1.1693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0621</td>\n",
       "      <td>0.9340</td>\n",
       "      <td>-0.0125</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>-0.0997</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>-0.1368</td>\n",
       "      <td>0.8552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                        0.0621   \n",
       "1       gender             1                       -0.0621   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0707    0.0125       1.25    0.0005     1.0005   \n",
       "1                   0.9340   -0.0125       0.80   -0.0005     0.9995   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0997           1.2084    0.1368     1.1693  \n",
       "1         -0.0997           0.8276   -0.1368     0.8552  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - MLP\")\n",
    "measure.bias(X_test, y_test, y_pred_mlp, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576976f",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – MLP by Gender\n",
    "## Stratified Bias Analysis – MLP by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the MLP model, stratified by gender.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Difference = +0.0621, Ratio = 1.0707  \n",
    "- **Males (1):** Difference = −0.0621, Ratio = 0.9340  \n",
    "- ➝ Females benefit from higher balanced accuracy, while males are disadvantaged.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** Diff = +0.0125, Ratio = 1.25  \n",
    "- **Males (1):** Diff = −0.0125, Ratio = 0.80  \n",
    "- ➝ Females experience a slightly higher false positive rate, whereas males benefit from fewer false alarms.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** Diff = +0.0005, Ratio = 1.0005  \n",
    "- **Males (1):** Diff = −0.0005, Ratio = 0.9995  \n",
    "- ➝ Precision is essentially equal across genders, with no meaningful disparity.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Diff = +0.0997, Ratio = 1.2084  \n",
    "- **Males (1):** Diff = −0.0997, Ratio = 0.8276  \n",
    "- ➝ Females are selected substantially more often, while males are under-selected.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** Diff = +0.1368, Ratio = 1.1693  \n",
    "- **Males (1):** Diff = −0.1368, Ratio = 0.8552  \n",
    "- ➝ Females enjoy much higher sensitivity, meaning their positive cases are detected more reliably. Males face a greater risk of missed detections.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged):** Gain advantages in balanced accuracy, selection rate, and sensitivity (TPR), but at the cost of a slightly higher false positive rate.  \n",
    "- **Males (privileged):** Are disadvantaged in sensitivity and balanced accuracy, though they benefit from fewer false positives.  \n",
    "- Precision (PPV) is virtually identical across genders, reducing concerns about prediction reliability.  \n",
    "\n",
    "Overall, the MLP model shows a **clear tilt in favor of females**, who receive more favorable treatment in detection and selection, though they are also more prone to false alarms. Males, in contrast, face more missed diagnoses, which is a critical fairness concern in healthcare contexts.\n",
    "## Stratified Bias Analysis – MLP by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the MLP model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Difference = +0.0621, Ratio = 1.0707  \n",
    "- **Males (1):** Difference = −0.0621, Ratio = 0.9340  \n",
    "- ➝ Females benefit from higher balanced accuracy, while males are disadvantaged.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** Diff = +0.0125, Ratio = 1.25  \n",
    "- **Males (1):** Diff = −0.0125, Ratio = 0.80  \n",
    "- ➝ Females experience a slightly higher false positive rate, whereas males benefit from fewer false alarms.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** Diff = +0.0005, Ratio = 1.0005  \n",
    "- **Males (1):** Diff = −0.0005, Ratio = 0.9995  \n",
    "- ➝ Precision is essentially equal across genders, with no meaningful disparity.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Diff = +0.0997, Ratio = 1.2084  \n",
    "- **Males (1):** Diff = −0.0997, Ratio = 0.8276  \n",
    "- ➝ Females are selected substantially more often, while males are under-selected.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** Diff = +0.1368, Ratio = 1.1693  \n",
    "- **Males (1):** Diff = −0.1368, Ratio = 0.8552  \n",
    "- ➝ Females enjoy much higher sensitivity, meaning their positive cases are detected more reliably. Males face a greater risk of missed detections.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged):** Gain advantages in balanced accuracy, selection rate, and sensitivity (TPR), but at the cost of a slightly higher false positive rate.  \n",
    "- **Males (privileged):** Are disadvantaged in sensitivity and balanced accuracy, though they benefit from fewer false positives.  \n",
    "- Precision (PPV) is virtually identical across genders, reducing concerns about prediction reliability.  \n",
    "\n",
    "Overall, the MLP model shows a **clear tilt in favor of females**, who receive more favorable treatment in detection and selection, though they are also more prone to false alarms. Males, in contrast, face more missed diagnoses, which is a critical fairness concern in healthcare contexts.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Difference = +0.0621, Ratio = 1.0707  \n",
    "- **Males (1):** Difference = −0.0621, Ratio = 0.9340  \n",
    "- ➝ Females benefit from higher balanced accuracy, while males are disadvantaged.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** Diff = +0.0125, Ratio = 1.25  \n",
    "- **Males (1):** Diff = −0.0125, Ratio = 0.80  \n",
    "- ➝ Females experience a slightly higher false positive rate, whereas males benefit from fewer false alarms.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** Diff = +0.0005, Ratio = 1.0005  \n",
    "- **Males (1):** Diff = −0.0005, Ratio = 0.9995  \n",
    "- ➝ Precision is essentially equal across genders, with no meaningful disparity.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Diff = +0.0997, Ratio = 1.2084  \n",
    "- **Males (1):** Diff = −0.0997, Ratio = 0.8276  \n",
    "- ➝ Females are selected substantially more often, while males are under-selected.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** Diff = +0.1368, Ratio = 1.1693  \n",
    "- **Males (1):** Diff = −0.1368, Ratio = 0.8552  \n",
    "- ➝ Females enjoy much higher sensitivity, meaning their positive cases are detected more reliably. Males face a greater risk of missed detections.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged):** Gain advantages in balanced accuracy, selection rate, and sensitivity (TPR), but at the cost of a slightly higher false positive rate.  \n",
    "- **Males (privileged):** Are disadvantaged in sensitivity and balanced accuracy, though they benefit from fewer false positives.  \n",
    "- Precision (PPV) is virtually identical across genders, reducing concerns about prediction reliability.  \n",
    "\n",
    "Overall, the MLP model shows a **clear tilt in favor of females**, who receive more favorable treatment in detection and selection, though they are also more prone to false alarms. Males, in contrast, face more missed diagnoses, which is a critical fairness concern in healthcare contexts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbc2caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>0.9339</td>\n",
       "      <td>0.0595</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.9138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.4783</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>0.9731</td>\n",
       "      <td>0.8077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5779</td>\n",
       "      <td>0.9416</td>\n",
       "      <td>0.9497</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9551</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.5550    0.9250   \n",
       "1        gender             0   46.0       0.5652           0.4783    0.8696   \n",
       "2        gender             1  154.0       0.5844           0.5779    0.9416   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9339  0.0595      —     0.9550   0.9778  0.9138  \n",
       "1    0.8750  0.0500      —     0.9545   0.9731  0.8077  \n",
       "2    0.9497  0.0625      —     0.9551   0.9806  0.9444  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_mlp = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_mlp = perf_table_mlp.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec7c2f",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – MLP by Gender\n",
    "\n",
    "This table shows the **stratified performance metrics** of the MLP model across gender groups.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9250)** and **F1-score (0.9339)** indicate strong overall performance.  \n",
    "- **ROC AUC (0.9778)** demonstrates excellent discriminatory ability.  \n",
    "- **Precision (0.9550)** is high, meaning positive predictions are reliable.  \n",
    "- **TPR (0.9138)** suggests that the model correctly detects most positive cases, though subgroup analysis reveals imbalances.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8696     | 0.9416   | Accuracy is much higher for males. |\n",
    "| **F1-Score**  | 0.8750     | 0.9497   | Model performs better for males. |\n",
    "| **FPR**       | 0.0500     | 0.0625   | Females experience slightly fewer false positives. |\n",
    "| **Precision** | 0.9545     | 0.9551   | Precision is nearly identical across genders. |\n",
    "| **ROC AUC**   | 0.9731     | 0.9806   | Males benefit from stronger ranking performance. |\n",
    "| **TPR**       | 0.8077     | 0.9444   | Females are more likely to be missed (lower sensitivity). |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged):**  \n",
    "  - Disadvantaged in accuracy, F1, ROC AUC, and especially sensitivity (TPR = 80.77%).  \n",
    "  - Benefit slightly from fewer false positives (5% vs. 6.25%).  \n",
    "  - Precision is nearly equal to males, showing prediction reliability is consistent.  \n",
    "\n",
    "- **Males (privileged):**  \n",
    "  - Enjoy higher accuracy, stronger F1, and better ranking ability.  \n",
    "  - Higher sensitivity (94.44%) means more male cases are correctly identified.  \n",
    "  - They face a slightly higher false positive rate, but the trade-off is favorable overall.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The MLP model demonstrates a **systematic disadvantage for females**, who suffer from lower sensitivity and weaker overall performance despite having slightly fewer false positives.  \n",
    "- **Males** benefit from higher recall, accuracy, and AUC, making predictions more favorable for them.  \n",
    "- **Females** risk more missed diagnoses, which is a critical fairness concern in clinical applications.  \n",
    "\n",
    "This indicates that the MLP model may be **biased in favor of males**, requiring fairness-aware adjustments to reduce disparities.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379dbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.8077\n",
      "  False Positive Rate (FPR): 0.0500\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9444\n",
      "  False Positive Rate (FPR): 0.0625\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_mlp == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_mlp == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fea8d1",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – MLP Model\n",
    "\n",
    "This section breaks down the classification performance of the MLP model across gender groups, using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Unprivileged (female = 0)    | 0.8077  | 0.0500  |\n",
    "| Privileged (male = 1)        | 0.9444  | 0.0625  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR)** is higher for males (94.44%) compared to females (80.77%).  \n",
    "  - This means the model is **better at correctly identifying true positive cases for males**, while females face more missed detections.  \n",
    "\n",
    "- **False Positive Rate (FPR)** is lower for females (5.00%) compared to males (6.25%).  \n",
    "  - This indicates that females are **less likely to receive false alarms** than males.  \n",
    "\n",
    "#### Implications\n",
    "\n",
    "- The MLP model shows a **gender-based trade-off**:  \n",
    "  - **Females (unprivileged):** benefit from fewer false positives (greater specificity) but suffer from lower sensitivity, meaning more true cases are missed.  \n",
    "  - **Males (privileged):** benefit from stronger sensitivity (higher TPR), but this comes with slightly more false alarms.  \n",
    "\n",
    "- These asymmetries align with the fairness metrics (e.g., **Equal Odds Difference = −0.1368** and **Equal Odds Ratio = 0.8000**), which reflect uneven error distributions across genders.  \n",
    "\n",
    "#### Recommendation\n",
    "\n",
    "- While both groups perform relatively well overall, the model tends to **favor males in sensitivity** (recall), while **females gain in specificity** (fewer false alarms).  \n",
    "- In a clinical setting:  \n",
    "  - **For early detection**, the male advantage in TPR is beneficial.  \n",
    "  - **For reducing unnecessary interventions**, the female advantage in FPR is preferable.  \n",
    "- These trade-offs highlight the need to consider **fairness-aware approaches** to balance error rates across genders.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
