{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba0b2b9",
   "metadata": {},
   "source": [
    "### Bias Detection and Fairness Evaluation on CVD Prediction (Mendeley Dataset) using FairMLhealth\n",
    "Source: https://data.mendeley.com/datasets/dzz48mvjht/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c29a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load X_test set\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57b21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup successful\n"
     ]
    }
   ],
   "source": [
    "import fairmlhealth\n",
    "import aif360\n",
    "print(\"Environment setup successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f126c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fairmlhealth\n",
      "Version: 1.0.2\n",
      "Summary: Health-centered variation analysis\n",
      "Home-page: https://github.com/KenSciResearch/fairMLHealth\n",
      "Author: Christine Allen\n",
      "Author-email: ca.magallen@gmail.com\n",
      "License: \n",
      "Location: c:\\users\\patri\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: aif360, ipython, jupyter, numpy, pandas, requests, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#have a look at the details of fairmlhealth - especially the version\n",
    "!pip show fairmlhealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76186324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "#have a look at the modules that are within fairmlhealth\n",
    "\n",
    "print(dir(fairmlhealth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44657ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "#load necessary modules \n",
    "\n",
    "#import module measure to use measure.summary for bias detection\n",
    "from fairmlhealth import measure\n",
    "\n",
    "#import module for investigation of individual cohorts \n",
    "from fairmlhealth.__utils import iterate_cohorts\n",
    "\n",
    "#import FairRanges to flag high values\n",
    "from fairmlhealth.__utils import FairRanges\n",
    "\n",
    "# Wrap the fairness summary function for cohort-wise analysis\n",
    "@iterate_cohorts\n",
    "def cohort_summary(**kwargs):\n",
    "    return measure.summary(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a099295",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models - KNN & DT\n",
    "\n",
    "#### K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d817251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true    y_prob  y_pred\n",
      "0       0       0  0.331268       0\n",
      "1       1       0  0.000000       0\n",
      "2       1       1  1.000000       1\n",
      "3       1       1  1.000000       1\n",
      "4       1       0  0.000000       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "knn_df = pd.read_csv(\"MendeleyData_75M25F_KNN_best_predictions.csv\")\n",
    "\n",
    "print(knn_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871aa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_knn = knn_df[\"y_true\"].values\n",
    "y_prob_knn = knn_df[\"y_prob\"].values\n",
    "y_pred_knn = knn_df[\"y_pred\"].values\n",
    "gender_knn = knn_df[\"gender\"].values\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_knn = gender_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad78836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.0556\n",
      "               Balanced Accuracy Difference           -0.1298\n",
      "               Balanced Accuracy Ratio                 0.8654\n",
      "               Disparate Impact Ratio                  0.8564\n",
      "               Equal Odds Difference                  -0.1752\n",
      "               Equal Odds Ratio                        6.4000\n",
      "               Positive Predictive Parity Difference  -0.0793\n",
      "               Positive Predictive Parity Ratio        0.9198\n",
      "               Statistical Parity Difference          -0.0802\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\measure.py:1224: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  prev = prev[0]\n"
     ]
    }
   ],
   "source": [
    "knn_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_true_knn,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn,\n",
    "    prtc_attr=protected_attr_knn,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,   # skip inconsistency metrics that cause NearestNeighbors error\n",
    "    skip_performance=True\n",
    ")\n",
    "\n",
    "print(knn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ba3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Custom scenario oriented bounds\n",
    "\n",
    "custom_ranges = {\n",
    "    \"tpr diff\": (-0.03, 0.03),\n",
    "    \"fpr diff\": (-0.03, 0.03),\n",
    "    \"equal odds difference\": (-0.04, 0.04),\n",
    "    \"statistical parity difference\": (-0.05, 0.05),\n",
    "    \"disparate impact ratio\": (0.9, 1.1),\n",
    "    \"selection ratio\": (0.9, 1.1),\n",
    "    \"auc difference\": (-0.02, 0.02),\n",
    "    \"balanced accuracy difference\": (-0.02, 0.02),\n",
    "}\n",
    "\n",
    "bounds = FairRanges().load_fair_ranges(custom_ranges=custom_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f14d2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  restore Styler.set_precision to adjust the highlighting color in the styled table\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "Styler = type(pd.DataFrame({\"_\":[0]}).style)  \n",
    "\n",
    "if not hasattr(Styler, \"set_precision\"):\n",
    "    def _set_precision(self, precision=4):\n",
    "        try:\n",
    "            return self.format(precision=precision)\n",
    "        except TypeError:\n",
    "            return self.format(formatter=lambda x:\n",
    "                f\"{x:.{precision}g}\" if isinstance(x, (int, float, np.floating)) else x\n",
    "            )\n",
    "    setattr(Styler, \"set_precision\", _set_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90435472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0daee_row0_col0, #T_0daee_row1_col0, #T_0daee_row3_col0, #T_0daee_row4_col0, #T_0daee_row5_col0, #T_0daee_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0daee\">\n",
       "  <caption>KNN Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0daee_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0daee_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_0daee_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_0daee_row0_col0\" class=\"data row0 col0\" >-0.0556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0daee_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_0daee_row1_col0\" class=\"data row1 col0\" >-0.1298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0daee_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_0daee_row2_col0\" class=\"data row2 col0\" >0.8654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0daee_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_0daee_row3_col0\" class=\"data row3 col0\" >0.8564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0daee_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_0daee_row4_col0\" class=\"data row4 col0\" >-0.1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0daee_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_0daee_row5_col0\" class=\"data row5 col0\" >6.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0daee_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_0daee_row6_col0\" class=\"data row6 col0\" >-0.0793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0daee_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_0daee_row7_col0\" class=\"data row7 col0\" >0.9198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0daee_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_0daee_row8_col0\" class=\"data row8 col0\" >-0.0802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0daee_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_0daee_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_0daee_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14a2585df90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "from fairmlhealth.__utils import Flagger\n",
    "\n",
    "class MyFlagger(Flagger):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.flag_color = \"#491ee6\"   \n",
    "        self.flag_type = \"background-color\"\n",
    "\n",
    "styled_knn = MyFlagger().apply_flag(\n",
    "    df=knn_bias,\n",
    "    caption=\"KNN Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d3278",
   "metadata": {},
   "source": [
    "## Gender Bias Detection Results for KNN Model \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (-0.0556)**: The ROC AUC is lower for females, indicating weaker ranking performance compared to males.  \n",
    "- **Balanced Accuracy Difference (-0.1298)** and **Ratio (0.8654)**: Substantial disparity, with females experiencing significantly worse balanced accuracy than males.  \n",
    "- **Disparate Impact Ratio (0.8564)**: Below the common fairness threshold of 0.80–1.25, suggesting unequal selection rates that disadvantage females.  \n",
    "- **Equal Odds Difference (-0.1752)** and **Equal Odds Ratio (6.4000)**: Large disparities in error rates (TPR and FPR) across genders, heavily favoring males.  \n",
    "- **Positive Predictive Parity Difference (-0.0793)** and **Ratio (0.9198)**: Predictions are less reliable for females, with lower precision compared to males.  \n",
    "- **Statistical Parity Difference (-0.0802)**: Females are selected at a lower rate than males, reinforcing evidence of imbalance.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation\n",
    "\n",
    "- The KNN model shows **marked fairness concerns**:  \n",
    "  - Females face **lower AUC and balanced accuracy**, indicating poorer overall predictive performance.  \n",
    "  - Error rates (equal odds) are highly skewed, with a **large disparity** suggesting males are treated much more favorably.  \n",
    "  - Females also experience **lower precision and reduced selection rates**, confirming consistent disadvantages across multiple fairness measures.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The KNN model demonstrates **systematic gender bias**, strongly favoring males (privileged group) at the expense of females (unprivileged group).  \n",
    "Compared to fairness thresholds, disparities in **balanced accuracy, equal odds, and selection rates** are substantial and indicate that KNN is **quite an unfair model**, requiring mitigation if considered for deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0eeae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1298</td>\n",
       "      <td>1.1555</td>\n",
       "      <td>-0.0844</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>1.0872</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>1.1677</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>1.2278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1298</td>\n",
       "      <td>0.8654</td>\n",
       "      <td>0.0844</td>\n",
       "      <td>6.4000</td>\n",
       "      <td>-0.0793</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>-0.0802</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>-0.1752</td>\n",
       "      <td>0.8145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                        0.1298   \n",
       "1       gender             1                       -0.1298   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.1555   -0.0844     0.1562    0.0793     1.0872   \n",
       "1                   0.8654    0.0844     6.4000   -0.0793     0.9198   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0802           1.1677    0.1752     1.2278  \n",
       "1         -0.0802           0.8564   -0.1752     0.8145  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - KNN\")\n",
    "measure.bias(X_test, y_test, y_pred_knn, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05764be",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – KNN by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the KNN model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Balanced Accuracy Difference = **+0.1298**, Ratio = **1.1555**  \n",
    "- **Males (1):** Balanced Accuracy Difference = **−0.1298**, Ratio = **0.8654**  \n",
    "- ➝ The model is **more balanced and accurate for females**, while males are disadvantaged.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** FPR Difference = **−0.0844**, Ratio = **0.1562**  \n",
    "- **Males (1):** FPR Difference = **+0.0844**, Ratio = **6.4000**  \n",
    "- ➝ Females face a **much lower false positive rate**, while males experience a disproportionally higher FPR.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** PPV Difference = **+0.0793**, Ratio = **1.0872**  \n",
    "- **Males (1):** PPV Difference = **−0.0793**, Ratio = **0.9198**  \n",
    "- ➝ Predictions are **more reliable for females**, while males see reduced precision.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Selection Difference = **+0.0802**, Ratio = **1.1677**  \n",
    "- **Males (1):** Selection Difference = **−0.0802**, Ratio = **0.8564**  \n",
    "- ➝ Females are **selected more often** than expected, whereas males are under-selected.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** TPR Difference = **+0.1752**, Ratio = **1.2278**  \n",
    "- **Males (1):** TPR Difference = **−0.1752**, Ratio = **0.8145**  \n",
    "- ➝ Females have a **much higher sensitivity**, meaning their true cases are almost always detected, while males face a significant risk of missed detections.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- The KNN model appears to **favor females (unprivileged group)** across nearly all metrics: higher balanced accuracy, lower false positive rate, higher precision, more favorable selection rates, and much stronger sensitivity.  \n",
    "- **Males (privileged group)** are consistently disadvantaged, with higher false positives, lower precision, under-selection, and a markedly lower true positive rate.  \n",
    "- This suggests that, unlike other models, KNN introduces a **reverse bias**, systematically favoring females over males.  \n",
    "\n",
    "While the model performs well for females, the **large disparities (especially in FPR and TPR)** highlight a fairness concern that should be addressed before deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fdeff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.5400</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.9829</td>\n",
       "      <td>0.9052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.4783</td>\n",
       "      <td>0.8261</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>0.9365</td>\n",
       "      <td>0.7692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5584</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>0.9922</td>\n",
       "      <td>0.9444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.5400    0.9300   \n",
       "1        gender             0   46.0       0.5652           0.4783    0.8261   \n",
       "2        gender             1  154.0       0.5844           0.5584    0.9610   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9375  0.0357      —     0.9722   0.9829  0.9052  \n",
       "1    0.8333  0.1000      —     0.9091   0.9365  0.7692  \n",
       "2    0.9659  0.0156      —     0.9884   0.9922  0.9444  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fairmlhealth import measure\n",
    "import pandas as pd\n",
    "from IPython.display import display  \n",
    "\n",
    "# convert gender into DataFrame with a clear column name to get a nice table as output\n",
    "gender_df = pd.DataFrame({\"gender\": X_test[\"gender\"].astype(int)})\n",
    "\n",
    "\n",
    "# Get the stratified table\n",
    "perf_table_knn = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_knn = perf_table_knn.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e845f8a",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – KNN by Gender\n",
    "\n",
    "This table shows the **stratified performance metrics** of the KNN model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9300)** and **F1-score (0.9375)** indicate good overall classification performance.  \n",
    "- **ROC AUC (0.9829)** shows excellent discriminatory power.  \n",
    "- **Precision (0.9722)** is very high, suggesting predictions are generally reliable.  \n",
    "- **TPR (0.9052)** reflects strong sensitivity overall, though subgroup breakdowns reveal disparities.  \n",
    "- **Note**: PR AUC is not available (“—”) due to insufficient subgroup size for reliable computation.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8261     | 0.9610   | Accuracy is substantially higher for males. |\n",
    "| **F1-Score**  | 0.8333     | 0.9659   | Model is more effective for males. |\n",
    "| **FPR**       | 0.1000     | 0.0156   | Females experience far more false positives. |\n",
    "| **Precision** | 0.9091     | 0.9884   | Predictions for males are more reliable. |\n",
    "| **ROC AUC**   | 0.9365     | 0.9922   | Strong disparity; males benefit from much better ranking performance. |\n",
    "| **TPR**       | 0.7692     | 0.9444   | Females are more likely to be missed (lower sensitivity). |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Face substantially worse performance: lower accuracy, F1, and ROC AUC.  \n",
    "  - Have a **much higher false positive rate (10%)** and a significantly lower **true positive rate (76.9%)**, meaning more missed CVD cases.  \n",
    "  - Precision (0.9091) is lower, so positive predictions for females are less trustworthy.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Benefit from consistently better metrics across the board.  \n",
    "  - High accuracy (96.1%), excellent sensitivity (94.4%), and very low false positive rate (1.56%).  \n",
    "  - Predictions are extremely reliable, with near-perfect precision (0.9884) and ROC AUC (0.9922).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The KNN model demonstrates a **systematic disadvantage for females**.  \n",
    "- Females are more likely to be misclassified, both in terms of **missed true cases (low TPR)** and **false alarms (high FPR)**.  \n",
    "- Males receive far more favorable outcomes across all key metrics, including accuracy, F1, precision, and AUC.  \n",
    "\n",
    "This performance disparity aligns with the fairness metrics, confirming that KNN introduces **strong gender bias in favor of males**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5db1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.7692\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9444\n",
      "  False Positive Rate (FPR): 0.0156\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#group specific error analysis\n",
    "\n",
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_knn == 0)  # unprivileged group (female)\n",
    "male_mask   = (protected_attr_knn == 1)  # privileged group (male)\n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba4143",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – KNN Model\n",
    "\n",
    "To further examine fairness at the subgroup level, we compared the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for the unprivileged (female) and privileged (male) groups.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                  | TPR     | FPR     |\n",
    "|------------------------|---------|---------|\n",
    "| Unprivileged (Female)  | 0.7692  | 0.1000  |\n",
    "| Privileged (Male)      | 0.9444  | 0.0156  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- The **privileged group (male)** has a considerably higher **TPR (94.44%)** than the unprivileged group (76.92%), showing that males are far more likely to be correctly identified when they have CVD.  \n",
    "- At the same time, the **FPR is much higher for females (10.00%)** compared to males (1.56%), meaning women are also more likely to be incorrectly flagged as having CVD.  \n",
    "- This double disparity indicates that the model is **both more sensitive and more specific for males**, while females face a greater risk of missed diagnoses and false alarms.  \n",
    "- These subgroup-level imbalances correspond to the fairness metrics, where the **Equal Odds Difference and Ratio** confirm significant disparities in error distributions between genders.  \n",
    "\n",
    "#### Summary\n",
    "\n",
    "The results reveal a **systematic disadvantage for the unprivileged group (females)**: they suffer from both lower sensitivity (missed true cases) and higher false positive rates. This highlights a critical fairness concern and underscores the need to apply **bias mitigation strategies** to ensure more equitable performance across genders.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb317ca",
   "metadata": {},
   "source": [
    "### Decision Tree - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2adbaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_dt    y_prob\n",
      "0       0       0          0  0.125000\n",
      "1       1       0          0  0.125000\n",
      "2       1       1          1  0.975845\n",
      "3       1       1          1  0.849057\n",
      "4       1       0          0  0.005236\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "dt_df = pd.read_csv(\"MendeleyData_75M25F_DT_tuned_predictions.csv\")\n",
    "\n",
    "print(dt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06530601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract common columns\n",
    "y_true_dt = dt_df[\"y_true\"].values\n",
    "y_prob_dt = dt_df[\"y_prob\"].values\n",
    "y_pred_dt = dt_df[\"y_pred_dt\"].values\n",
    "gender_dt = dt_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_dt = gender_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b83e2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Decision Tree Gender Bias Report ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.0294\n",
      "               Balanced Accuracy Difference            0.0152\n",
      "               Balanced Accuracy Ratio                 1.0169\n",
      "               Disparate Impact Ratio                  0.9360\n",
      "               Equal Odds Difference                  -0.0406\n",
      "               Equal Odds Ratio                        0.7111\n",
      "               Positive Predictive Parity Difference   0.0199\n",
      "               Positive Predictive Parity Ratio        1.0220\n",
      "               Statistical Parity Difference          -0.0387\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\measure.py:1224: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  prev = prev[0]\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Gender Bias Report\n",
    "print(\"\\n--- Decision Tree Gender Bias Report ---\")\n",
    "\n",
    "dt_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt,\n",
    "    prtc_attr=protected_attr_dt,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,  \n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(dt_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afc2d88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_58baf_row0_col0, #T_58baf_row4_col0, #T_58baf_row5_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_58baf\">\n",
       "  <caption>Decision Tree Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_58baf_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_58baf_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_58baf_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_58baf_row0_col0\" class=\"data row0 col0\" >-0.0294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58baf_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_58baf_row1_col0\" class=\"data row1 col0\" >0.0152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58baf_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_58baf_row2_col0\" class=\"data row2 col0\" >1.0169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58baf_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_58baf_row3_col0\" class=\"data row3 col0\" >0.9360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58baf_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_58baf_row4_col0\" class=\"data row4 col0\" >-0.0406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58baf_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_58baf_row5_col0\" class=\"data row5 col0\" >0.7111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58baf_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_58baf_row6_col0\" class=\"data row6 col0\" >0.0199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58baf_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_58baf_row7_col0\" class=\"data row7 col0\" >1.0220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58baf_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_58baf_row8_col0\" class=\"data row8 col0\" >-0.0387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58baf_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_58baf_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_58baf_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14a2585ebc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "styled_dt = MyFlagger().apply_flag(\n",
    "    df=dt_bias,\n",
    "    caption=\"Decision Tree Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1c7f5",
   "metadata": {},
   "source": [
    "## Fairness Evaluation – Decision Tree by Gender\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (-0.0294):** Females have slightly lower AUC compared to males, but the difference is modest.  \n",
    "- **Balanced Accuracy Difference (0.0152)** and **Ratio (1.0169):** Balanced accuracy is very similar across groups, with a minor advantage for females.  \n",
    "- **Disparate Impact Ratio (0.9360):** Slightly below the ideal range (0.8–1.25), suggesting females are selected at somewhat lower rates than males.  \n",
    "- **Equal Odds Difference (-0.0406)** and **Equal Odds Ratio (0.7111):** Notable disparity in error rates (TPR and FPR), with females experiencing worse error balance compared to males.  \n",
    "- **Positive Predictive Parity Difference (0.0199)** and **Ratio (1.0220):** Predictions for females are slightly more reliable (better precision).  \n",
    "- **Statistical Parity Difference (-0.0387):** Indicates a small disadvantage for females in overall selection rates.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation\n",
    "- The Decision Tree model shows **mixed fairness outcomes**:  \n",
    "  - **Advantages for females**: Slightly better balanced accuracy and predictive parity.  \n",
    "  - **Disadvantages for females**: Lower AUC, reduced selection rates, and higher disparity in error distribution (equal odds).  \n",
    "- The **Equal Odds Ratio (0.7111)** highlights that males benefit from more balanced error rates, while females face less equitable treatment in terms of sensitivity and specificity.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "Overall, the Decision Tree demonstrates **moderate gender disparities**.  \n",
    "While females enjoy marginal gains in predictive precision and balanced accuracy, they are disadvantaged in terms of **error distribution and selection rates**, raising fairness concerns. The disparities are not extreme but point to a systematic imbalance that may require mitigation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "631b7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - DT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0152</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>1.4062</td>\n",
       "      <td>-0.0199</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>1.0684</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>1.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>1.0169</td>\n",
       "      <td>-0.0406</td>\n",
       "      <td>0.7111</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>1.0220</td>\n",
       "      <td>-0.0387</td>\n",
       "      <td>0.9360</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>0.9890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0152   \n",
       "1       gender             1                        0.0152   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9833    0.0406     1.4062   -0.0199     0.9785   \n",
       "1                   1.0169   -0.0406     0.7111    0.0199     1.0220   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0387           1.0684    0.0103     1.0111  \n",
       "1         -0.0387           0.9360   -0.0103     0.9890  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - DT\")\n",
    "measure.bias(X_test, y_test, y_pred_dt, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa9add",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – Decision Tree by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the Decision Tree model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Balanced Accuracy Difference = −0.0152, Ratio = 0.9833  \n",
    "- **Males (1):** Balanced Accuracy Difference = +0.0152, Ratio = 1.0169  \n",
    "- ➝ Balanced accuracy is very similar across genders, with a **slight advantage for males**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** FPR Diff = +0.0406, Ratio = 1.4062  \n",
    "- **Males (1):** FPR Diff = −0.0406, Ratio = 0.7111  \n",
    "- ➝ Females experience a **higher false positive rate**, meaning they are more likely to be incorrectly flagged with CVD.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** PPV Diff = −0.0199, Ratio = 0.9785  \n",
    "- **Males (1):** PPV Diff = +0.0199, Ratio = 1.0220  \n",
    "- ➝ Males benefit from **slightly higher precision**, with more reliable positive predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Selection Diff = +0.0387, Ratio = 1.0684  \n",
    "- **Males (1):** Selection Diff = −0.0387, Ratio = 0.9360  \n",
    "- ➝ Females are **selected slightly more often** than males, despite lower precision.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** TPR Diff = +0.0103, Ratio = 1.0111  \n",
    "- **Males (1):** TPR Diff = −0.0103, Ratio = 0.9890  \n",
    "- ➝ Sensitivity is nearly equal, with **females having a marginal advantage**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged)**: Slightly higher selection rates and sensitivity, but disadvantaged by **higher false positive rates** and marginally lower precision.  \n",
    "- **Males (privileged)**: Benefit from **lower false positives and higher precision**, but are selected less often overall.  \n",
    "- The disparities are **small but meaningful**: the Decision Tree model shows a **mild imbalance**, where females face more false alarms, while males enjoy greater reliability in predictions.  \n",
    "\n",
    "Overall, fairness concerns exist but are **less severe** than those observed in KNN, suggesting the Decision Tree is comparatively more balanced across gender groups.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1acccc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.9191</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9076</td>\n",
       "      <td>0.9258</td>\n",
       "      <td>0.9310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.9130</td>\n",
       "      <td>0.9231</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9231</td>\n",
       "      <td>0.9058</td>\n",
       "      <td>0.9231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.6039</td>\n",
       "      <td>0.9026</td>\n",
       "      <td>0.9180</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9032</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.5950    0.9050   \n",
       "1        gender             0   46.0       0.5652           0.5652    0.9130   \n",
       "2        gender             1  154.0       0.5844           0.6039    0.9026   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9191  0.1310      —     0.9076   0.9258  0.9310  \n",
       "1    0.9231  0.1000      —     0.9231   0.9058  0.9231  \n",
       "2    0.9180  0.1406      —     0.9032   0.9352  0.9333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_dt = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_dt = perf_table_dt.fillna(\"—\")\n",
    "\n",
    "# Display pretty table\n",
    "display(perf_table_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936a469",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Decision Tree by Gender\n",
    "\n",
    "This table shows the **stratified performance metrics** of the Decision Tree model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9050)** and **F1-score (0.9191)** indicate strong overall classification performance.  \n",
    "- **ROC AUC (0.9258)** demonstrates good discriminatory ability.  \n",
    "- **Precision (0.9076)** and **TPR (0.9310)** show that the model balances predictive reliability and sensitivity well.  \n",
    "- **Note**: PR AUC is not available (“—”), likely due to subgroup sample size limitations.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.9130     | 0.9026   | Accuracy is slightly higher for females. |\n",
    "| **F1-Score**  | 0.9231     | 0.9180   | Performance is nearly balanced, with a small edge for females. |\n",
    "| **FPR**       | 0.1000     | 0.1406   | Females face fewer false positives compared to males. |\n",
    "| **Precision** | 0.9231     | 0.9032   | Predictions are more reliable for females. |\n",
    "| **ROC AUC**   | 0.9058     | 0.9352   | Males benefit from stronger ranking ability. |\n",
    "| **TPR**       | 0.9231     | 0.9333   | Sensitivity is slightly higher for males. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Show marginally higher accuracy, F1, and precision compared to males.  \n",
    "  - Experience a **lower false positive rate (10% vs. 14.06%)**, which reduces unnecessary false alarms.  \n",
    "  - Slightly weaker ROC AUC, suggesting less effective ranking compared to males.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Benefit from higher ROC AUC and slightly higher sensitivity (TPR).  \n",
    "  - However, they experience a **higher false positive rate** and somewhat weaker precision.  \n",
    "  - Overall, their predictions are still reliable, but less balanced compared to females.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Decision Tree model shows **relatively balanced performance across genders**, with **females enjoying advantages in precision, accuracy, and lower false positives**, while **males benefit from stronger ROC AUC and slightly higher sensitivity**.  \n",
    "The disparities are modest and indicate that the Decision Tree is more equitable than models such as KNN, though small trade-offs remain between sensitivity and specificity across groups.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53556439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.9231\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9333\n",
      "  False Positive Rate (FPR): 0.1406\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_dt == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_dt == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97580f6",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Decision Tree\n",
    "\n",
    "This section analyzes the classification performance of the Decision Tree model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Unprivileged (female = 0)    | 0.9231  | 0.1000  |\n",
    "| Privileged (male = 1)        | 0.9333  | 0.1406  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR)** is **high and very similar** across genders:  \n",
    "  - Females (unprivileged): **92.31%**  \n",
    "  - Males (privileged): **93.33%**  \n",
    "  - This shows the model is nearly equally effective in detecting true CVD cases for both groups.  \n",
    "\n",
    "- **False Positive Rate (FPR)** differs more noticeably:  \n",
    "  - Females: **10.00%**  \n",
    "  - Males: **14.06%**  \n",
    "  - Males are therefore **more frequently misclassified as having CVD**, indicating a disadvantage for the privileged group in terms of specificity.  \n",
    "\n",
    "#### Implications\n",
    "\n",
    "- The **Decision Tree achieves balanced sensitivity (TPR)** across genders.  \n",
    "- However, the **higher FPR for males** suggests a trade-off: while both groups benefit from strong detection, males face more false alarms.  \n",
    "- These subgroup disparities align with fairness metrics such as the **Equal Odds Difference** and **Equal Odds Ratio**, which capture uneven error distributions.  \n",
    "\n",
    "#### Summary\n",
    "\n",
    "Overall, the Decision Tree model delivers **balanced sensitivity across genders**, but the **elevated false positive rate for males** introduces a fairness concern. In practice, this means men may face a greater burden of unnecessary follow-ups, while women benefit from slightly better specificity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5168d",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaa2a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_rf_tuned    y_prob\n",
      "0       0       0                0  0.344558\n",
      "1       1       0                0  0.010588\n",
      "2       1       1                1  0.902502\n",
      "3       1       1                1  0.937066\n",
      "4       1       0                0  0.000000\n"
     ]
    }
   ],
   "source": [
    "rf_df = pd.read_csv(\"MendeleyData_75M25F_RF_tuned_predictions.csv\")\n",
    "print(rf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "278af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_rf = rf_df[\"y_true\"].values\n",
    "y_pred_rf = rf_df[\"y_pred_rf_tuned\"].values\n",
    "y_prob_rf = rf_df[\"y_prob\"].values\n",
    "gender_rf = rf_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_rf = gender_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "689f0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest Gender Bias Report ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0104\n",
      "               Balanced Accuracy Difference           -0.0066\n",
      "               Balanced Accuracy Ratio                 0.9931\n",
      "               Disparate Impact Ratio                  1.0775\n",
      "               Equal Odds Difference                   0.0688\n",
      "               Equal Odds Ratio                        3.2000\n",
      "               Positive Predictive Parity Difference  -0.0484\n",
      "               Positive Predictive Parity Ratio        0.9504\n",
      "               Statistical Parity Difference           0.0438\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\measure.py:1224: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  prev = prev[0]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Gender Bias Report\n",
    "print(\"\\n--- Random Forest Gender Bias Report ---\")\n",
    "\n",
    "rf_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf,\n",
    "    prtc_attr=protected_attr_rf,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(rf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7a8e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_92598_row4_col0, #T_92598_row5_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_92598\">\n",
       "  <caption>Random Forest Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_92598_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_92598_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_92598_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_92598_row0_col0\" class=\"data row0 col0\" >0.0104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92598_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_92598_row1_col0\" class=\"data row1 col0\" >-0.0066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92598_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_92598_row2_col0\" class=\"data row2 col0\" >0.9931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92598_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_92598_row3_col0\" class=\"data row3 col0\" >1.0775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92598_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_92598_row4_col0\" class=\"data row4 col0\" >0.0688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92598_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_92598_row5_col0\" class=\"data row5 col0\" >3.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92598_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_92598_row6_col0\" class=\"data row6 col0\" >-0.0484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92598_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_92598_row7_col0\" class=\"data row7 col0\" >0.9504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92598_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_92598_row8_col0\" class=\"data row8 col0\" >0.0438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92598_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_92598_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_92598_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14a25abd990>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for Random Forest\n",
    "styled_rf = MyFlagger().apply_flag(\n",
    "    df=rf_bias,\n",
    "    caption=\"Random Forest Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7c650",
   "metadata": {},
   "source": [
    "## Fairness Evaluation – Random Forest by Gender\n",
    "\n",
    "The table summarizes fairness metrics for the Random Forest model, with gender as the protected attribute  \n",
    "(**0 = Female / unprivileged, 1 = Male / privileged**).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (0.0104):** Very small gap in overall ranking performance (slightly favoring females).  \n",
    "- **Balanced Accuracy Difference (−0.0066)** and **Ratio (0.9931):** Balanced accuracy is nearly equal across genders, showing minimal disparity.  \n",
    "- **Disparate Impact Ratio (1.0775):** Within the generally accepted range (0.8–1.25), indicating relatively fair selection rates between groups.  \n",
    "- **Equal Odds Difference (0.0688)** and **Equal Odds Ratio (3.2000):** A more notable imbalance, pointing to uneven error rates (TPR/FPR) between males and females.  \n",
    "- **Positive Predictive Parity Difference (−0.0484)** and **Ratio (0.9504):** Males have slightly higher precision (positive predictions are more reliable for them).  \n",
    "- **Statistical Parity Difference (0.0438):** Suggests a modest advantage for females in selection rates.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Interpretation\n",
    "\n",
    "- The Random Forest model is **largely fair across most metrics**: AUC, balanced accuracy, and disparate impact show only minor differences.  \n",
    "- However, **equal odds metrics reveal a stronger disparity**, meaning error distributions (sensitivity and false positive rates) differ more noticeably between genders.  \n",
    "- Precision is slightly higher for males, while females appear more frequently selected.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Random Forest model achieves **mostly balanced performance across genders**, but fairness concerns emerge in the **equal odds metrics**, which suggest that one gender (likely males) benefits from more favorable error trade-offs. Overall, disparities are **moderate** , highlighting the need to monitor and potentially mitigate bias in error distribution.\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47479386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - RF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>1.0069</td>\n",
       "      <td>-0.0688</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>1.0522</td>\n",
       "      <td>-0.0438</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>-0.0556</td>\n",
       "      <td>0.9444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0066</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>0.0688</td>\n",
       "      <td>3.2000</td>\n",
       "      <td>-0.0484</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>1.0775</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>1.0588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                        0.0066   \n",
       "1       gender             1                       -0.0066   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0069   -0.0688     0.3125    0.0484     1.0522   \n",
       "1                   0.9931    0.0688     3.2000   -0.0484     0.9504   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0         -0.0438           0.9281   -0.0556     0.9444  \n",
       "1          0.0438           1.0775    0.0556     1.0588  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - RF\")\n",
    "measure.bias(X_test, y_test, y_pred_rf, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be772b16",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – Random Forest by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the Random Forest model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Difference = +0.0066, Ratio = 1.0069  \n",
    "- **Males (1):** Difference = −0.0066, Ratio = 0.9931  \n",
    "- ➝ Balanced accuracy is nearly equal, with a very slight advantage for females.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** Diff = −0.0688, Ratio = 0.3125  \n",
    "- **Males (1):** Diff = +0.0688, Ratio = 3.2000  \n",
    "- ➝ Females have a **much lower false positive rate**, while males are disproportionately more likely to be incorrectly flagged as positive.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** Diff = +0.0484, Ratio = 1.0522  \n",
    "- **Males (1):** Diff = −0.0484, Ratio = 0.9504  \n",
    "- ➝ Predictions are **more reliable for females**, with higher precision compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Diff = −0.0438, Ratio = 0.9281  \n",
    "- **Males (1):** Diff = +0.0438, Ratio = 1.0775  \n",
    "- ➝ Males are **selected more often**, while females are under-selected.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** Diff = −0.0556, Ratio = 0.9444  \n",
    "- **Males (1):** Diff = +0.0556, Ratio = 1.0588  \n",
    "- ➝ Males enjoy **higher sensitivity**, meaning they are more likely to be correctly identified when they have CVD, whereas females face more missed cases.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged)**: Benefit from **lower false positive rates** and **higher precision**, but are disadvantaged by **lower sensitivity** (TPR) and lower selection rates.  \n",
    "- **Males (privileged)**: Benefit from **higher sensitivity and more frequent selection**, but at the cost of **more false positives** and slightly lower precision.  \n",
    "- The Random Forest model shows a **trade-off in fairness**: females experience fewer false alarms but risk missed detections, while males are more often detected but face more false positives.  \n",
    "\n",
    "Overall, disparities are moderate but noticeable, consistent with the **Equal Odds metrics** reported earlier, confirming uneven error distributions across gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6573254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9881</td>\n",
       "      <td>0.9569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5649</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>0.9605</td>\n",
       "      <td>0.0312</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>0.9858</td>\n",
       "      <td>0.9444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.5750    0.9550   \n",
       "1        gender             0   46.0       0.5652           0.6087    0.9565   \n",
       "2        gender             1  154.0       0.5844           0.5649    0.9545   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9610  0.0476      —     0.9652   0.9881  0.9569  \n",
       "1    0.9630  0.1000      —     0.9286   0.9962  1.0000  \n",
       "2    0.9605  0.0312      —     0.9770   0.9858  0.9444  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_rf = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_rf = perf_table_rf.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de72316",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – Random Forest by Gender\n",
    "\n",
    "This table presents the **stratified performance metrics** of the Random Forest model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9550)** and **F1-score (0.9610)** indicate excellent overall performance.  \n",
    "- **ROC AUC (0.9881)** demonstrates very strong discriminatory ability.  \n",
    "- **Precision (0.9652)** and **TPR (0.9569)** show that the model achieves a good balance between predictive reliability and sensitivity.  \n",
    "- **Note**: For some subgroups, **PR AUC is reported as “—”** because the subgroup sample size did not allow reliable calculation of a precision–recall curve. This does not affect the validity of the other metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.9565     | 0.9545   | Accuracy is almost identical across genders, with a slight edge for females. |\n",
    "| **F1-Score**  | 0.9630     | 0.9605   | Females perform marginally better. |\n",
    "| **FPR**       | 0.1000     | 0.0312   | Males face fewer false positives, while females are more often incorrectly flagged. |\n",
    "| **Precision** | 0.9286     | 0.9770   | Predictions are more reliable for males. |\n",
    "| **ROC AUC**   | 0.9962     | 0.9858   | Both groups achieve excellent discrimination, with females slightly ahead. |\n",
    "| **TPR**       | 1.0000     | 0.9444   | Females are perfectly identified when they have CVD, while males have slightly lower sensitivity. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Achieve **perfect sensitivity (TPR = 1.0000)**, meaning no missed CVD cases.  \n",
    "  - Benefit from slightly higher accuracy, F1, and ROC AUC.  \n",
    "  - However, they experience a **higher false positive rate (10% vs. 3.1%)**, resulting in more false alarms.  \n",
    "  - Precision (0.9286) is lower, so positive predictions for females are less reliable than for males.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Benefit from **lower false positive rates** and **higher precision**, making their predictions more trustworthy.  \n",
    "  - Sensitivity is slightly weaker than for females, meaning some true cases are missed.  \n",
    "  - Despite minor trade-offs, performance remains very strong across all metrics.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The Random Forest model performs **very well for both genders**, with only modest disparities.  \n",
    "- **Females** enjoy stronger sensitivity and slightly better overall accuracy and AUC.  \n",
    "- **Males** benefit from more reliable predictions and fewer false alarms.  \n",
    "These results suggest a **trade-off rather than a clear systematic bias**: females are more likely to be over-diagnosed (higher FPR, lower precision), while males are more likely to be under-diagnosed (lower TPR).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a52946a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 1.0000\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9444\n",
      "  False Positive Rate (FPR): 0.0312\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_rf == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_rf == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06d248",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Random Forest\n",
    "\n",
    "This section presents the performance of the Random Forest model across gender groups, focusing on **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Unprivileged (female = 0)    | 1.0000  | 0.1000  |\n",
    "| Privileged (male = 1)        | 0.9444  | 0.0312  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR)** is **perfect for females (100%)**, compared to **94.44% for males**.  \n",
    "  - This means the model correctly identifies all positive CVD cases among females, but misses a small proportion of cases among males.  \n",
    "\n",
    "- **False Positive Rate (FPR)** is **higher for females (10.00%)** than for males (3.12%).  \n",
    "  - This indicates that females are more likely to receive **false alarms**, being incorrectly flagged as having CVD.  \n",
    "\n",
    "#### Implications\n",
    "\n",
    "- The model shows a **gender-based trade-off**:  \n",
    "  - **Females (unprivileged)**: enjoy higher sensitivity (no missed cases), but at the cost of more false positives.  \n",
    "  - **Males (privileged)**: benefit from better specificity (fewer false positives), but experience slightly reduced sensitivity.  \n",
    "\n",
    "- This asymmetry highlights that the Random Forest does not consistently favor one group, but rather distributes errors differently:  \n",
    "  - **Females are over-diagnosed** (more false positives).  \n",
    "  - **Males are under-diagnosed** (slightly more missed true cases).  \n",
    "\n",
    "- Depending on the clinical use case, these imbalances could have different consequences: females may face unnecessary follow-ups, while males risk missed diagnoses—both raising fairness considerations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136de6",
   "metadata": {},
   "source": [
    "### Deep Learning Model - Feed Forward Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3c69d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred        y_prob\n",
      "0       0       0       0  2.048477e-13\n",
      "1       1       0       0  1.560384e-15\n",
      "2       1       1       1  1.000000e+00\n",
      "3       1       1       1  1.000000e+00\n",
      "4       1       0       0  3.068694e-17\n"
     ]
    }
   ],
   "source": [
    "mlp_df = pd.read_csv(\"MendeleyData_75M25F_MLP_lbfgs_predictions.csv\")\n",
    "print(mlp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f65a498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns \n",
    "y_true_mlp = mlp_df[\"y_true\"].values \n",
    "y_prob_mlp = mlp_df[\"y_prob\"].values\n",
    "y_pred_mlp = mlp_df[\"y_pred\"].values\n",
    "gender_mlp = mlp_df[\"gender\"].values \n",
    "\n",
    "# Use gender_mlp as the protected attribute\n",
    "protected_attr_mlp = gender_mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6969bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\measure.py:1224: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  prev = prev[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                         -0.0560\n",
      "               Balanced Accuracy Difference           -0.0308\n",
      "               Balanced Accuracy Ratio                 0.9667\n",
      "               Disparate Impact Ratio                  0.9000\n",
      "               Equal Odds Difference                  -0.0709\n",
      "               Equal Odds Ratio                        0.9143\n",
      "               Positive Predictive Parity Difference  -0.0047\n",
      "               Positive Predictive Parity Ratio        0.9949\n",
      "               Statistical Parity Difference          -0.0604\n",
      "Data Metrics   Prevalence of Privileged Class (%)     77.0000\n"
     ]
    }
   ],
   "source": [
    "#Run fairmlhealth bias detection for MLP \n",
    "\n",
    "mlp_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp,\n",
    "    prtc_attr=protected_attr_mlp,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(mlp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbf7eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_13e4a_row0_col0, #T_13e4a_row1_col0, #T_13e4a_row3_col0, #T_13e4a_row4_col0, #T_13e4a_row8_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_13e4a\">\n",
       "  <caption>MLP Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_13e4a_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_13e4a_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_13e4a_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_13e4a_row0_col0\" class=\"data row0 col0\" >-0.0560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_13e4a_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_13e4a_row1_col0\" class=\"data row1 col0\" >-0.0308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_13e4a_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_13e4a_row2_col0\" class=\"data row2 col0\" >0.9667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_13e4a_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_13e4a_row3_col0\" class=\"data row3 col0\" >0.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_13e4a_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_13e4a_row4_col0\" class=\"data row4 col0\" >-0.0709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_13e4a_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_13e4a_row5_col0\" class=\"data row5 col0\" >0.9143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_13e4a_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_13e4a_row6_col0\" class=\"data row6 col0\" >-0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_13e4a_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_13e4a_row7_col0\" class=\"data row7 col0\" >0.9949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_13e4a_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_13e4a_row8_col0\" class=\"data row8 col0\" >-0.0604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_13e4a_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_13e4a_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_13e4a_row9_col0\" class=\"data row9 col0\" >77.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14a25b2e920>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for MLP\n",
    "styled_mlp = MyFlagger().apply_flag(\n",
    "    df=mlp_bias,\n",
    "    caption=\"MLP Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713caa5",
   "metadata": {},
   "source": [
    "## Fairness Evaluation – MLP by Gender\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Group Fairness Metrics\n",
    "\n",
    "- **AUC Difference (−0.0560):** Females have lower AUC than males, reflecting weaker ranking performance in distinguishing positive from negative cases.  \n",
    "- **Balanced Accuracy Difference (−0.0308)** and **Ratio (0.9667):** Balanced accuracy is slightly lower for females, suggesting modest disparities in classification accuracy across groups.  \n",
    "- **Disparate Impact Ratio (0.9000):** Below the ideal fairness range (0.8–1.25) but close to 1, indicating that females are selected at somewhat lower rates than males.  \n",
    "- **Equal Odds Difference (−0.0709)** and **Ratio (0.9143):** Reveal imbalances in error rates (TPR/FPR), with females experiencing less favorable outcomes compared to males.  \n",
    "- **Positive Predictive Parity Difference (−0.0047)** and **Ratio (0.9949):** Precision is nearly identical across genders, meaning predictive reliability is balanced.  \n",
    "- **Statistical Parity Difference (−0.0604):** Indicates a small disadvantage for females in overall selection rates.  \n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- The MLP model shows **modest but consistent disparities**, with males (privileged group) benefiting from slightly better AUC, balanced accuracy, and error distributions.  \n",
    "- Females (unprivileged group) face disadvantages in ranking ability, balanced accuracy, and selection rates, though differences remain relatively small.  \n",
    "- On the positive side, **predictive precision is nearly equal** across genders, reducing concerns about reliability of positive predictions.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The MLP model demonstrates **generally balanced fairness**, but with small systematic disadvantages for females in terms of accuracy, AUC, and error rate distributions.  \n",
    "While the disparities are not extreme, they highlight areas where bias mitigation could further improve gender equity in predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aa66097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>1.0938</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>1.0051</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.1112</td>\n",
       "      <td>0.0709</td>\n",
       "      <td>1.0802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0308</td>\n",
       "      <td>0.9667</td>\n",
       "      <td>-0.0094</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>-0.0047</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>-0.0604</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>-0.0709</td>\n",
       "      <td>0.9258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                        0.0308   \n",
       "1       gender             1                       -0.0308   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0345    0.0094     1.0938    0.0047     1.0051   \n",
       "1                   0.9667   -0.0094     0.9143   -0.0047     0.9949   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0604           1.1112    0.0709     1.0802  \n",
       "1         -0.0604           0.9000   -0.0709     0.9258  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - MLP\")\n",
    "measure.bias(X_test, y_test, y_pred_mlp, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576976f",
   "metadata": {},
   "source": [
    "## Stratified Bias Analysis – MLP by Gender\n",
    "\n",
    "This table presents **group-specific fairness metrics** for the MLP model, stratified by gender.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Balanced Accuracy\n",
    "- **Females (0):** Difference = +0.0308, Ratio = 1.0345  \n",
    "- **Males (1):** Difference = −0.0308, Ratio = 0.9667  \n",
    "- ➝ Females benefit from slightly higher balanced accuracy compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. False Positive Rate (FPR)\n",
    "- **Females (0):** Diff = +0.0094, Ratio = 1.0938  \n",
    "- **Males (1):** Diff = −0.0094, Ratio = 0.9143  \n",
    "- ➝ Females experience a marginally higher false positive rate, while males have fewer false alarms.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positive Predictive Value (PPV / Precision)\n",
    "- **Females (0):** Diff = +0.0047, Ratio = 1.0051  \n",
    "- **Males (1):** Diff = −0.0047, Ratio = 0.9949  \n",
    "- ➝ Predictions are slightly more reliable for females, though the difference is negligible.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Selection Rate\n",
    "- **Females (0):** Diff = +0.0604, Ratio = 1.1112  \n",
    "- **Males (1):** Diff = −0.0604, Ratio = 0.9000  \n",
    "- ➝ Females are selected more often, whereas males are under-selected relative to the baseline.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. True Positive Rate (TPR / Sensitivity)\n",
    "- **Females (0):** Diff = +0.0709, Ratio = 1.0802  \n",
    "- **Males (1):** Diff = −0.0709, Ratio = 0.9258  \n",
    "- ➝ Females enjoy a higher sensitivity, meaning more of their true cases are correctly detected compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Females (unprivileged):** Benefit from slightly higher balanced accuracy, precision, selection rates, and sensitivity, but also face a marginally higher false positive rate.  \n",
    "- **Males (privileged):** Experience fewer false positives but are disadvantaged by lower balanced accuracy, reduced sensitivity, and lower selection rates.  \n",
    "\n",
    "Overall, the disparities are **small**, but the MLP model shows a mild tendency to **favor females** in terms of sensitivity and overall detection, while males benefit slightly from fewer false positives. This trade-off indicates relatively balanced performance with only modest fairness concerns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbc2caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__preprocessing.py:68: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  X.loc[:, col] = pd.to_numeric(X[col], errors=\"ignore\")\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.9200</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9237</td>\n",
       "      <td>0.9663</td>\n",
       "      <td>0.9397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.5652</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9200</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>0.8846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.6039</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.9399</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>—</td>\n",
       "      <td>0.9247</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>0.9556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value   Obs.  Mean Target  Mean Prediction  Accuracy  \\\n",
       "0  ALL FEATURES    ALL VALUES  200.0       0.5800           0.5900    0.9200   \n",
       "1        gender             0   46.0       0.5652           0.5435    0.8913   \n",
       "2        gender             1  154.0       0.5844           0.6039    0.9286   \n",
       "\n",
       "   F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.9316  0.1071      —     0.9237   0.9663  0.9397  \n",
       "1    0.9020  0.1000      —     0.9200   0.9260  0.8846  \n",
       "2    0.9399  0.1094      —     0.9247   0.9819  0.9556  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_mlp = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_mlp = perf_table_mlp.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec7c2f",
   "metadata": {},
   "source": [
    "## Stratified Performance Analysis – MLP by Gender\n",
    "\n",
    "This table shows the **stratified performance metrics** of the MLP model across gender groups.  \n",
    "Here, **0 = Female (unprivileged)** and **1 = Male (privileged)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Overall Performance (All Features)\n",
    "- **Accuracy (0.9200)** and **F1-score (0.9316)** indicate strong overall classification performance.  \n",
    "- **ROC AUC (0.9663)** shows excellent discriminatory ability.  \n",
    "- **Precision (0.9237)** and **TPR (0.9397)** suggest the model achieves a good balance between predictive reliability and sensitivity.  \n",
    "- **Note**: PR AUC is reported as “—” because subgroup sample sizes did not allow reliable calculation of precision–recall curves.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Subgroup Comparison\n",
    "\n",
    "| Metric        | Female (0) | Male (1) | Interpretation |\n",
    "|---------------|------------|----------|----------------|\n",
    "| **Accuracy**  | 0.8913     | 0.9286   | Accuracy is higher for males. |\n",
    "| **F1-Score**  | 0.9020     | 0.9399   | The model performs better for males. |\n",
    "| **FPR**       | 0.1000     | 0.1094   | Females experience slightly fewer false positives. |\n",
    "| **Precision** | 0.9200     | 0.9247   | Predictions are marginally more reliable for males. |\n",
    "| **ROC AUC**   | 0.9260     | 0.9819   | Males benefit from stronger ranking performance. |\n",
    "| **TPR**       | 0.8846     | 0.9556   | Males are more likely to be correctly identified when they have CVD. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Interpretation\n",
    "- **Females (unprivileged)**:  \n",
    "  - Show lower accuracy, F1, and ROC AUC compared to males.  \n",
    "  - Slightly fewer false positives (FPR = 10.00%), but this comes with reduced sensitivity (TPR = 88.46%), meaning more missed cases.  \n",
    "  - Predictions are reliable, but less favorable overall than for males.  \n",
    "\n",
    "- **Males (privileged)**:  \n",
    "  - Enjoy higher accuracy, F1, precision, and significantly better ROC AUC.  \n",
    "  - Higher sensitivity (TPR = 95.56%) means fewer missed cases, though they face slightly more false positives (10.94%).  \n",
    "  - Overall, outcomes for males are more favorable, reflecting stronger model performance.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The MLP model demonstrates **gender disparities in performance**.  \n",
    "- **Males** benefit from higher accuracy, stronger recall, and much better ROC AUC, making predictions more favorable for this group.  \n",
    "- **Females** have slightly fewer false positives but are disadvantaged by lower sensitivity and weaker overall performance.  \n",
    "\n",
    "These findings suggest the MLP model may be **biased in favor of males**, as they consistently receive more reliable and accurate outcomes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "379dbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.8846\n",
      "  False Positive Rate (FPR): 0.1000\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.9556\n",
      "  False Positive Rate (FPR): 0.1094\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_mlp == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_mlp == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fea8d1",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – MLP Model\n",
    "\n",
    "This section breaks down the classification performance of the MLP model across gender groups, using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Unprivileged (female = 0)    | 0.8846  | 0.1000  |\n",
    "| Privileged (male = 1)        | 0.9556  | 0.1094  |\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **True Positive Rate (TPR)** is higher for males (95.56%) compared to females (88.46%).  \n",
    "  - This means the model is **better at correctly identifying true positive cases for males**, while females face more missed detections.  \n",
    "\n",
    "- **False Positive Rate (FPR)** is slightly lower for females (10.00%) than for males (10.94%).  \n",
    "  - This suggests that females are **less likely to receive false alarms** compared to males.  \n",
    "\n",
    "#### Implications\n",
    "\n",
    "- The MLP model shows a **gender-based trade-off**:  \n",
    "  - **Females (unprivileged)**: face reduced sensitivity (lower TPR), meaning more true cases are missed, but benefit from fewer false positives.  \n",
    "  - **Males (privileged)**: enjoy stronger sensitivity (higher TPR), but at the cost of a slightly higher false positive rate.  \n",
    "\n",
    "- These asymmetries are consistent with the fairness metrics (e.g., **Equal Odds Difference = −0.0709** and **Equal Odds Ratio = 0.9143**), which reflect uneven error distributions between groups.  \n",
    "\n",
    "#### Recommendation\n",
    "\n",
    "- While the disparities are not extreme, the model tends to **favor males in terms of sensitivity**, while **females benefit from fewer false positives**.  \n",
    "- Depending on the clinical context, these trade-offs could matter:  \n",
    "  - **For early detection**, higher sensitivity for males is advantageous.  \n",
    "  - **For reducing unnecessary interventions**, lower false positives for females are beneficial.  \n",
    "- This highlights the importance of considering **fairness mitigation strategies** to balance sensitivity and specificity across genders.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
