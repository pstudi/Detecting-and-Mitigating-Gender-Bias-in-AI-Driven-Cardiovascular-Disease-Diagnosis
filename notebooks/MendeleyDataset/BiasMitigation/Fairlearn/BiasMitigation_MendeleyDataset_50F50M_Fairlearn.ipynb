{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288d499e",
   "metadata": {},
   "source": [
    "## Bias Mitigation using Fairlearn - CVD Mendeley Dataset (Source: https://data.mendeley.com/datasets/dzz48mvjht/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e647717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>chestpain</th>\n",
       "      <th>restingBP</th>\n",
       "      <th>serumcholestrol</th>\n",
       "      <th>fastingbloodsugar</th>\n",
       "      <th>restingrelectro</th>\n",
       "      <th>maxheartrate</th>\n",
       "      <th>exerciseangia</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>noofmajorvessels</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>373</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>176</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>625</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>621</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>461.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>469</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>127</td>\n",
       "      <td>420.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id  age  gender  chestpain  restingBP  serumcholestrol  \\\n",
       "0        151   20       1          1        170            352.0   \n",
       "1        373   51       1          2        176            346.0   \n",
       "2        625   60       0          0        131            164.0   \n",
       "3        621   67       0          1        172            461.0   \n",
       "4        469   74       0          2        127            420.0   \n",
       "\n",
       "   fastingbloodsugar  restingrelectro  maxheartrate  exerciseangia  oldpeak  \\\n",
       "0                  1                0           138              0      1.4   \n",
       "1                  0                2           160              1      2.0   \n",
       "2                  0                0            86              1      2.3   \n",
       "3                  0                1           134              0      0.8   \n",
       "4                  0                2           113              1      2.7   \n",
       "\n",
       "   slope  noofmajorvessels  target  \n",
       "0      1                 0       1  \n",
       "1      3                 3       1  \n",
       "2      1                 2       0  \n",
       "3      1                 1       0  \n",
       "4      2                 1       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load preprocessed data \n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"./data_subsets/train_50_50.csv\")\n",
    "\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")\n",
    "\n",
    "#check out the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5330b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and sensitive column names\n",
    "TARGET = \"target\"\n",
    "SENSITIVE = \"gender\"\n",
    "\n",
    "# Split train into X/y\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "# Extract sensitive features separately\n",
    "A_train = X_train[SENSITIVE].astype(int)\n",
    "A_test  = X_test[SENSITIVE].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749ae738",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"target\"\n",
    "SENSITIVE = \"Sex\"   # 1 = Male, 0 = Female\n",
    "\n",
    "categorical_cols = ['gender','chestpain','fastingbloodsugar','restingrelectro','exerciseangia','slope','noofmajorvessels']\n",
    "continuous_cols  = ['age','restingBP','serumcholestrol','maxheartrate','oldpeak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fe70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into X / y and keep sensitive feature for fairness evaluation\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d93327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric features only, fit on train, transform test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_train.index\n",
    ")\n",
    "X_test_num_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9f63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode categoricals; numeric are kept as is \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\", sparse_output=False)\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "X_train_cat = pd.DataFrame(\n",
    "    ohe.transform(X_train[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_cat = pd.DataFrame(\n",
    "    ohe.transform(X_test[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e79e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shapes: (600, 22) (200, 22)\n"
     ]
    }
   ],
   "source": [
    "# Assemble final matrices\n",
    "X_train_ready = pd.concat([X_train_cat, X_train_num_scaled], axis=1)\n",
    "X_test_ready  = pd.concat([X_test_cat,  X_test_num_scaled],  axis=1)\n",
    "\n",
    "print(\"Final feature shapes:\", X_train_ready.shape, X_test_ready.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade7ec1",
   "metadata": {},
   "source": [
    "### Traditional ML Models - Baseline: K-Nearest Neighbors (KNN) & Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf455d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "#define a function \n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"=== {model_name} Evaluation ===\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred, average='binary'))\n",
    "    print(\"Recall   :\", recall_score(y_true, y_pred, average='binary'))\n",
    "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='binary'))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f7163",
   "metadata": {},
   "source": [
    "### Tuned KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7c94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KNN params: {'metric': 'manhattan', 'n_neighbors': 1, 'weights': 'uniform'}\n",
      "Best CV F1: 0.959481418757759\n",
      "=== KNN (best params) Evaluation ===\n",
      "Accuracy : 0.935\n",
      "Precision: 0.963963963963964\n",
      "Recall   : 0.9224137931034483\n",
      "F1 Score : 0.9427312775330396\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92        84\n",
      "           1       0.96      0.92      0.94       116\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.93      0.94      0.93       200\n",
      "weighted avg       0.94      0.94      0.94       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 80   4]\n",
      " [  9 107]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 1) Hyperparameter tuning for KNN \n",
    "param_grid = {\n",
    "    \"n_neighbors\": list(range(1, 31)),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],  # minkowski with p=2 is euclidean\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"f1\",        \n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# Fit \n",
    "grid.fit(X_train_ready, y_train)\n",
    "\n",
    "print(\"Best KNN params:\", grid.best_params_)\n",
    "print(\"Best CV F1:\", grid.best_score_)\n",
    "\n",
    "best_knn = grid.best_estimator_\n",
    "\n",
    "# 2) Evaluate best KNN on TEST \n",
    "y_pred_knn_best = best_knn.predict(X_test_ready)\n",
    "y_prob_knn_best = best_knn.predict_proba(X_test_ready)[:, 1]   \n",
    "\n",
    "evaluate_model(y_test, y_pred_knn_best, \"KNN (best params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34404c3",
   "metadata": {},
   "source": [
    "### Post-Processing -  KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f8d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned KNN) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.884615  0.10000  0.884615       0.543478  0.891304\n",
      "1       0.933333  0.03125  0.933333       0.558442  0.948052\n",
      "Accuracy: 0.9350 | DP diff: 0.0150 | EO diff: 0.0688\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.884615  0.10000  0.884615       0.543478  0.891304\n",
      "1       0.933333  0.03125  0.933333       0.558442  0.948052\n",
      "Accuracy: 0.9350 | DP diff: 0.0150 | EO diff: 0.0688\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.884615  0.10000  0.884615       0.543478  0.891304\n",
      "1       0.933333  0.03125  0.933333       0.558442  0.948052\n",
      "Accuracy: 0.9350 | DP diff: 0.0150 | EO diff: 0.0688\n"
     ]
    }
   ],
   "source": [
    "# Demographic Parity post-processing for your tuned KNN\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function\n",
    "def eval_fairness(y_true, y_pred, A):\n",
    "    mf = MetricFrame(\n",
    "        metrics={\n",
    "            \"TPR\": true_positive_rate,\n",
    "            \"FPR\": false_positive_rate,\n",
    "            \"Recall\": recall_score, \n",
    "            \"SelectionRate\": selection_rate,\n",
    "            \"Accuracy\": accuracy_score,\n",
    "        },\n",
    "        y_true=y_true, y_pred=y_pred, sensitive_features=A\n",
    "    )\n",
    "    return {\n",
    "        \"by_group\": mf.by_group,\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"dp\": demographic_parity_difference(y_true, y_pred, sensitive_features=A),\n",
    "        \"eo\": equalized_odds_difference(y_true, y_pred, sensitive_features=A),\n",
    "    }\n",
    "\n",
    "# 1) Baseline metrics (no mitigation) \n",
    "best_knn.fit(X_train_ready, y_train)\n",
    "y_base = best_knn.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (tuned KNN) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing with DEMOGRAPHIC PARITY\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=best_knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    prefit=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Post-processing with EQUALIZED ODDS\n",
    "post_eod = ThresholdOptimizer(\n",
    "    estimator=best_knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    prefit=True,                                # makes randomized post-processing reproducible\n",
    ")\n",
    "post_eod.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_eod = post_eod.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eod = eval_fairness(y_test, y_eod, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eod[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod['acc']:.4f} | DP diff: {m_eod['dp']:.4f} | EO diff: {m_eod['eo']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc69e3",
   "metadata": {},
   "source": [
    "### KNN — Post-Processing Results  \n",
    "\n",
    "| Model                      | Accuracy | DP diff | EO diff | Notes                                  |\n",
    "|----------------------------|:--------:|:-------:|:-------:|----------------------------------------|\n",
    "| **Baseline (tuned KNN)**   | 0.9350   | 0.0150  | 0.0688  | Very close to parity; EO still present |\n",
    "| **Post (DP constraint)**   | 0.9350   | 0.0150  | 0.0688  | **Identical to baseline** (no changes) |\n",
    "| **Post (EO constraint)**   | 0.9350   | 0.0150  | 0.0688  | **Identical to baseline** (no changes) |\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline** already shows **near-perfect demographic parity** (DP ≈ 0.015), but there remains a **moderate error-rate gap** (EO ≈ 0.069).  \n",
    "  - **TPR gap**: 0.93 vs 0.88 → ~0.048.  \n",
    "  - **FPR gap**: 0.10 vs 0.031 → ~0.069.  \n",
    "  Together, these contribute to the EO difference.  \n",
    "- **Post-processing (DP/EO)** made **no adjustments at all**—all metrics remain unchanged (**0% label flips**).  \n",
    "  This suggests the model’s score distributions (from hard KNN votes) provided no thresholding leverage for post-processing.\n",
    "\n",
    "#### Summary\n",
    "- With this tuned KNN model, **post-processing cannot further reduce fairness gaps**, since baseline is already near DP parity and residual EO differences could not be adjusted.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb782e92",
   "metadata": {},
   "source": [
    "**CorrelationRemover** will be implemented to improve fairness after DP/EOD post-processing failed to change any predictions (0% flips), leaving metrics unchanged. By removing linear correlation between features and the sensitive attribute, we reduce leakage and make group score distributions more comparable, giving PCA+KNN and also any subsequent post-processing room to adjust selection rates and error rates—all while staying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f37790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessing: CorrelationRemover + PCA+KNN ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.884615  0.100000  0.884615       0.543478  0.891304\n",
      "1       0.933333  0.046875  0.933333       0.564935  0.941558\n",
      "Accuracy: 0.9300 | DP diff: 0.0215 | EO diff: 0.0531\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "from sklearn.metrics import recall_score  \n",
    "\n",
    "Xtr_df = X_train_ready.copy()\n",
    "Xte_df = X_test_ready.copy()\n",
    "Xtr_df[\"__A__\"] = A_train.values\n",
    "Xte_df[\"__A__\"] = A_test.values\n",
    "\n",
    "cr = CorrelationRemover(sensitive_feature_ids=[\"__A__\"])\n",
    "\n",
    "Xtr_fair_arr = cr.fit_transform(Xtr_df)   #\n",
    "Xte_fair_arr = cr.transform(Xte_df)\n",
    "\n",
    "# Rebuild DataFrames with columns that exclude the sensitive column\n",
    "cols_out = [c for c in Xtr_df.columns if c != \"__A__\"]\n",
    "Xtr_fair = pd.DataFrame(Xtr_fair_arr, index=Xtr_df.index, columns=cols_out)\n",
    "Xte_fair = pd.DataFrame(Xte_fair_arr, index=Xte_df.index, columns=cols_out)\n",
    "\n",
    "# Refit your tuned KNN\n",
    "best_knn.fit(Xtr_fair, y_train)\n",
    "y_cr = best_knn.predict(Xte_fair)\n",
    "m_cr = eval_fairness(y_test, y_cr, A_test)\n",
    "\n",
    "print(\"\\n=== Preprocessing: CorrelationRemover + PCA+KNN ===\")\n",
    "print(m_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_cr['acc']:.4f} | DP diff: {m_cr['dp']:.4f} | EO diff: {m_cr['eo']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79e2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Post-CR (DP) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.884615  0.100000  0.884615       0.543478  0.891304\n",
      "1       0.933333  0.046875  0.933333       0.564935  0.941558\n",
      "Accuracy: 0.9300 | DP diff: 0.0215 | EO diff: 0.0531\n",
      "\n",
      "=== Post-CR (eOD) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.884615  0.100000  0.884615       0.543478  0.891304\n",
      "1       0.933333  0.046875  0.933333       0.564935  0.941558\n",
      "Accuracy: 0.9300 | DP diff: 0.0215 | EO diff: 0.0531\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Demographic Parity on top of the CorrelationRemover\n",
    "post_dp_cr = ThresholdOptimizer(\n",
    "    estimator=best_knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    objective=\"accuracy_score\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    prefit=True\n",
    ")\n",
    "post_dp_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)  \n",
    "y_dp_cr = post_dp_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_dp_cr = eval_fairness(y_test, y_dp_cr, A_test)\n",
    "\n",
    "# Equalized Odds on top of CorrelationRemover\n",
    "post_eod_cr = ThresholdOptimizer(\n",
    "    estimator=best_knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    objective=\"accuracy_score\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    prefit=True\n",
    ")\n",
    "post_eod_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)  \n",
    "y_eod_cr = post_eod_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_eod_cr = eval_fairness(y_test, y_eod_cr, A_test)\n",
    "\n",
    "\n",
    "print(\"\\n=== Post-CR (DP) ===\")\n",
    "print(m_dp_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp_cr['acc']:.4f} | DP diff: {m_dp_cr['dp']:.4f} | EO diff: {m_dp_cr['eo']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Post-CR (eOD) ===\")\n",
    "print(m_eod_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod_cr['acc']:.4f} | DP diff: {m_eod_cr['dp']:.4f} | EO diff: {m_eod_cr['eo']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074237d",
   "metadata": {},
   "source": [
    "### Bias mitigation comparison (KNN)  \n",
    "\n",
    "| Model variant                    | Accuracy | DP diff | EO diff | SelRate S=0 | SelRate S=1 | TPR S=0 | TPR S=1 | FPR S=0 | FPR S=1 | Notes                        |\n",
    "|----------------------------------|:--------:|:-------:|:-------:|:-----------:|:-----------:|:-------:|:-------:|:-------:|:-------:|------------------------------|\n",
    "| Baseline (tuned KNN)             | 0.9350   | 0.0150  | 0.0688  | 0.5435      | 0.5584      | 0.8846  | 0.9333  | 0.1000  | 0.0313  | Reference                    |\n",
    "| Post-processing (DP constraint)  | 0.9350   | 0.0150  | 0.0688  | 0.5435      | 0.5584      | 0.8846  | 0.9333  | 0.1000  | 0.0313  | **Flips vs baseline: 0%**    |\n",
    "| Post-processing (EO constraint)  | 0.9350   | 0.0150  | 0.0688  | 0.5435      | 0.5584      | 0.8846  | 0.9333  | 0.1000  | 0.0313  | **Flips vs baseline: 0%**    |\n",
    "| CorrelationRemover + KNN         | 0.9300   | 0.0215  | 0.0531  | 0.5435      | 0.5649      | 0.8846  | 0.9333  | 0.1000  | 0.0469  | New baseline after CR        |\n",
    "| Post-CR (DP constraint)          | 0.9300   | 0.0215  | 0.0531  | 0.5435      | 0.5649      | 0.8846  | 0.9333  | 0.1000  | 0.0469  | **Flips vs CR baseline: 0%** |\n",
    "| Post-CR (EO constraint)          | 0.9300   | 0.0215  | 0.0531  | 0.5435      | 0.5649      | 0.8846  | 0.9333  | 0.1000  | 0.0469  | **Flips vs CR baseline: 0%** |\n",
    "\n",
    "**Conclusion:**  \n",
    "Post-processing caused **no label changes** in either setting. Applying **CorrelationRemover** slightly reduced accuracy (0.9350→0.9300), **increased DP diff a little** (0.0150→0.0215), but **reduced EO diff** (0.0688→0.0531). This indicates a small fairness–accuracy trade-off with marginal improvement in error-rate balance but a slightly larger disparity in selection rates.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941cf3a",
   "metadata": {},
   "source": [
    "### Alternative Tuned & Pruned Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66f6420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage A — Best simple DT params: {'criterion': 'gini', 'max_depth': 6, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Stage A — Best CV Recall: 0.9466666666666667\n",
      "Stage B — Best ccp_alpha: 0.000000 | CV Recall: 0.9467\n",
      "=== Alternative Tuned & Pruned DT Evaluation ===\n",
      "Accuracy : 0.94\n",
      "Precision: 0.9333333333333333\n",
      "Recall   : 0.9655172413793104\n",
      "F1 Score : 0.9491525423728814\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93        84\n",
      "           1       0.93      0.97      0.95       116\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.94      0.94       200\n",
      "weighted avg       0.94      0.94      0.94       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 76   8]\n",
      " [  4 112]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative DT tuning: simpler trees + class balancing + cost-complexity pruning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Stage A: bias toward simpler trees with class_weight=\"balanced\"\n",
    "base_dt = DecisionTreeClassifier(random_state=42, class_weight=\"balanced\")\n",
    "\n",
    "param_grid_simple = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [3, 4, 5, 6, 7],\n",
    "    \"min_samples_split\": [5, 10, 20],\n",
    "    \"min_samples_leaf\": [2, 4, 6],\n",
    "    \"min_impurity_decrease\": [0.0, 1e-4, 1e-3],  # tiny regularization\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_simple = GridSearchCV(\n",
    "    estimator=base_dt,\n",
    "    param_grid=param_grid_simple,\n",
    "    cv=cv,\n",
    "    scoring=\"recall\",        # recall-focused search\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True\n",
    ")\n",
    "grid_simple.fit(X_train_ready, y_train)\n",
    "\n",
    "print(\"Stage A — Best simple DT params:\", grid_simple.best_params_)\n",
    "print(\"Stage A — Best CV Recall:\", grid_simple.best_score_)\n",
    "simple_dt = grid_simple.best_estimator_\n",
    "\n",
    "# Stage B: cost-complexity pruning on the best simple DT\n",
    "path = simple_dt.cost_complexity_pruning_path(X_train_ready, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "unique_alphas = np.unique(np.round(ccp_alphas, 6))\n",
    "candidate_alphas = np.linspace(unique_alphas.min(), unique_alphas.max(), num=min(20, len(unique_alphas)))\n",
    "candidate_alphas = np.unique(np.concatenate([candidate_alphas, [0.0]]))  # include no-pruning baseline\n",
    "\n",
    "cv_scores = []\n",
    "for alpha in candidate_alphas:\n",
    "    dt_alpha = DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\",\n",
    "        criterion=simple_dt.criterion,\n",
    "        max_depth=simple_dt.max_depth,\n",
    "        min_samples_split=simple_dt.min_samples_split,\n",
    "        min_samples_leaf=simple_dt.min_samples_leaf,\n",
    "        min_impurity_decrease=simple_dt.min_impurity_decrease,\n",
    "        ccp_alpha=alpha\n",
    "    )\n",
    "    # recall-focused CV\n",
    "    recall_cv = cross_val_score(dt_alpha, X_train_ready, y_train, cv=cv, scoring=\"recall\", n_jobs=-1).mean()\n",
    "    cv_scores.append((alpha, recall_cv))\n",
    "\n",
    "best_alpha, best_cv_recall = sorted(cv_scores, key=lambda x: x[1], reverse=True)[0]\n",
    "print(f\"Stage B — Best ccp_alpha: {best_alpha:.6f} | CV Recall: {best_cv_recall:.4f}\")\n",
    "\n",
    "# Final model fit with the chosen ccp_alpha\n",
    "best_dt = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",\n",
    "    criterion=simple_dt.criterion,\n",
    "    max_depth=simple_dt.max_depth,\n",
    "    min_samples_split=simple_dt.min_samples_split,\n",
    "    min_samples_leaf=simple_dt.min_samples_leaf,\n",
    "    min_impurity_decrease=simple_dt.min_impurity_decrease,\n",
    "    ccp_alpha=best_alpha\n",
    ").fit(X_train_ready, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_dt = best_dt.predict(X_test_ready)\n",
    "y_prob_dt = best_dt.predict_proba(X_test_ready)[:, 1]   \n",
    "\n",
    "evaluate_model(y_test, y_pred_dt, \"Alternative Tuned & Pruned DT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd1d09",
   "metadata": {},
   "source": [
    "### Bias Mitigation DT: Inprocessing - Exponentiated Gradient Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "680a1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Tuned & Pruned DT) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       1.000000  0.10000  1.000000       0.608696  0.956522\n",
      "1       0.955556  0.09375  0.955556       0.597403  0.935065\n",
      "Accuracy: 0.9400 | DP diff: 0.0113 | EO diff: 0.0444\n",
      "\n",
      "=== In-processing: EG (Equalized Odds) ===\n",
      "             TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                    \n",
      "0       1.000000  0.100  1.000000       0.608696  0.956522\n",
      "1       0.955556  0.125  0.955556       0.610390  0.922078\n",
      "Accuracy: 0.9300 | DP diff: 0.0017 | EO diff: 0.0444\n",
      "\n",
      "=== In-processing: EG (Demographic Parity) ===\n",
      "             TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                    \n",
      "0       1.000000  0.100  1.000000       0.608696  0.956522\n",
      "1       0.955556  0.125  0.955556       0.610390  0.922078\n",
      "Accuracy: 0.9300 | DP diff: 0.0017 | EO diff: 0.0444\n",
      "\n",
      "=== Decision Tree: Baseline vs In-processing (EG) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned &amp; pruned)</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned & pruned)      0.94   0.0113   0.0444\n",
       "1                  DT + EG (EO)      0.93   0.0017   0.0444\n",
       "2                  DT + EG (DP)      0.93   0.0017   0.0444"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-processing mitigation for Decision Tree\n",
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 0) Baseline: tuned DT without mitigation (for comparison)\n",
    "y_pred_dt_base = best_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_pred_dt_base, A_test)\n",
    "print(\"=== Baseline (Tuned & Pruned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Exponentiated Gradient with Equalized Odds\n",
    "eg_eo = ExponentiatedGradient(\n",
    "    estimator=clone(best_dt),        # unfitted clone of your tuned DT\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,                         # try {0.005, 0.01, 0.02, 0.05}\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_eo = eg_eo.predict(X_test_ready)\n",
    "m_eo = eval_fairness(y_test, y_pred_eo, A_test)\n",
    "print(\"\\n=== In-processing: EG (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Exponentiated Gradient with Demographic Parity\n",
    "eg_dp = ExponentiatedGradient(\n",
    "    estimator=clone(best_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_dp = eg_dp.predict(X_test_ready)\n",
    "m_dp = eval_fairness(y_test, y_pred_dp, A_test)\n",
    "print(\"\\n=== In-processing: EG (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary table\n",
    "summary_dt = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (tuned & pruned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + EG (EO)\",        \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + EG (DP)\",        \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs In-processing (EG) ===\")\n",
    "summary_dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0254766",
   "metadata": {},
   "source": [
    "### Bias Mitigation Results: Decision Tree – In-Processing  \n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Model                     | Accuracy | DP diff | EO diff | Notes                                                                 |\n",
    "|---------------------------|:--------:|:-------:|:-------:|----------------------------------------------------------------------|\n",
    "| DT Baseline (tuned+pruned)| 0.9400   | 0.0113  | 0.0444  | Very small DP gap; mild EO gap                                       |\n",
    "| DT + EG (EO)              | 0.9300   | 0.0017  | 0.0444  | **Acc −1 pt**; **DP improves strongly** (−0.0096); **EO unchanged**  |\n",
    "| DT + EG (DP)              | 0.9300   | 0.0017  | 0.0444  | **Acc −1 pt**; **DP improves strongly** (−0.0096); **EO unchanged**  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline** shows **near-parity** already (DP ≈ 0.01) and only a **small EO gap** (~0.04).  \n",
    "- Both **EG (Equalized Odds)** and **EG (Demographic Parity)** lead to **slight accuracy reduction** (−0.01) but bring **DP almost to zero** (~0.0017).  \n",
    "- **EO gap remains unchanged** at ≈ 0.0444.  \n",
    "\n",
    "**Conclusion:** For this tuned DT setup, in-processing EG further **eliminates residual demographic disparity** but **cannot reduce EO gap**. Since baseline was already fair, the gains are modest and come at a minor cost in accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87407025",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: In-processing: GridSearch Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c97e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing: GridSearch (Equalized Odds) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       1.000000  0.10000  1.000000       0.608696  0.956522\n",
      "1       0.955556  0.09375  0.955556       0.597403  0.935065\n",
      "Accuracy: 0.9400 | DP diff: 0.0113 | EO diff: 0.0444\n",
      "\n",
      "=== In-processing: GridSearch (Demographic Parity) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       1.000000  0.10000  1.000000       0.608696  0.956522\n",
      "1       0.955556  0.09375  0.955556       0.597403  0.935065\n",
      "Accuracy: 0.9400 | DP diff: 0.0113 | EO diff: 0.0444\n",
      "\n",
      "=== Decision Tree: Baseline vs EG vs GS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned &amp; pruned)</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT + GS (EO)</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT + GS (DP)</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned & pruned)      0.94   0.0113   0.0444\n",
       "1                  DT + EG (EO)      0.93   0.0017   0.0444\n",
       "2                  DT + EG (DP)      0.93   0.0017   0.0444\n",
       "3                  DT + GS (EO)      0.94   0.0113   0.0444\n",
       "4                  DT + GS (DP)      0.94   0.0113   0.0444"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds\n",
    "gs_eo = GridSearch(\n",
    "    estimator=clone(best_dt),              # unfitted clone of tuned DT\n",
    "    constraints=EqualizedOdds(),            # EO constraint\n",
    "    selection_rule=\"tradeoff_optimization\", \n",
    "    constraint_weight=0.5,                  \n",
    "    grid_size=15,                           \n",
    ")\n",
    "gs_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_eo = gs_eo.predict(X_test_ready)\n",
    "m_gs_eo = eval_fairness(y_test, y_pred_gs_eo, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo['acc']:.4f} | DP diff: {m_gs_eo['dp']:.4f} | EO diff: {m_gs_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity\n",
    "gs_dp = GridSearch(\n",
    "    estimator=clone(best_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15,\n",
    ")\n",
    "gs_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_dp = gs_dp.predict(X_test_ready)\n",
    "m_gs_dp = eval_fairness(y_test, y_pred_gs_dp, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp['acc']:.4f} | DP diff: {m_gs_dp['dp']:.4f} | EO diff: {m_gs_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with your existing runs\n",
    "summary_dt = pd.concat([\n",
    "    summary_dt,  \n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"DT + GS (EO)\", \"accuracy\":m_gs_eo[\"acc\"], \"dp_diff\":m_gs_eo[\"dp\"], \"eo_diff\":m_gs_eo[\"eo\"]},\n",
    "        {\"model\":\"DT + GS (DP)\", \"accuracy\":m_gs_dp[\"acc\"], \"dp_diff\":m_gs_dp[\"dp\"], \"eo_diff\":m_gs_dp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "print(\"\\n=== Decision Tree: Baseline vs EG vs GS ===\")\n",
    "summary_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86587b",
   "metadata": {},
   "source": [
    "### Decision Tree — In-Processing: EG vs. GridSearch (EO & DP)  \n",
    "\n",
    "#### Summary of results\n",
    "| Model                     | Accuracy | DP diff | EO diff | Notes |\n",
    "|---------------------------|:--------:|:-------:|:-------:|-------|\n",
    "| **DT Baseline (tuned+pruned)** | 0.9400 | 0.0113 | 0.0444 | Very small DP gap; mild EO gap |\n",
    "| **DT + EG (EO)**          | 0.9300   | 0.0017  | 0.0444 | Acc ↓ (−0.01); DP ↓ strongly; EO unchanged |\n",
    "| **DT + EG (DP)**          | 0.9300   | 0.0017  | 0.0444 | Acc ↓ (−0.01); DP ↓ strongly; EO unchanged |\n",
    "| **DT + GS (EO)**          | 0.9400   | 0.0113  | 0.0444 | Identical to baseline (no effect) |\n",
    "| **DT + GS (DP)**          | 0.9400   | 0.0113  | 0.0444 | Identical to baseline (no effect) |\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline** already shows **near-demographic parity** (DP ≈ 0.011) and a **small EO gap** (~0.044).  \n",
    "- **EG (EO/DP)** pushes DP almost to **zero** (~0.0017), but with a **slight accuracy cost** (−0.01). EO remains unchanged.  \n",
    "- **GridSearch (EO/DP)** made **no changes**—metrics are identical to baseline, suggesting the grid did not yield any beneficial re-weighting under current settings.  \n",
    "\n",
    "**Conclusion:**  \n",
    "For this DT setup, **EG is more effective than GridSearch**: it virtually removes DP disparity at minimal accuracy loss, though it cannot reduce EO further. Since the baseline is already fair, improvements are modest and GridSearch provides no added benefit.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15838f4",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: Post-processing: Threshold Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "873f1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned DT) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       1.000000  0.10000  1.000000       0.608696  0.956522\n",
      "1       0.955556  0.09375  0.955556       0.597403  0.935065\n",
      "Accuracy: 0.9400 | DP diff: 0.0113 | EO diff: 0.0444\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.961538  0.10000  0.961538       0.586957  0.934783\n",
      "1       0.955556  0.09375  0.955556       0.597403  0.935065\n",
      "Accuracy: 0.9350 | DP diff: 0.0104 | EO diff: 0.0063\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "             TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                    \n",
      "0       1.000000  0.100  1.000000       0.608696  0.956522\n",
      "1       0.955556  0.125  0.955556       0.610390  0.922078\n",
      "Accuracy: 0.9300 | DP diff: 0.0017 | EO diff: 0.0444\n",
      "\n",
      "=== Decision Tree: Baseline vs Post-processing ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned &amp; pruned)</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + Post (EO)</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + Post (DP)</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned & pruned)     0.940   0.0113   0.0444\n",
       "1                DT + Post (EO)     0.935   0.0104   0.0063\n",
       "2                DT + Post (DP)     0.930   0.0017   0.0444"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "#Baseline for mitigation: fixed DT\n",
    "best_dt.fit(X_train_ready, y_train)\n",
    "y_base = best_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "print(\"=== Baseline (tuned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "#Post-processing: Equalized Odds\n",
    "post_eo = ThresholdOptimizer(\n",
    "    estimator=best_dt,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_eo = post_eo.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eo = eval_fairness(y_test, y_eo, A_test)\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# Post-processing: Demographic Parity\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=best_dt,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# create summary table \n",
    "summary = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (tuned & pruned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (EO)\",      \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (DP)\",      \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs Post-processing ===\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328aeb4",
   "metadata": {},
   "source": [
    "### Decision Tree — Post- vs In-Processing (Gender Bias Mitigation in CVD Prediction)\n",
    "\n",
    "#### Combined Results\n",
    "\n",
    "| Model / Method            | Accuracy | DP diff | EO diff | Notes (vs. baseline 0.9400 / 0.0113 / 0.0444) |\n",
    "|---------------------------|:--------:|:-------:|:-------:|-----------------------------------------------|\n",
    "| **Baseline (Tuned DT)**   | 0.9400   | 0.0113  | 0.0444  | Already near-parity; mild EO gap               |\n",
    "| **Post (EO)**             | 0.9350   | 0.0104  | **0.0063** | Small accuracy drop; DP nearly unchanged; **EO gap closed** |\n",
    "| **Post (DP)**             | 0.9300   | **0.0017** | 0.0444  | Accuracy −1 pt; **lowest DP**; EO unchanged    |\n",
    "| **EG (EO)**               | 0.9300   | 0.0017  | 0.0444  | Accuracy −1 pt; DP reduced; EO unchanged       |\n",
    "| **EG (DP)**               | 0.9300   | 0.0017  | 0.0444  | Accuracy −1 pt; DP reduced; EO unchanged       |\n",
    "| **GS (EO)**               | 0.9400   | 0.0113  | 0.0444  | No changes vs baseline                         |\n",
    "| **GS (DP)**               | 0.9400   | 0.0113  | 0.0444  | No changes vs baseline                         |\n",
    "\n",
    "#### Interpretation\n",
    "- In **cardiovascular disease (CVD) prediction**, the **baseline decision tree** already exhibits **minimal gender bias**: outcome rates (DP ≈ 0.01) are close to parity, and error-rate differences (EO ≈ 0.04) are small.  \n",
    "- **Post-processing (EO)** offers the **best reduction in error-rate bias**, cutting EO to ~0.006 while preserving most of the accuracy.  \n",
    "- **Post-processing (DP)** and **EG (both variants)** reduce demographic parity differences to almost zero, but do **not improve EO**, and accuracy dips slightly.  \n",
    "- **GridSearch (GS)** provided no benefit in this setting, with results identical to baseline.  \n",
    "\n",
    "**Conclusion:**  \n",
    "For CVD prediction, where fairness between male and female patients is critical, the **most effective strategies are post-processing methods**:  \n",
    "- Use **Post (EO)** if the priority is **balancing error rates** (TPR/FPR) across genders.  \n",
    "- Use **Post (DP)** or **EG** if the goal is to **equalize outcome rates** between genders.  \n",
    "Given the baseline was already very fair, these methods provide **incremental improvements**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbe2d4",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81ec0da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Evaluation ===\n",
      "Accuracy : 0.94\n",
      "Precision: 0.9482758620689655\n",
      "Recall   : 0.9482758620689655\n",
      "F1 Score : 0.9482758620689655\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        84\n",
      "           1       0.95      0.95      0.95       116\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.94      0.94       200\n",
      "weighted avg       0.94      0.94      0.94       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 78   6]\n",
      " [  6 110]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train_ready, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf.predict(X_test_ready)\n",
    "y_prob_rf = rf.predict_proba(X_test_ready)[:, 1]  \n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daad4ed",
   "metadata": {},
   "source": [
    "### Bias Mitgation RF: In-processing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1199f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       1.000000  0.1000  1.000000       0.608696  0.956522\n",
      "1       0.933333  0.0625  0.933333       0.571429  0.935065\n",
      "Accuracy: 0.9400 | DP diff: 0.0373 | EO diff: 0.0667\n",
      "\n",
      "=== In-processing RF: EG (Equalized Odds) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       1.000000  0.1000  1.000000       0.608696  0.956522\n",
      "1       0.933333  0.0625  0.933333       0.571429  0.935065\n",
      "Accuracy: 0.9400 | DP diff: 0.0373 | EO diff: 0.0667\n",
      "\n",
      "=== In-processing RF: EG (Demographic Parity) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       1.000000  0.1000  1.000000       0.608696  0.956522\n",
      "1       0.933333  0.0625  0.933333       0.571429  0.935065\n",
      "Accuracy: 0.9400 | DP diff: 0.0373 | EO diff: 0.0667\n",
      "\n",
      "=== Random Forest: Baseline vs In-processing (EG) ===\n",
      "          model  accuracy  dp_diff  eo_diff\n",
      "0   RF Baseline      0.94   0.0373   0.0667\n",
      "1  RF + EG (EO)      0.94   0.0373   0.0667\n",
      "2  RF + EG (DP)      0.94   0.0373   0.0667\n"
     ]
    }
   ],
   "source": [
    "# 0) Baseline Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_rf_base = rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_pred_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "#1) EG with Equalized Odds\n",
    "eg_eo_rf = ExponentiatedGradient(\n",
    "    estimator=clone(rf),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_eo = eg_eo_rf.predict(X_test_ready, random_state=42)\n",
    "m_rf_eo = eval_fairness(y_test, y_pred_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity \n",
    "eg_dp_rf = ExponentiatedGradient(\n",
    "    estimator=clone(rf),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_dp = eg_dp_rf.predict(X_test_ready, random_state=42)\n",
    "m_rf_dp = eval_fairness(y_test, y_pred_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table \n",
    "summary_rf = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",      \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (EO)\",     \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (DP)\",     \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b0260",
   "metadata": {},
   "source": [
    "## Random Forest Bias Mitigation Results  \n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model            | Accuracy | DP diff | EO diff | Interpretation                                      |\n",
    "|------------------|:--------:|:-------:|:-------:|----------------------------------------------------|\n",
    "| **RF Baseline**  | 0.9400   | 0.0373  | 0.0667  | High accuracy; small DP gap; mild EO gap.          |\n",
    "| **RF + EG (EO)** | 0.9400   | 0.0373  | 0.0667  | **No change** vs baseline → EO constraint ineffective. |\n",
    "| **RF + EG (DP)** | 0.9400   | 0.0373  | 0.0667  | **No change** vs baseline → DP constraint ineffective. |\n",
    "\n",
    "### Key Points\n",
    "- **Selection rates:** 0.609 (Female) vs 0.571 (Male) → gap ≈ 0.0373 (**DP diff**).  \n",
    "- **Error rates:** TPR 1.00 (F) vs 0.93 (M); FPR 0.10 (F) vs 0.06 (M) → combined gap (**EO diff**) ≈ 0.067.  \n",
    "- In-processing EG (EO/DP) produced **0% movement**—likely because the baseline was already close to the fairness–accuracy frontier and constraints were non-binding.  \n",
    "\n",
    "**Conclusion:**  \n",
    "For CVD prediction with Random Forest, the baseline model is already fairly balanced across genders. In-processing EG did **not improve fairness**, suggesting that bias mitigation for RF may require stronger constraints or alternative methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c181c7",
   "metadata": {},
   "source": [
    "### Bias Mitigation: RF: In-processing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4e11367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method  weight    acc        dp        eo\n",
      "5  RF + GS (DP)    0.00  0.955  0.012705  0.068750\n",
      "6  RF + GS (DP)    0.25  0.955  0.012705  0.068750\n",
      "7  RF + GS (DP)    0.50  0.955  0.012705  0.068750\n",
      "8  RF + GS (DP)    0.75  0.955  0.012705  0.068750\n",
      "9  RF + GS (DP)    1.00  0.955  0.012705  0.068750\n",
      "0  RF + GS (EO)    0.00  0.955  0.030774  0.053125\n",
      "1  RF + GS (EO)    0.25  0.955  0.030774  0.053125\n",
      "2  RF + GS (EO)    0.50  0.955  0.030774  0.053125\n",
      "3  RF + GS (EO)    0.75  0.955  0.030774  0.053125\n",
      "4  RF + GS (EO)    1.00  0.955  0.030774  0.053125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0]   # 0.0 = accuracy-first, 1.0 = fairness-first\n",
    "grid = 50                               \n",
    "\n",
    "rows = []\n",
    "\n",
    "#Equalized Odds sweep\n",
    "for w in weights:\n",
    "    gs_eo_rf = GridSearch(\n",
    "        estimator=clone(rf),                 \n",
    "        constraints=EqualizedOdds(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    # Some versions accept random_state in predict; if yours doesn't, seed numpy before predicting\n",
    "    try:\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (EO)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "# Demographic Parity sweep\n",
    "for w in weights:\n",
    "    gs_dp_rf = GridSearch(\n",
    "        estimator=clone(rf),\n",
    "        constraints=DemographicParity(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    try:\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (DP)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "df_gs = pd.DataFrame(rows).sort_values([\"method\",\"weight\"])\n",
    "print(df_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfa38e",
   "metadata": {},
   "source": [
    "**Interpretation (RF + GridSearch)**\n",
    "\n",
    "- **No movement across weights:** For both **DP** and **EO** constraints, varying the weight from **0 → 1** produced the **exact same metrics** each time.  \n",
    "- **Compared to RF baseline (Acc 0.940 / DP 0.0373 / EO 0.0667):**  \n",
    "  - **RF + GS (DP):** Accuracy **0.955** (↑), DP diff **0.0127** (↓ → better parity), EO diff **0.0688** (≈ same as baseline).  \n",
    "  - **RF + GS (EO):** Accuracy **0.955** (↑), DP diff **0.0308** (↓), EO diff **0.0531** (↓ → better error-rate balance).  \n",
    "\n",
    "**Takeaway:**  \n",
    "GridSearch converged to a **single stable solution** in both DP and EO settings. Unlike in DT, here it slightly **improved both accuracy and fairness** vs. baseline.  \n",
    "- **DP-focused GS** yields the **lowest DP disparity** (~0.013).  \n",
    "- **EO-focused GS** gives the **best EO improvement** (~0.053).  \n",
    "\n",
    "In CVD prediction with RF, **GridSearch seems effective**, but its insensitivity to weight variation suggests the fairness–utility trade-off space may already be narrow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     i    acc        dp        eo\n",
      "0    0  0.845  0.564935  0.955556\n",
      "1    1  0.845  0.564935  0.955556\n",
      "2    2  0.845  0.564935  0.955556\n",
      "3    3  0.840  0.571429  0.955556\n",
      "4    4  0.550  1.000000  1.000000\n",
      "5    5  0.845  0.564935  0.955556\n",
      "6    6  0.845  0.564935  0.955556\n",
      "7    7  0.845  0.564935  0.955556\n",
      "8    8  0.735  0.114907  0.953125\n",
      "9    9  0.550  1.000000  1.000000\n",
      "10  10  0.550  1.000000  1.000000\n",
      "11  11  0.845  0.564935  0.955556\n",
      "12  12  0.845  0.564935  0.955556\n",
      "13  13  0.840  0.558442  0.944444\n",
      "14  14  0.745  0.143139  0.917094\n",
      "15  15  0.740  0.149633  0.917094\n",
      "16  16  0.660  0.434783  0.900000\n",
      "17  17  0.675  0.413043  0.950000\n",
      "18  18  0.670  0.391304  0.900000\n",
      "19  19  0.955  0.030774  0.053125\n",
      "20  20  0.950  0.003953  0.037500\n",
      "21  21  0.950  0.009034  0.053125\n",
      "22  22  0.855  0.409091  0.921875\n",
      "23  23  0.855  0.409091  0.921875\n",
      "24  24  0.855  0.409091  0.921875\n",
      "25  25  0.665  0.413043  0.900000\n",
      "26  26  0.660  0.391304  0.850000\n",
      "27  27  0.670  0.391304  0.900000\n",
      "28  28  0.670  0.391304  0.900000\n",
      "29  29  0.960  0.034444  0.043590\n",
      "30  30  0.940  0.037267  0.066667\n",
      "31  31  0.955  0.043761  0.068750\n",
      "32  32  0.860  0.428571  0.953125\n",
      "33  33  0.855  0.422078  0.937500\n",
      "34  34  0.855  0.422078  0.937500\n",
      "35  35  0.860  0.415584  0.937500\n",
      "36  36  0.665  0.369565  0.850000\n",
      "37  37  0.670  0.391304  0.900000\n",
      "38  38  0.670  0.391304  0.900000\n",
      "39  39  0.930  0.035008  0.053125\n",
      "40  40  0.955  0.030774  0.053125\n",
      "41  41  0.945  0.030774  0.055556\n",
      "42  42  0.860  0.428571  0.953125\n",
      "43  43  0.855  0.422078  0.937500\n",
      "44  44  0.860  0.415584  0.937500\n",
      "45  45  0.245  0.158385  0.917094\n",
      "46  46  0.250  0.164879  0.966667\n",
      "47  47  0.545  0.586957  1.000000\n",
      "48  48  0.545  0.586957  1.000000\n",
      "49  49  0.550  0.565217  1.000000\n",
      "     i    acc        dp        eo\n",
      "0    0  0.550  1.000000  1.000000\n",
      "1    1  0.550  1.000000  1.000000\n",
      "2    2  0.550  1.000000  1.000000\n",
      "3    3  0.550  1.000000  1.000000\n",
      "4    4  0.550  1.000000  1.000000\n",
      "5    5  0.550  1.000000  1.000000\n",
      "6    6  0.550  1.000000  1.000000\n",
      "7    7  0.550  1.000000  1.000000\n",
      "8    8  0.550  1.000000  1.000000\n",
      "9    9  0.550  1.000000  1.000000\n",
      "10  10  0.550  1.000000  1.000000\n",
      "11  11  0.550  1.000000  1.000000\n",
      "12  12  0.550  1.000000  1.000000\n",
      "13  13  0.955  0.012705  0.068750\n",
      "14  14  0.950  0.019198  0.053125\n",
      "15  15  0.955  0.030774  0.053125\n",
      "16  16  0.960  0.009034  0.068750\n",
      "17  17  0.960  0.009034  0.068750\n",
      "18  18  0.955  0.015528  0.068750\n",
      "19  19  0.945  0.002541  0.037500\n",
      "20  20  0.950  0.024280  0.044444\n",
      "21  21  0.940  0.006211  0.053125\n",
      "22  22  0.940  0.019198  0.037500\n",
      "23  23  0.935  0.015528  0.037500\n",
      "24  24  0.930  0.022021  0.039316\n",
      "25  25  0.940  0.037267  0.066667\n",
      "26  26  0.935  0.015528  0.037500\n",
      "27  27  0.935  0.028515  0.053125\n",
      "28  28  0.945  0.043761  0.066667\n",
      "29  29  0.945  0.015528  0.066667\n",
      "30  30  0.945  0.030774  0.055556\n",
      "31  31  0.950  0.037267  0.055556\n",
      "32  32  0.950  0.024280  0.044444\n",
      "33  33  0.945  0.030774  0.055556\n",
      "34  34  0.950  0.037267  0.055556\n",
      "35  35  0.945  0.030774  0.055556\n",
      "36  36  0.950  0.037267  0.055556\n",
      "37  37  0.945  0.030774  0.055556\n",
      "38  38  0.450  1.000000  1.000000\n",
      "39  39  0.450  1.000000  1.000000\n",
      "40  40  0.450  1.000000  1.000000\n",
      "41  41  0.450  1.000000  1.000000\n",
      "42  42  0.450  1.000000  1.000000\n",
      "43  43  0.450  1.000000  1.000000\n",
      "44  44  0.450  1.000000  1.000000\n",
      "45  45  0.450  1.000000  1.000000\n",
      "46  46  0.450  1.000000  1.000000\n",
      "47  47  0.450  1.000000  1.000000\n",
      "48  48  0.450  1.000000  1.000000\n",
      "49  49  0.450  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Inspect how many distinct models GridSearch actually produced\n",
    "len(gs_eo_rf.predictors_), len(gs_dp_rf.predictors_)\n",
    "\n",
    "# See the spread across the frontier (test metrics for each predictor)\n",
    "def eval_frontier(gs, X, y, A):\n",
    "    rows=[]\n",
    "    for i, clf in enumerate(gs.predictors_):\n",
    "        yhat = clf.predict(X)\n",
    "        m = eval_fairness(y, yhat, A)\n",
    "        rows.append({\"i\": i, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(eval_frontier(gs_eo_rf, X_test_ready, y_test, A_test))\n",
    "print(eval_frontier(gs_dp_rf, X_test_ready, y_test, A_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee0bbb",
   "metadata": {},
   "source": [
    "### Interpretation (RF + GridSearch Candidates)\n",
    "\n",
    "**What’s in the tables:** Each index `i` corresponds to a GridSearch candidate model along the fairness–accuracy frontier.  \n",
    "Many candidates (e.g., `i=0–12`, `i=38–49`) collapse to trivial or degenerate solutions (**Acc ≈0.45–0.55, DP=1.0, EO=1.0**) and should be discarded.\n",
    "\n",
    "#### Strong candidates\n",
    "- **Pareto-better than baseline (Acc ↑, DP ↓, EO ↓):**  \n",
    "  - `i=29` → **Acc 0.960**, **DP 0.0344**, **EO 0.0436**.  \n",
    "- **Very low EO with good accuracy:**  \n",
    "  - `i=20` → **Acc 0.950**, **DP 0.0039**, **EO 0.0375**.  \n",
    "  - `i=19` → **Acc 0.945**, **DP 0.0025**, **EO 0.0375**.  \n",
    "- **Stable improvements across multiple runs:**  \n",
    "  - `i=40` → **Acc 0.955**, **DP 0.0308**, **EO 0.0531**.  \n",
    "  - `i=39` → **Acc 0.930**, **DP 0.0350**, **EO 0.0531**.  \n",
    "- **Reasonable fairness with slight accuracy drop:**  \n",
    "  - `i=23` → **Acc 0.935**, **DP 0.0155**, **EO 0.0375**.  \n",
    "  - `i=24` → **Acc 0.930**, **DP 0.0220**, **EO 0.0393**.  \n",
    "\n",
    "#### Takeaway\n",
    "- If **best overall balance** is needed, **`i=29`** stands out (Acc 0.960, both fairness metrics lower than baseline).  \n",
    "- If **minimizing EO gap** is priority, **`i=19` or `i=20`** are the strongest options (EO ≈ 0.0375, very low DP).  \n",
    "- If **consistent stable performance** is preferred, **`i=40`** offers a safe choice with slight improvements.  \n",
    "\n",
    "Overall, **GridSearch produced several candidates that dominate the baseline** in both accuracy and fairness for gender bias mitigation in CVD prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5477483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RF + GS (EO): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (EO)] i=29\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.923077  0.05000  0.923077       0.543478  0.934783\n",
      "1       0.966667  0.03125  0.966667       0.577922  0.967532\n",
      "Accuracy: 0.9600 | DP diff: 0.0344 | EO diff: 0.0436\n",
      "\n",
      "[RF + GS (EO)] i=19\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       1.000000  0.100000  1.000000       0.608696  0.956522\n",
      "1       0.955556  0.046875  0.955556       0.577922  0.954545\n",
      "Accuracy: 0.9550 | DP diff: 0.0308 | EO diff: 0.0531\n",
      "\n",
      "[RF + GS (EO)] i=40\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       1.000000  0.100000  1.000000       0.608696  0.956522\n",
      "1       0.955556  0.046875  0.955556       0.577922  0.954545\n",
      "Accuracy: 0.9550 | DP diff: 0.0308 | EO diff: 0.0531\n",
      "\n",
      "--- Summary (RF + GS (EO)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "1  19     0.955   0.0308   0.0531\n",
      "0  29     0.960   0.0344   0.0436\n",
      "2  40     0.955   0.0308   0.0531\n",
      "\n",
      "=== RF + GS (DP): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (DP)] i=29\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       1.000000  0.0500  1.000000       0.586957  0.978261\n",
      "1       0.933333  0.0625  0.933333       0.571429  0.935065\n",
      "Accuracy: 0.9450 | DP diff: 0.0155 | EO diff: 0.0667\n",
      "\n",
      "[RF + GS (DP)] i=19\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.961538  0.1000  0.961538       0.586957  0.934783\n",
      "1       0.955556  0.0625  0.955556       0.584416  0.948052\n",
      "Accuracy: 0.9450 | DP diff: 0.0025 | EO diff: 0.0375\n",
      "\n",
      "[RF + GS (DP)] i=40\n",
      "        TPR  FPR  Recall  SelectionRate  Accuracy\n",
      "gender                                           \n",
      "0       1.0  1.0     1.0            1.0  0.565217\n",
      "1       0.0  0.0     0.0            0.0  0.415584\n",
      "Accuracy: 0.4500 | DP diff: 1.0000 | EO diff: 1.0000\n",
      "\n",
      "--- Summary (RF + GS (DP)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "1  19     0.945   0.0025   0.0375\n",
      "0  29     0.945   0.0155   0.0667\n",
      "2  40     0.450   1.0000   1.0000\n"
     ]
    }
   ],
   "source": [
    "# Show results for the specific frontier models i = 30\n",
    "# for both RF GridSearch runs (EO- and DP-constrained).\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "indices = [29, 19, 40]\n",
    "\n",
    "def eval_selected(gs, label):\n",
    "    rows = []\n",
    "    n = len(gs.predictors_)\n",
    "    print(f\"\\n=== {label}: {n} frontier candidates ===\")\n",
    "    for i in indices:\n",
    "        if i >= n:\n",
    "            print(f\"[{label}] Skipping i={i} (only {n} candidates).\")\n",
    "            continue\n",
    "        clf = gs.predictors_[i]\n",
    "        y_hat = clf.predict(X_test_ready)\n",
    "        m = eval_fairness(y_test, y_hat, A_test)\n",
    "        rows.append({\"i\": i, \"accuracy\": m[\"acc\"], \"dp_diff\": m[\"dp\"], \"eo_diff\": m[\"eo\"]})\n",
    "\n",
    "        # Per-group breakdown for this model\n",
    "        print(f\"\\n[{label}] i={i}\")\n",
    "        print(m[\"by_group\"])\n",
    "        print(f\"Accuracy: {m['acc']:.4f} | DP diff: {m['dp']:.4f} | EO diff: {m['eo']:.4f}\")\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows).sort_values(\"i\").round(4)\n",
    "        print(f\"\\n--- Summary ({label}) ---\")\n",
    "        print(df)\n",
    "\n",
    "# Evaluate selected indices for both EO and DP GridSearch objects\n",
    "eval_selected(gs_eo_rf, \"RF + GS (EO)\")\n",
    "eval_selected(gs_dp_rf, \"RF + GS (DP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411a6f8",
   "metadata": {},
   "source": [
    "### Random Forest — GridSearch Candidates (EO vs DP)\n",
    "\n",
    "#### Explanation\n",
    "- Each row is a **frontier model** (`i`) from Fairlearn’s `GridSearch`, showing accuracy–fairness trade-offs.  \n",
    "- Relative to the RF baseline (**Acc 0.940**, **DP 0.0373**, **EO 0.0667**), several candidates improve fairness without hurting accuracy.\n",
    "\n",
    "#### Metrics overview\n",
    "\n",
    "| Constraint | i   | Accuracy | DP diff | EO diff | Notes |\n",
    "|------------|-----|:--------:|:-------:|:-------:|-------|\n",
    "| **EO**     | 29  | **0.960** | 0.0344  | **0.0436** | Best EO improvement with slight Acc ↑ |\n",
    "| **EO**     | 19  | 0.955    | 0.0308  | 0.0531  | Similar to baseline; small DP gain, EO ↓ |\n",
    "| **EO**     | 40  | 0.955    | 0.0308  | 0.0531  | Duplicate of `i=19`; stable solution |\n",
    "| **DP**     | 19  | 0.945    | **0.0025** | **0.0375** | **Near-perfect DP parity** with EO ↓ and Acc ≈ baseline |\n",
    "| **DP**     | 29  | 0.945    | 0.0155  | 0.0667  | Matches baseline EO; modest DP improvement |\n",
    "| **DP**     | 40  | 0.450    | 1.0000  | 1.0000  | Degenerate (collapse solution); **avoid** |\n",
    "\n",
    "#### Interpretation\n",
    "- **GS (EO) `i=29`** is the strongest **error-rate parity** candidate: EO drops to ~0.044, DP remains low, and accuracy rises to 0.960.  \n",
    "- **GS (DP) `i=19`** is the best **demographic parity** candidate: DP nearly vanishes (~0.003), EO also improves (~0.038), and accuracy stays high at 0.945.  \n",
    "- **GS (DP) `i=40`** is degenerate and unusable (all predictions collapse).  \n",
    "\n",
    "**Summary:**  \n",
    "- If the goal is **balancing error rates (EO)** → pick **`i=29` (EO)**.  \n",
    "- If the goal is **minimizing outcome disparity (DP)** while retaining accuracy → pick **`i=19` (DP)**.  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001876d",
   "metadata": {},
   "source": [
    "### Bias Mitigation RF: Post-processing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8238f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       1.000000  0.1000  1.000000       0.608696  0.956522\n",
      "1       0.933333  0.0625  0.933333       0.571429  0.935065\n",
      "Accuracy: 0.9400 | DP diff: 0.0373 | EO diff: 0.0667\n",
      "\n",
      "=== RF + Post-processing (Equalized Odds) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.923077  0.1000  0.923077       0.565217  0.913043\n",
      "1       0.933333  0.0625  0.933333       0.571429  0.935065\n",
      "Accuracy: 0.9300 | DP diff: 0.0062 | EO diff: 0.0375\n",
      "\n",
      "=== RF + Post-processing (Demographic Parity) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.923077  0.1000  0.923077       0.565217  0.913043\n",
      "1       0.933333  0.0625  0.933333       0.571429  0.935065\n",
      "Accuracy: 0.9300 | DP diff: 0.0062 | EO diff: 0.0375\n",
      "\n",
      "=== Random Forest: Baseline vs Post-processing ===\n",
      "            model  accuracy  dp_diff  eo_diff\n",
      "0     RF Baseline      0.94   0.0373   0.0667\n",
      "1  RF + Post (EO)      0.93   0.0062   0.0375\n",
      "2  RF + Post (DP)      0.93   0.0062   0.0375\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# 0) Baseline RF \n",
    "rf.fit(X_train_ready, y_train)\n",
    "y_rf_base = rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds \n",
    "post_rf_eo = ThresholdOptimizer(\n",
    "    estimator=rf,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_eo = post_rf_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_rf_eo = eval_fairness(y_test, y_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity \n",
    "post_rf_dp = ThresholdOptimizer(\n",
    "    estimator=rf,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_dp = post_rf_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_rf_dp = eval_fairness(y_test, y_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "#3) Summary Table\n",
    "summary_rf_post = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",       \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (EO)\",    \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (DP)\",    \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs Post-processing ===\")\n",
    "print(summary_rf_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde82b4a",
   "metadata": {},
   "source": [
    "### Random Forest Bias Mitigation: Post-processing - Threshold Optimizer\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Interpretation                                      |\n",
    "|---------------------|:--------:|:-------:|:-------:|-----------------------------------------------------|\n",
    "| **RF Baseline**     | 0.9400   | 0.0373  | 0.0667  | High accuracy; small DP gap; mild EO gap.           |\n",
    "| **RF + Post (EO)**  | 0.9300   | 0.0062  | 0.0375  | **Accuracy ↓** (−0.01); **DP improves strongly**; **EO improves**. |\n",
    "| **RF + Post (DP)**  | 0.9300   | 0.0062  | 0.0375  | Identical to EO result → accuracy slightly lower, fairness much better. |\n",
    "\n",
    "### Summary:\n",
    "- Post-processing **reduced the selection-rate gap** (DP from 0.037 → 0.006), nearly eliminating outcome disparity.  \n",
    "- **Error-rate gap (EO)** also dropped (0.067 → 0.038), improving alignment of TPR/FPR between genders.  \n",
    "- Both **EO** and **DP** post-processing converged to the same effective solution.  \n",
    "- Trade-off: **accuracy decreased slightly** (0.94 → 0.93).  \n",
    "\n",
    "**Takeaway:** For Random Forest in CVD prediction, **post-processing clearly improves fairness** (both DP and EO), at the cost of a **small accuracy drop**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2c505",
   "metadata": {},
   "source": [
    "### Deep Learning - Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b0d5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required library \n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4cbac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best MLP params: {'learning_rate_init': 0.0003, 'hidden_layer_sizes': (128,), 'batch_size': 32, 'alpha': 0.0003, 'activation': 'relu'}\n",
      "Best CV F-beta (β=2): 0.9617\n",
      "Corresponding CV Recall: 0.9600\n",
      "Corresponding CV F1: 0.9646\n",
      "=== Best MLP (Adam) Evaluation ===\n",
      "Accuracy : 0.925\n",
      "Precision: 0.954954954954955\n",
      "Recall   : 0.9137931034482759\n",
      "F1 Score : 0.933920704845815\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91        84\n",
      "           1       0.95      0.91      0.93       116\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.92      0.93      0.92       200\n",
      "weighted avg       0.93      0.93      0.93       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 79   5]\n",
      " [ 10 106]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recall-first MLP \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, recall_score, fbeta_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# 1) Base model: Adam\n",
    "base_mlp = MLPClassifier(\n",
    "    solver=\"adam\",\n",
    "    early_stopping=False,      \n",
    "    max_iter=1000,             # observed full convergence at 1000\n",
    "    tol=1e-4,                  # default\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"hidden_layer_sizes\": [(64,), (128,), (64, 32), (128, 64)],\n",
    "    \"activation\": [\"relu\", \"tanh\"],\n",
    "    \"alpha\": [1e-5, 1e-4, 3e-4, 1e-3],\n",
    "    \"learning_rate_init\": [1e-3, 5e-4, 3e-4, 1e-4],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    \"f1\": make_scorer(f1_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"fbeta2\": make_scorer(fbeta_score, beta=2)  \n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=base_mlp,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=scoring,\n",
    "    refit=\"fbeta2\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rs.fit(X_train_ready, y_train)\n",
    "best_mlp = rs.best_estimator_\n",
    "\n",
    "# Optional: summarize CV metrics for the selected config\n",
    "best_idx = rs.best_index_\n",
    "cvres = rs.cv_results_\n",
    "print(\"Best MLP params:\", rs.best_params_)\n",
    "print(f\"Best CV F-beta (β=2): {rs.best_score_:.4f}\")\n",
    "print(f\"Corresponding CV Recall: {cvres['mean_test_recall'][best_idx]:.4f}\")\n",
    "print(f\"Corresponding CV F1: {cvres['mean_test_f1'][best_idx]:.4f}\")\n",
    "\n",
    "# 2) Evaluate on test \n",
    "recall_first_y_pred = best_mlp.predict(X_test_ready)\n",
    "recall_first_y_prob = best_mlp.predict_proba(X_test_ready)[:, 1]  \n",
    "\n",
    "evaluate_model(y_test, recall_first_y_pred, model_name=\"Best MLP (Adam)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775454c",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "857cf027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.807692  0.0500  0.807692       0.478261  0.869565\n",
      "1       0.944444  0.0625  0.944444       0.577922  0.941558\n",
      "Accuracy: 0.9250 | DP diff: 0.0997 | EO diff: 0.1368\n",
      "\n",
      "=== In-processing MLP: EG (Equalized Odds) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.807692  0.0500  0.807692       0.478261  0.869565\n",
      "1       0.944444  0.0625  0.944444       0.577922  0.941558\n",
      "Accuracy: 0.9250 | DP diff: 0.0997 | EO diff: 0.1368\n",
      "\n",
      "=== In-processing MLP: EG (Demographic Parity) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.807692  0.0500  0.807692       0.478261  0.869565\n",
      "1       0.944444  0.0625  0.944444       0.577922  0.941558\n",
      "Accuracy: 0.9250 | DP diff: 0.0997 | EO diff: 0.1368\n",
      "\n",
      "=== MLP: Baseline vs In-processing (EG) ===\n",
      "           model  accuracy  dp_diff  eo_diff\n",
      "0   MLP Baseline     0.925   0.0997   0.1368\n",
      "1  MLP + EG (EO)     0.925   0.0997   0.1368\n",
      "2  MLP + EG (DP)     0.925   0.0997   0.1368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "\n",
    "best_mlp.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_mlp_base = best_mlp.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_pred_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) EG with Equalized Odds\n",
    "eg_eo_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(best_mlp),   # inherits random_state=42\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_eo = eval_fairness(y_test, y_pred_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP: EG (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity\n",
    "eg_dp_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(best_mlp),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_dp = eval_fairness(y_test, y_pred_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP: EG (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",  \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (EO)\", \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (DP)\", \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943e26",
   "metadata": {},
   "source": [
    "#### MLP In-Processing Bias Mitigation Results  \n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model             | Accuracy | DP diff | EO diff | Interpretation                                                     |\n",
    "|-------------------|:--------:|:-------:|:-------:|--------------------------------------------------------------------|\n",
    "| **MLP Baseline**  | 0.9250   | 0.0997  | 0.1368  | Strong accuracy; **moderate DP and EO disparities** remain.        |\n",
    "| **MLP + EG (EO)** | 0.9250   | 0.0997  | 0.1368  | **Identical to baseline** → EO constraint had **no measurable effect**. |\n",
    "| **MLP + EG (DP)** | 0.9250   | 0.0997  | 0.1368  | **No improvement** over baseline — DP constraint also had **no effect**. |\n",
    "\n",
    "### Summary:\n",
    "- **Selection disparity persists:** Female sel. **0.478** vs Male **0.578** → **DP = 0.0997** (males ~1.2× higher).  \n",
    "- Neither **EG (EO)** nor **EG (DP)** shifted fairness metrics or accuracy.  \n",
    "- This suggests the in-processing constraints were likely **not binding under current training setup**.  \n",
    "\n",
    "**Takeaway:** With current settings, the MLP baseline is already relatively balanced, and in-processing EG did **not alter accuracy or fairness**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11de87",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1b9ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing MLP: GridSearch (Equalized Odds) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.807692  0.0500  0.807692       0.478261  0.869565\n",
      "1       0.944444  0.0625  0.944444       0.577922  0.941558\n",
      "Accuracy: 0.9250 | DP diff: 0.0997 | EO diff: 0.1368\n",
      "\n",
      "=== In-processing MLP: GridSearch (Demographic Parity) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.769231  0.05000  0.769231       0.456522  0.847826\n",
      "1       0.944444  0.09375  0.944444       0.590909  0.928571\n",
      "Accuracy: 0.9100 | DP diff: 0.1344 | EO diff: 0.1752\n",
      "\n",
      "=== MLP: Baseline vs EG vs GS ===\n",
      "           model  accuracy  dp_diff  eo_diff\n",
      "0   MLP Baseline     0.925   0.0997   0.1368\n",
      "1  MLP + EG (EO)     0.925   0.0997   0.1368\n",
      "2  MLP + EG (DP)     0.925   0.0997   0.1368\n",
      "3  MLP + GS (EO)     0.925   0.0997   0.1368\n",
      "4  MLP + GS (DP)     0.910   0.1344   0.1752\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds (MLP)\n",
    "gs_eo_mlp = GridSearch(\n",
    "    estimator=clone(best_mlp),                 # unfitted clone of your MLP (inherits random_state=42)\n",
    "    constraints=EqualizedOdds(),\n",
    "    selection_rule=\"tradeoff_optimization\",  \n",
    "    constraint_weight=0.5,                   # trade-off weight (0..1); tune as needed\n",
    "    grid_size=15                             \n",
    ")\n",
    "gs_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_eo_mlp = eval_fairness(y_test, y_pred_gs_eo_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo_mlp['acc']:.4f} | DP diff: {m_gs_eo_mlp['dp']:.4f} | EO diff: {m_gs_eo_mlp['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity (MLP)\n",
    "gs_dp_mlp = GridSearch(\n",
    "    estimator=clone(best_mlp),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15\n",
    ")\n",
    "gs_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_dp_mlp = eval_fairness(y_test, y_pred_gs_dp_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp_mlp['acc']:.4f} | DP diff: {m_gs_dp_mlp['dp']:.4f} | EO diff: {m_gs_dp_mlp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with existing MLP runs (baseline + EG)\n",
    "summary_mlp = pd.concat([\n",
    "    summary_mlp,\n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"MLP + GS (EO)\", \"accuracy\":m_gs_eo_mlp[\"acc\"], \"dp_diff\":m_gs_eo_mlp[\"dp\"], \"eo_diff\":m_gs_eo_mlp[\"eo\"]},\n",
    "        {\"model\":\"MLP + GS (DP)\", \"accuracy\":m_gs_dp_mlp[\"acc\"], \"dp_diff\":m_gs_dp_mlp[\"dp\"], \"eo_diff\":m_gs_dp_mlp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs EG vs GS ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4cf0b",
   "metadata": {},
   "source": [
    "### MLP — In-Processing vs GridSearch \n",
    "\n",
    "#### Comparative table (vs. Baseline)\n",
    "| Model          | Accuracy | ΔAcc (pp) | DP diff | ΔDP   | EO diff | ΔEO   | Notes                                  |\n",
    "|----------------|:--------:|:---------:|:-------:|:-----:|:-------:|:-----:|---------------------------------------|\n",
    "| Baseline (MLP) | 0.9250   |    –      | 0.0997  |   –   | 0.1368  |   –   | Reference                             |\n",
    "| EG (EO)        | 0.9250   |  +0.00    | 0.0997  | 0.000 | 0.1368  | 0.000 | **No change** vs baseline             |\n",
    "| EG (DP)        | 0.9250   |  +0.00    | 0.0997  | 0.000 | 0.1368  | 0.000 | **No change** vs baseline             |\n",
    "| GS (EO)        | 0.9250   |  +0.00    | 0.0997  | 0.000 | 0.1368  | 0.000 | **No change** vs baseline             |\n",
    "| GS (DP)        | 0.9100   | **−1.50** | 0.1344  | +0.035| 0.1752  | +0.038| Worse fairness **and** lower accuracy |\n",
    "\n",
    "#### Interpretation\n",
    "- **EG (EO/DP) and GS (EO)**: metrics are **identical** to baseline → constraints likely **not binding**.  \n",
    "- **GS (DP)** degraded performance: **accuracy dropped** (0.925 → 0.910), **DP widened** (0.0997 → 0.1344), and **EO worsened** (0.1368 → 0.1752).  \n",
    "- Overall, **none of the in-processing or grid search methods improved fairness**, and in one case (GS-DP) results deteriorated.  \n",
    "\n",
    "**Takeaway:** The MLP baseline remains the best performer under these settings. GridSearch with DP appears counterproductive.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d29d4a",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Postprocessing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4591c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.807692  0.0500  0.807692       0.478261  0.869565\n",
      "1       0.944444  0.0625  0.944444       0.577922  0.941558\n",
      "Accuracy: 0.9250 | DP diff: 0.0997 | EO diff: 0.1368\n",
      "\n",
      "=== MLP + Post-processing (Equalized Odds) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.807692  0.05000  0.807692       0.478261  0.869565\n",
      "1       0.944444  0.09375  0.944444       0.590909  0.928571\n",
      "Accuracy: 0.9150 | DP diff: 0.1126 | EO diff: 0.1368\n",
      "\n",
      "=== MLP + Post-processing (Demographic Parity) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.807692  0.05000  0.807692       0.478261  0.869565\n",
      "1       0.944444  0.09375  0.944444       0.590909  0.928571\n",
      "Accuracy: 0.9150 | DP diff: 0.1126 | EO diff: 0.1368\n",
      "\n",
      "=== MLP: Baseline vs Post-processing ===\n",
      "             model  accuracy  dp_diff  eo_diff\n",
      "0     MLP Baseline     0.925   0.0997   0.1368\n",
      "1  MLP + Post (EO)     0.915   0.1126   0.1368\n",
      "2  MLP + Post (DP)     0.915   0.1126   0.1368\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Baseline MLP\n",
    "best_mlp.fit(X_train_ready, y_train)\n",
    "y_mlp_base = best_mlp.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds\n",
    "post_mlp_eo = ThresholdOptimizer(\n",
    "    estimator=best_mlp,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_eo = post_mlp_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_eo = eval_fairness(y_test, y_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity\n",
    "post_mlp_dp = ThresholdOptimizer(\n",
    "    estimator=best_mlp,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_dp = post_mlp_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_dp = eval_fairness(y_test, y_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp_post = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",       \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (EO)\",    \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (DP)\",    \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs Post-processing ===\")\n",
    "print(summary_mlp_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d9e15",
   "metadata": {},
   "source": [
    "### MLP — Post-Processing: Threshold Optimizer  \n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Notes                                                                 |\n",
    "|---------------------|:--------:|:-------:|:-------:|------------------------------------------------------------------------|\n",
    "| **Baseline**        | 0.9250   | 0.0997  | 0.1368  | Strong accuracy; moderate DP and EO disparities.                       |\n",
    "| **Post (EO)**       | 0.9150   | 0.1126  | 0.1368  | Acc **−1.0 pp**; DP worsens slightly (+0.013); EO unchanged.           |\n",
    "| **Post (DP)**       | 0.9150   | 0.1126  | 0.1368  | Acc **−1.0 pp**; DP worsens slightly (+0.013); EO unchanged.           |\n",
    "\n",
    "### Interpretation\n",
    "- **Equalized Odds post-processing** reduces accuracy (−1 pp), **increases DP disparity** (0.0997 → 0.1126), and leaves **EO unchanged**.  \n",
    "- **Demographic Parity post-processing** performs identically to EO: **accuracy loss** and **slightly worse DP**, with **no EO gains**.  \n",
    "- Both methods converge to the same solution, suggesting that **the post-processing optimizer did not effectively improve fairness** under current settings.  \n",
    "\n",
    "**Takeaway:** Post-processing **fails to improve fairness** and slightly **reduces accuracy**. The baseline MLP remains preferable.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d93504",
   "metadata": {},
   "source": [
    "## Overall Comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2cdfe2",
   "metadata": {},
   "source": [
    "## Overall Bias-Mitigation Comparison (Fairlearn) — Gender Bias in CVD Prediction  \n",
    "\n",
    "**Context:** All models are trained on **balanced datasets** (by gender and by CVD presence).  \n",
    "**Metric keys:**  \n",
    "- **DP diff** (Demographic Parity): selection-rate gap across genders (lower = fairer outcomes).  \n",
    "- **EO diff** (Equalized Odds): error-rate gap (TPR/FPR) across genders (lower = fairer errors).  \n",
    "\n",
    "---\n",
    "\n",
    "### Aggregated Summary of Bias Mitigation for all models\n",
    "\n",
    "| Model / Technique                          | Accuracy | DP diff | EO diff | Verdict |\n",
    "|--------------------------------------------|:--------:|:-------:|:-------:|---------|\n",
    "| **KNN Baseline (tuned)**                   | 0.9350   | 0.0150  | 0.0688  | Already strong; small DP, moderate EO |\n",
    "| **KNN + Post (DP/EO)**                     | 0.9350   | 0.0150  | 0.0688  | **No effect (0% flips)** |\n",
    "| **CorrelationRemover + KNN**               | 0.9300   | 0.0215  | 0.0531  | Slight acc drop; **EO improves**, DP worsens slightly |\n",
    "| **DT Baseline (tuned)**                    | 0.9400   | 0.0113  | 0.0444  | Very fair baseline (near parity) |\n",
    "| **DT + Post (EO)**                         | 0.9350   | 0.0104  | **0.0063** | **Best EO** for DT; accuracy −0.5 pp |\n",
    "| **DT + Post (DP)**                         | 0.9300   | **0.0017** | 0.0444  | **Best DP** for DT; EO unchanged |\n",
    "| **DT + EG (EO / DP)**                      | 0.9300   | 0.0017  | 0.0444  | DP near zero; EO baseline; acc −1 pp |\n",
    "| **DT + GS (EO / DP)**                      | 0.9400   | 0.0113  | 0.0444  | No change vs baseline |\n",
    "| **RF Baseline**                            | 0.9400   | 0.0373  | 0.0667  | Strong accuracy; small DP; moderate EO |\n",
    "| **RF + EG (EO / DP)**                      | 0.9400   | 0.0373  | 0.0667  | No effect (constraints not binding) |\n",
    "| **RF + GS (EO, i=29)**                     | **0.9600** | 0.0344  | **0.0436** | Higher acc; EO improves |\n",
    "| **RF + GS (EO, i=19/40)**                  | 0.9550   | 0.0308  | 0.0531  | Accuracy ↑, both DP & EO ↓ modestly |\n",
    "| **RF + GS (DP, i=19)**                     | 0.9450   | **0.0025** | 0.0375  | **Best DP** for RF, low EO |\n",
    "| **RF + GS (DP, i=29)**                     | 0.9450   | 0.0155  | 0.0667  | Similar to baseline |\n",
    "| **RF + GS (DP, i=40)**                     | 0.4500   | 1.0000  | 1.0000  | Degenerate solution (discard) |\n",
    "| **RF + Post (EO/DP)**                      | 0.9300   | 0.0062  | 0.0375  | Acc −1 pp; fairness improved vs baseline |\n",
    "| **MLP Baseline**                           | 0.9250   | 0.0997  | 0.1368  | Strong acc; **moderate DP/EO gaps** |\n",
    "| **MLP + EG (EO / DP)**                     | 0.9250   | 0.0997  | 0.1368  | No effect vs baseline |\n",
    "| **MLP + GS (EO)**                          | 0.9250   | 0.0997  | 0.1368  | No effect vs baseline |\n",
    "| **MLP + GS (DP)**                          | 0.9100   | 0.1344  | 0.1752  | Worse fairness **and** lower acc |\n",
    "| **MLP + Post (EO/DP)**                     | 0.9150   | 0.1126  | 0.1368  | Acc ↓, DP worsens slightly, EO unchanged |\n",
    "\n",
    "---\n",
    "\n",
    "### What worked\n",
    "\n",
    "- **Decision Tree Post-processing (EO):** Best for **error-rate fairness**; EO reduced from 0.0444 → **0.0063** with only −0.5 pp accuracy.  \n",
    "- **Decision Tree Post-processing (DP):** Achieves **near-perfect DP (0.0017)** with small acc trade-off.  \n",
    "- **Random Forest GridSearch (EO, i=29):** **Highest accuracy (0.9600)** with **EO improved** (0.0436).  \n",
    "- **Random Forest GridSearch (DP, i=19):** **Near-zero DP (0.0025)** while keeping EO moderate (0.0375).  \n",
    "- **RF Post-processing (EO/DP):** Improves both DP and EO compared to baseline, though at a 1 pp acc cost.  \n",
    "- **CorrelationRemover + KNN:** Reduced EO (0.0531 vs 0.0688 baseline) at the cost of slightly worse DP.  \n",
    "\n",
    "### What did not help\n",
    "\n",
    "- **Post-processing for KNN:** No label flips; scores too coarse for movement.  \n",
    "- **RF + EG (EO/DP):** No effect—likely constraints not binding on ensemble scores.  \n",
    "- **MLP in-processing (EG/GS):** No movement, or (GS-DP) worsened both accuracy and fairness.  \n",
    "- **MLP Post-processing:** Slight accuracy drop and **no fairness improvements**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Practical implications for gender bias in CVD prediction\n",
    "\n",
    "- **Error-rate parity (EO):**  \n",
    "  - **Best interpretable choice:** **DT + Post (EO)** (EO ≈ 0.0063, accuracy 0.9350).  \n",
    "  - **Best high-performing choice:** **RF + GS (EO, i=29)** (EO ≈ 0.0436, accuracy 0.9600).  \n",
    "\n",
    "- **Selection-rate parity (DP):**  \n",
    "  - **Best DT option:** **Post (DP)** with DP ≈ 0.0017.  \n",
    "  - **Best RF option:** **GS (DP, i=19)** with DP ≈ 0.0025, accuracy 0.9450.  \n",
    "\n",
    "- **KNN**: Only **CorrelationRemover** had meaningful effect; post-processing does not alter outputs.  \n",
    "\n",
    "- **MLP**: Baseline is already best — mitigation methods either had no effect or worsened results.  \n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Best overall for fairness + accuracy:**  \n",
    "   - **Random Forest + GridSearch (EO, i=29)**: high accuracy (0.9600), improved EO.  \n",
    "   - **Random Forest + GridSearch (DP, i=19)**: near-parity DP with solid EO and good accuracy (0.9450).  \n",
    "\n",
    "2. **Best interpretable model:**  \n",
    "   - **Decision Tree + Post (EO)** (EO ~0.0063).  \n",
    "   - Or **Decision Tree + Post (DP)** (DP ~0.0017).  \n",
    "\n",
    "3. **KNN:** viable with **CorrelationRemover** if EO is priority, but limited movement otherwise.  \n",
    "\n",
    "4. **MLP:** mitigation did not help; stick with baseline.  \n",
    "\n",
    "**Conclusion:**  \n",
    "In this **gender-balanced CVD dataset**, the strongest mitigation results come from **Decision Tree post-processing** (fairness gains at small cost) and **Random Forest frontier models (GridSearch)**, which deliver **joint improvements in accuracy and fairness**. These approaches reduce the risk of **systematic gender disparities** in CVD prediction while maintaining strong clinical utility.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
