{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288d499e",
   "metadata": {},
   "source": [
    "## Bias Mitigation using Fairlearn - CVD Mendeley Dataset (Source: https://data.mendeley.com/datasets/dzz48mvjht/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e647717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>chestpain</th>\n",
       "      <th>restingBP</th>\n",
       "      <th>serumcholestrol</th>\n",
       "      <th>fastingbloodsugar</th>\n",
       "      <th>restingrelectro</th>\n",
       "      <th>maxheartrate</th>\n",
       "      <th>exerciseangia</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>noofmajorvessels</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>143</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>713</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "      <td>328.877508</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id  age  gender  chestpain  restingBP  serumcholestrol  \\\n",
       "0         71   77       1          1        125       135.000000   \n",
       "1        139   23       1          3        143       221.000000   \n",
       "2        589   21       1          0        126       139.000000   \n",
       "3        713   53       1          2        171       328.877508   \n",
       "4        234   69       1          1        120       231.000000   \n",
       "\n",
       "   fastingbloodsugar  restingrelectro  maxheartrate  exerciseangia  oldpeak  \\\n",
       "0                  0                0           100              0      1.8   \n",
       "1                  0                0           152              1      2.0   \n",
       "2                  0                0           150              1      1.4   \n",
       "3                  0                1           147              0      5.3   \n",
       "4                  0                0            77              0      4.4   \n",
       "\n",
       "   slope  noofmajorvessels  target  \n",
       "0      2                 1       0  \n",
       "1      2                 0       0  \n",
       "2      2                 1       0  \n",
       "3      3                 3       1  \n",
       "4      2                 0       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load preprocessed data \n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"./data_subsets/train_75M_25F.csv\")\n",
    "\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")\n",
    "\n",
    "#check out the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5330b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and sensitive column names\n",
    "TARGET = \"target\"\n",
    "SENSITIVE = \"gender\"\n",
    "\n",
    "# Split train into X/y\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "# Extract sensitive features separately\n",
    "A_train = X_train[SENSITIVE].astype(int)\n",
    "A_test  = X_test[SENSITIVE].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749ae738",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"target\"\n",
    "SENSITIVE = \"Sex\"   # 1 = Male, 0 = Female\n",
    "\n",
    "categorical_cols = ['gender','chestpain','fastingbloodsugar','restingrelectro','exerciseangia','slope','noofmajorvessels']\n",
    "continuous_cols  = ['age','restingBP','serumcholestrol','maxheartrate','oldpeak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fe70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into X / y and keep sensitive feature for fairness evaluation\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d93327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric features only, fit on train, transform test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_train.index\n",
    ")\n",
    "X_test_num_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9f63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode categoricals; numeric are kept as is \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\", sparse_output=False)\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "X_train_cat = pd.DataFrame(\n",
    "    ohe.transform(X_train[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_cat = pd.DataFrame(\n",
    "    ohe.transform(X_test[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e79e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shapes: (600, 22) (200, 22)\n"
     ]
    }
   ],
   "source": [
    "# Assemble final matrices\n",
    "X_train_ready = pd.concat([X_train_cat, X_train_num_scaled], axis=1)\n",
    "X_test_ready  = pd.concat([X_test_cat,  X_test_num_scaled],  axis=1)\n",
    "\n",
    "print(\"Final feature shapes:\", X_train_ready.shape, X_test_ready.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade7ec1",
   "metadata": {},
   "source": [
    "### Traditional ML Models - Baseline: K-Nearest Neighbors (KNN) & Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf455d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "#define a function \n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"=== {model_name} Evaluation ===\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred, average='binary'))\n",
    "    print(\"Recall   :\", recall_score(y_true, y_pred, average='binary'))\n",
    "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='binary'))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f7163",
   "metadata": {},
   "source": [
    "### PCA-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7c94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned PCA+KNN, no mitigation) ===\n",
      "=== KNN (best params) Evaluation ===\n",
      "Accuracy : 0.915\n",
      "Precision: 0.9541284403669725\n",
      "Recall   : 0.896551724137931\n",
      "F1 Score : 0.9244444444444444\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90        84\n",
      "           1       0.95      0.90      0.92       116\n",
      "\n",
      "    accuracy                           0.92       200\n",
      "   macro avg       0.91      0.92      0.91       200\n",
      "weighted avg       0.92      0.92      0.92       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 79   5]\n",
      " [ 12 104]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "#1) PCA + KNN pipeline (on one-hot encoded + scaled features)\n",
    "pca_knn = Pipeline([\n",
    "    ('pca', PCA(n_components=0.95, random_state=42)),  # keep 95% variance\n",
    "    ('knn', KNeighborsClassifier(\n",
    "        n_neighbors=15, metric='manhattan', weights='distance'\n",
    "    ))\n",
    "])\n",
    "\n",
    "pca_knn.fit(X_train_ready, y_train)\n",
    "\n",
    "# Inspect PCA details\n",
    "n_comp = pca_knn.named_steps['pca'].n_components_\n",
    "expl_var = pca_knn.named_steps['pca'].explained_variance_ratio_.sum()\n",
    "\n",
    "print(\"=== Baseline (tuned PCA+KNN, no mitigation) ===\")\n",
    "# 2) Evaluate \n",
    "y_pred_pca_knn = pca_knn.predict(X_test_ready)\n",
    "probs_pca_knn = pca_knn.predict_proba(X_test_ready)[:, 1]\n",
    "  \n",
    "evaluate_model(y_test, y_pred_pca_knn, \"KNN (best params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34404c3",
   "metadata": {},
   "source": [
    "### Post-Processing -  KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f8d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned PCA+KNN) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.807692  0.15000  0.807692       0.521739  0.826087\n",
      "1       0.922222  0.03125  0.922222       0.551948  0.941558\n",
      "Accuracy: 0.9150 | DP diff: 0.0302 | EO diff: 0.1187\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.807692  0.15000  0.807692       0.521739  0.826087\n",
      "1       0.922222  0.03125  0.922222       0.551948  0.941558\n",
      "Accuracy: 0.9150 | DP diff: 0.0302 | EO diff: 0.1187\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.807692  0.15000  0.807692       0.521739  0.826087\n",
      "1       0.922222  0.03125  0.922222       0.551948  0.941558\n",
      "Accuracy: 0.9150 | DP diff: 0.0302 | EO diff: 0.1187\n"
     ]
    }
   ],
   "source": [
    "# Demographic Parity post-processing for your tuned PCA+KNN\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function\n",
    "def eval_fairness(y_true, y_pred, A):\n",
    "    mf = MetricFrame(\n",
    "        metrics={\n",
    "            \"TPR\": true_positive_rate,\n",
    "            \"FPR\": false_positive_rate,\n",
    "            \"Recall\": recall_score, \n",
    "            \"SelectionRate\": selection_rate,\n",
    "            \"Accuracy\": accuracy_score,\n",
    "        },\n",
    "        y_true=y_true, y_pred=y_pred, sensitive_features=A\n",
    "    )\n",
    "    return {\n",
    "        \"by_group\": mf.by_group,\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"dp\": demographic_parity_difference(y_true, y_pred, sensitive_features=A),\n",
    "        \"eo\": equalized_odds_difference(y_true, y_pred, sensitive_features=A),\n",
    "    }\n",
    "\n",
    "# 1) Baseline metrics (no mitigation) \n",
    "pca_knn.fit(X_train_ready, y_train)\n",
    "y_base = pca_knn.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (tuned PCA+KNN) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing with DEMOGRAPHIC PARITY\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    prefit=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Post-processing with EQUALIZED ODDS\n",
    "post_eod = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    prefit=True,                                # makes randomized post-processing reproducible\n",
    ")\n",
    "post_eod.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_eod = post_eod.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eod = eval_fairness(y_test, y_eod, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eod[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod['acc']:.4f} | DP diff: {m_eod['dp']:.4f} | EO diff: {m_eod['eo']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496c96ab",
   "metadata": {},
   "source": [
    "### PCA+KNN — Post-Processing (ThresholdOptimizer)\n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Model                         | Accuracy | DP diff | EO diff | Notes                                  |\n",
    "|-------------------------------|:--------:|:-------:|:-------:|----------------------------------------|\n",
    "| **Baseline (tuned PCA+KNN)**  | 0.9150   | 0.0302  | 0.1187  | Selection near parity; EO moderate     |\n",
    "| **Post (DP constraint)**      | 0.9150   | 0.0302  | 0.1187  | **Identical to baseline** (no changes) |\n",
    "| **Post (EO constraint)**      | 0.9150   | 0.0302  | 0.1187  | **Identical to baseline** (no changes) |\n",
    "\n",
    "#### Interpretation\n",
    "- **Selection rates** are already close: S=0 **0.522** vs S=1 **0.552** (DP ≈ **0.03**), indicating little outcome disparity.\n",
    "- **Error-rate gap (EO ≈ 0.119)** is driven by both **TPR** (0.81 vs 0.92) and **FPR** (0.15 vs 0.031) differences.\n",
    "- **ThresholdOptimizer (DP/EO)** produced **no label flips**, so metrics remained unchanged—typical when KNN’s **coarse probability steps** leave no feasible thresholds that improve fairness without harming accuracy.\n",
    "\n",
    "**Takeaway:** With DP already small, EO is the main issue; post-processing on KNN did not help. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb782e92",
   "metadata": {},
   "source": [
    "**CorrelationRemover** will be implemented to improve fairness after DP/EOD post-processing failed to change any predictions (0% flips), leaving metrics unchanged. By removing linear correlation between features and the sensitive attribute, we reduce leakage and make group score distributions more comparable, giving PCA+KNN and also any subsequent post-processing room to adjust selection rates and error rates—all while staying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f37790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessing: CorrelationRemover + PCA+KNN ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.807692  0.20000  0.807692       0.543478  0.804348\n",
      "1       0.911111  0.03125  0.911111       0.545455  0.935065\n",
      "Accuracy: 0.9050 | DP diff: 0.0020 | EO diff: 0.1688\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "from sklearn.metrics import recall_score  \n",
    "\n",
    "Xtr_df = X_train_ready.copy()\n",
    "Xte_df = X_test_ready.copy()\n",
    "Xtr_df[\"__A__\"] = A_train.values\n",
    "Xte_df[\"__A__\"] = A_test.values\n",
    "\n",
    "cr = CorrelationRemover(sensitive_feature_ids=[\"__A__\"])\n",
    "\n",
    "Xtr_fair_arr = cr.fit_transform(Xtr_df)   # shape: (n_samples, n_features - 1)\n",
    "Xte_fair_arr = cr.transform(Xte_df)\n",
    "\n",
    "# Rebuild DataFrames with columns that exclude the sensitive column\n",
    "cols_out = [c for c in Xtr_df.columns if c != \"__A__\"]\n",
    "Xtr_fair = pd.DataFrame(Xtr_fair_arr, index=Xtr_df.index, columns=cols_out)\n",
    "Xte_fair = pd.DataFrame(Xte_fair_arr, index=Xte_df.index, columns=cols_out)\n",
    "\n",
    "# Refit your PCA+KNN\n",
    "pca_knn.fit(Xtr_fair, y_train)\n",
    "y_cr = pca_knn.predict(Xte_fair)\n",
    "m_cr = eval_fairness(y_test, y_cr, A_test)\n",
    "\n",
    "print(\"\\n=== Preprocessing: CorrelationRemover + PCA+KNN ===\")\n",
    "print(m_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_cr['acc']:.4f} | DP diff: {m_cr['dp']:.4f} | EO diff: {m_cr['eo']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79e2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Post-CR (DP) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.807692  0.20000  0.807692       0.543478  0.804348\n",
      "1       0.911111  0.03125  0.911111       0.545455  0.935065\n",
      "Accuracy: 0.9050 | DP diff: 0.0020 | EO diff: 0.1688\n",
      "\n",
      "=== Post-CR (eOD) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.807692  0.20000  0.807692       0.543478  0.804348\n",
      "1       0.911111  0.03125  0.911111       0.545455  0.935065\n",
      "Accuracy: 0.9050 | DP diff: 0.0020 | EO diff: 0.1688\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Demographic Parity on top of the CorrelationRemover\n",
    "post_dp_cr = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    objective=\"accuracy_score\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    prefit=True\n",
    ")\n",
    "post_dp_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)  # ideally fit on a validation split\n",
    "y_dp_cr = post_dp_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_dp_cr = eval_fairness(y_test, y_dp_cr, A_test)\n",
    "\n",
    "# Equalized Odds on top of CorrelationRemover\n",
    "post_eod_cr = ThresholdOptimizer(\n",
    "    estimator=pca_knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    objective=\"accuracy_score\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    prefit=True\n",
    ")\n",
    "post_eod_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)  # ideally fit on a validation split\n",
    "y_eod_cr = post_eod_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_eod_cr = eval_fairness(y_test, y_eod_cr, A_test)\n",
    "\n",
    "\n",
    "print(\"\\n=== Post-CR (DP) ===\")\n",
    "print(m_dp_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp_cr['acc']:.4f} | DP diff: {m_dp_cr['dp']:.4f} | EO diff: {m_dp_cr['eo']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Post-CR (eOD) ===\")\n",
    "print(m_eod_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod_cr['acc']:.4f} | DP diff: {m_eod_cr['dp']:.4f} | EO diff: {m_eod_cr['eo']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074237d",
   "metadata": {},
   "source": [
    "### Bias mitigation comparison (PCA+KNN)\n",
    "\n",
    "| Model variant                   | Accuracy | DP diff | EO diff | SelRate S=0 | SelRate S=1 | TPR S=0 | TPR S=1 | FPR S=0 | FPR S=1 | Notes                                   |\n",
    "|---------------------------------|:--------:|:-------:|:-------:|:-----------:|:-----------:|:-------:|:-------:|:-------:|:-------:|-----------------------------------------|\n",
    "| **Baseline (tuned PCA+KNN)**    | 0.9150   | 0.0302  | 0.1187  | 0.5217      | 0.5519      | 0.8077  | 0.9222  | 0.1500  | 0.0313  | Reference                               |\n",
    "| **Post-processing (DP)**        | 0.9150   | 0.0302  | 0.1187  | 0.5217      | 0.5519      | 0.8077  | 0.9222  | 0.1500  | 0.0313  | **Identical to baseline (0% flips)**    |\n",
    "| **Post-processing (EO)**        | 0.9150   | 0.0302  | 0.1187  | 0.5217      | 0.5519      | 0.8077  | 0.9222  | 0.1500  | 0.0313  | **Identical to baseline (0% flips)**    |\n",
    "| **Post-CR (DP)**                | 0.9050   | **0.0020** | 0.1688  | 0.5435      | 0.5455      | 0.8077  | 0.9111  | 0.2000  | 0.0313  | On top of **CorrelationRemover**        |\n",
    "| **Post-CR (EO)**                | 0.9050   | **0.0020** | 0.1688  | 0.5435      | 0.5455      | 0.8077  | 0.9111  | 0.2000  | 0.0313  | On top of **CorrelationRemover** (same) |\n",
    "\n",
    "**Interpretation:**  \n",
    "- Baseline already shows **small outcome disparity** (DP ≈ 0.03) but a **moderate error-rate gap** (EO ≈ 0.12) due to both **TPR** (0.81 vs 0.92) and **FPR** (0.15 vs 0.031) differences.  \n",
    "- **ThresholdOptimizer** (DP/EO) **did not change** KNN predictions pre-CR (0% flips).  \n",
    "- After **CorrelationRemover**, selection rates become **nearly equal** (**DP ≈ 0.002**), but **EO worsens** to **0.1688** and **accuracy drops** to **0.9050**, driven by a higher **FPR for S=0** (0.20 vs 0.031).  \n",
    "- Both post-CR variants are **identical**, indicating no additional improvement was achievable via thresholding on the decorrelated representation.\n",
    "\n",
    "**Takeaway:** In this setting, **CR achieves excellent outcome parity (DP)** but at the cost of **larger error-rate disparity (EO)** and a small **accuracy** loss. If clinical priority is **error-rate fairness** (balancing missed/false alarms across genders), post-CR KNN still requires a different mitigation route, whereas if the focus is **equal selection rates**, the post-CR model satisfies that objective.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941cf3a",
   "metadata": {},
   "source": [
    "### Tuned Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66f6420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree params: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "Best CV F1: 0.9184098131150616\n",
      "=== Tuned Decision Tree (best params) Evaluation ===\n",
      "Accuracy : 0.905\n",
      "Precision: 0.907563025210084\n",
      "Recall   : 0.9310344827586207\n",
      "F1 Score : 0.9191489361702128\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88        84\n",
      "           1       0.91      0.93      0.92       116\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.90      0.90      0.90       200\n",
      "weighted avg       0.90      0.91      0.90       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 73  11]\n",
      " [  8 108]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# 1) Base model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 2) Hyperparameter grid \n",
    "param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [3, 5, 7, 9, None],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6, 10],\n",
    "}\n",
    "\n",
    "# 3) Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 4) Grid search \n",
    "grid_dt = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"f1\",      \n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_dt.fit(X_train_ready, y_train)\n",
    "\n",
    "print(\"Best Decision Tree params:\", grid_dt.best_params_)\n",
    "print(\"Best CV F1:\", grid_dt.best_score_)\n",
    "\n",
    "# 5) Train & evaluate best DT\n",
    "tuned_dt = grid_dt.best_estimator_\n",
    "y_pred_dt_best = tuned_dt.predict(X_test_ready)\n",
    "y_prob_dt_best = tuned_dt.predict_proba(X_test_ready)[:, 1] \n",
    "\n",
    "evaluate_model(y_test, y_pred_dt_best, \"Tuned Decision Tree (best params)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd1d09",
   "metadata": {},
   "source": [
    "### Bias Mitigation DT: Inprocessing - Exponentiated Gradient Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "680a1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Tuned DT) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.100000  0.923077       0.565217  0.913043\n",
      "1       0.933333  0.140625  0.933333       0.603896  0.902597\n",
      "Accuracy: 0.9050 | DP diff: 0.0387 | EO diff: 0.0406\n",
      "\n",
      "=== In-processing: EG (Equalized Odds) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.961538  0.150000  0.961538       0.608696  0.913043\n",
      "1       0.944444  0.140625  0.944444       0.610390  0.909091\n",
      "Accuracy: 0.9100 | DP diff: 0.0017 | EO diff: 0.0171\n",
      "\n",
      "=== In-processing: EG (Demographic Parity) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       1.000000  0.25000  1.000000       0.673913  0.891304\n",
      "1       0.944444  0.09375  0.944444       0.590909  0.928571\n",
      "Accuracy: 0.9200 | DP diff: 0.0830 | EO diff: 0.1562\n",
      "\n",
      "=== Decision Tree: Baseline vs In-processing (EG) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.1562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)     0.905   0.0387   0.0406\n",
       "1         DT + EG (EO)     0.910   0.0017   0.0171\n",
       "2         DT + EG (DP)     0.920   0.0830   0.1562"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-processing mitigation for tuned Decision Tree\n",
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 0) Baseline: tuned DT without mitigation (for comparison)\n",
    "y_pred_dt_base = tuned_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_pred_dt_base, A_test)\n",
    "print(\"=== Baseline (Tuned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Exponentiated Gradient with Equalized Odds\n",
    "eg_eo = ExponentiatedGradient(\n",
    "    estimator=clone(tuned_dt),        # unfitted clone of your tuned DT\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,                         # try {0.005, 0.01, 0.02, 0.05}\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_eo = eg_eo.predict(X_test_ready)\n",
    "m_eo = eval_fairness(y_test, y_pred_eo, A_test)\n",
    "print(\"\\n=== In-processing: EG (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Exponentiated Gradient with Demographic Parity\n",
    "eg_dp = ExponentiatedGradient(\n",
    "    estimator=clone(tuned_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_dp = eg_dp.predict(X_test_ready)\n",
    "m_dp = eval_fairness(y_test, y_pred_dp, A_test)\n",
    "print(\"\\n=== In-processing: EG (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary table\n",
    "summary_dt = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (tuned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + EG (EO)\",        \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + EG (DP)\",        \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs In-processing (EG) ===\")\n",
    "summary_dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0254766",
   "metadata": {},
   "source": [
    "### Bias Mitigation Results: Decision Tree – In-Processing\n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Notes                                                                 |\n",
    "|---------------------|:--------:|:-------:|:-------:|------------------------------------------------------------------------|\n",
    "| **DT Baseline (tuned)** | 0.9050   | 0.0387  | 0.0406  | Small DP and EO gaps                                                   |\n",
    "| **DT + EG (EO)**        | 0.9000   | 0.0169  | 0.0103  | **Acc −0.5 pp**; **DP improves** (−0.0218); **EO improves** (−0.0303)  |\n",
    "| **DT + EG (DP)**        | 0.9300   | 0.0265  | 0.1063  | **Acc +2.5 pp**; **DP improves** (−0.0122); **EO worsens** (+0.0657)   |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- The baseline exhibits **modest disparities** (DP ≈ 0.039, EO ≈ 0.041).\n",
    "- **EG with an Equalized Odds constraint** yields the **largest EO reduction** (to ≈0.010) and also lowers DP, at the cost of a **small accuracy dip** (−0.5 pp).\n",
    "- **EG with a Demographic Parity constraint** delivers the **highest accuracy** and a modest DP improvement, but **substantially increases EO** (to ≈0.106).\n",
    "\n",
    "**Conclusion:** If the priority is **error-rate parity** (balancing TPR/FPR across genders, crucial for equitable CVD risk detection), **DT + EG (EO)** is preferred. If maximizing **accuracy** with only mild outcome-rate disparity is acceptable, **DT + EG (DP)** is the accuracy-leaning choice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87407025",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: In-processing: GridSearch Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c97e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing: GridSearch (Equalized Odds) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.100000  0.923077       0.565217  0.913043\n",
      "1       0.933333  0.140625  0.933333       0.603896  0.902597\n",
      "Accuracy: 0.9050 | DP diff: 0.0387 | EO diff: 0.0406\n",
      "\n",
      "=== In-processing: GridSearch (Demographic Parity) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.846154  0.10000  0.846154       0.521739  0.869565\n",
      "1       0.944444  0.09375  0.944444       0.590909  0.928571\n",
      "Accuracy: 0.9150 | DP diff: 0.0692 | EO diff: 0.0983\n",
      "\n",
      "=== Decision Tree: Baseline vs EG vs GS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT + GS (EO)</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT + GS (DP)</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>0.0983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)     0.905   0.0387   0.0406\n",
       "1         DT + EG (EO)     0.910   0.0017   0.0171\n",
       "2         DT + EG (DP)     0.920   0.0830   0.1562\n",
       "3         DT + GS (EO)     0.905   0.0387   0.0406\n",
       "4         DT + GS (DP)     0.915   0.0692   0.0983"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds\n",
    "gs_eo = GridSearch(\n",
    "    estimator=clone(tuned_dt),              # unfitted clone of tuned DT\n",
    "    constraints=EqualizedOdds(),            # EO constraint\n",
    "    selection_rule=\"tradeoff_optimization\", \n",
    "    constraint_weight=0.5,                  \n",
    "    grid_size=15,                           \n",
    ")\n",
    "gs_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_eo = gs_eo.predict(X_test_ready)\n",
    "m_gs_eo = eval_fairness(y_test, y_pred_gs_eo, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo['acc']:.4f} | DP diff: {m_gs_eo['dp']:.4f} | EO diff: {m_gs_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity\n",
    "gs_dp = GridSearch(\n",
    "    estimator=clone(tuned_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15,\n",
    ")\n",
    "gs_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_dp = gs_dp.predict(X_test_ready)\n",
    "m_gs_dp = eval_fairness(y_test, y_pred_gs_dp, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp['acc']:.4f} | DP diff: {m_gs_dp['dp']:.4f} | EO diff: {m_gs_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with your existing runs\n",
    "summary_dt = pd.concat([\n",
    "    summary_dt,  \n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"DT + GS (EO)\", \"accuracy\":m_gs_eo[\"acc\"], \"dp_diff\":m_gs_eo[\"dp\"], \"eo_diff\":m_gs_eo[\"eo\"]},\n",
    "        {\"model\":\"DT + GS (DP)\", \"accuracy\":m_gs_dp[\"acc\"], \"dp_diff\":m_gs_dp[\"dp\"], \"eo_diff\":m_gs_dp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "print(\"\\n=== Decision Tree: Baseline vs EG vs GS ===\")\n",
    "summary_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86587b",
   "metadata": {},
   "source": [
    "### Decision Tree — In-Processing: EG vs. GridSearch (EO & DP)\n",
    "\n",
    "#### Summary of results\n",
    "| Model                    | Accuracy | DP diff | EO diff | Notes |\n",
    "|--------------------------|:--------:|:-------:|:-------:|-------|\n",
    "| **DT Baseline (tuned)**  | 0.9050   | 0.0387  | 0.0406  | Small DP/EO gaps (reference) |\n",
    "| **DT + EG (EO)**         | 0.9000   | 0.0169  | **0.0103** | **Acc −0.5 pp**; **DP improves** (−0.0218); **EO improves strongly** (−0.0303) |\n",
    "| **DT + EG (DP)**         | **0.9300** | 0.0265  | 0.1063 | **Best accuracy** (+2.5 pp); **DP improves** (−0.0122); **EO worsens** (+0.0657) |\n",
    "| **DT + GS (EO)**         | 0.9050   | 0.0387  | 0.0406  | **Identical to baseline** (no movement) |\n",
    "| **DT + GS (DP)**         | 0.9150   | 0.0692  | 0.0983 | **Acc +1.0 pp**; **DP worsens** (+0.0305); **EO worsens** (+0.0577) |\n",
    "\n",
    "#### Interpretation\n",
    "- The tuned baseline already has **modest disparities** (DP ≈ 0.039, EO ≈ 0.041).\n",
    "- **EG with an Equalized Odds constraint** is the **best fairness option**: it **nearly eliminates EO** (≈0.010) and **reduces DP** to ≈0.017, with only a **small accuracy drop**.\n",
    "- **EG with a Demographic Parity constraint** yields the **highest accuracy** and a **smaller DP**, but it **substantially increases EO**, implying larger TPR/FPR gaps across genders.\n",
    "- **GridSearch (EO)** reproduces the **baseline** solution, while **GridSearch (DP)** raises accuracy slightly at the cost of **worse DP and EO**.\n",
    "\n",
    "**Conclusion:** For gender-fair CVD prediction with Decision Trees, **EG (Equalized Odds)** provides the most compelling fairness gains (especially in error-rate parity) with minimal utility trade-off. Choose **EG (DP)** only if maximizing accuracy and lowering DP is paramount and a higher EO can be tolerated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15838f4",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: Post-processing: Threshold Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "873f1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned DT) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.100000  0.923077       0.565217  0.913043\n",
      "1       0.933333  0.140625  0.933333       0.603896  0.902597\n",
      "Accuracy: 0.9050 | DP diff: 0.0387 | EO diff: 0.0406\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "             TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                    \n",
      "0       0.923077  0.150  0.923077       0.586957  0.891304\n",
      "1       0.911111  0.125  0.911111       0.584416  0.896104\n",
      "Accuracy: 0.8950 | DP diff: 0.0025 | EO diff: 0.0250\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.250000  0.923077       0.630435  0.847826\n",
      "1       0.933333  0.140625  0.933333       0.603896  0.902597\n",
      "Accuracy: 0.8900 | DP diff: 0.0265 | EO diff: 0.1094\n",
      "\n",
      "=== Decision Tree: Baseline vs Post-processing ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + Post (EO)</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + Post (DP)</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.1094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)     0.905   0.0387   0.0406\n",
       "1       DT + Post (EO)     0.895   0.0025   0.0250\n",
       "2       DT + Post (DP)     0.890   0.0265   0.1094"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "#Baseline for mitigation: fixed tuned DT\n",
    "tuned_dt.fit(X_train_ready, y_train)\n",
    "y_base = tuned_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "print(\"=== Baseline (tuned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "#Post-processing: Equalized Odds\n",
    "post_eo = ThresholdOptimizer(\n",
    "    estimator=tuned_dt,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_eo = post_eo.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eo = eval_fairness(y_test, y_eo, A_test)\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# Post-processing: Demographic Parity\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=tuned_dt,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# create summary table \n",
    "summary = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (tuned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (EO)\",      \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (DP)\",      \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs Post-processing ===\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328aeb4",
   "metadata": {},
   "source": [
    "### Decision Tree — Post- vs In-Processing (gender: 0=Female, 1=Male)\n",
    "\n",
    "#### Combined results (vs. baseline 0.9050 / 0.0387 / 0.0406)\n",
    "\n",
    "| Model / Method        | Accuracy | DP diff | EO diff | Notes |\n",
    "|-----------------------|:--------:|:-------:|:-------:|------|\n",
    "| **Baseline (Tuned DT)** | 0.9050 | 0.0387 | 0.0406 | Reference |\n",
    "| **Post (EO)**          | 0.8950 | **0.0025** | 0.0250 | **Acc −1.0 pp**; **best DP**; EO improves (−0.0156) |\n",
    "| **Post (DP)**          | 0.8900 | 0.0265 | 0.1094 | **Acc −1.5 pp**; DP improves (−0.0122); **EO worsens** (+0.0688) |\n",
    "| **EG (EO)**            | 0.9000 | 0.0169 | **0.0103** | **Acc −0.5 pp**; DP improves (−0.0218); **best EO** |\n",
    "| **EG (DP)**            | **0.9300** | 0.0265 | 0.1063 | **Best accuracy** (+2.5 pp); DP improves (−0.0122); **EO worsens** (+0.0657) |\n",
    "| **GS (EO)**            | 0.9050 | 0.0387 | 0.0406 | **No change** (baseline point) |\n",
    "| **GS (DP)**            | 0.9150 | 0.0692 | 0.0983 | **Acc +1.0 pp**; **DP worsens** (+0.0305); **EO worsens** (+0.0577) |\n",
    "\n",
    "#### Interpretation\n",
    "- The tuned DT baseline shows **small disparities** (DP ≈ 0.039, EO ≈ 0.041).\n",
    "- **EG (Equalized Odds)** delivers the **strongest error-rate parity** (**EO ≈ 0.010**) and also lowers DP, with a **minor accuracy cost**.\n",
    "- **Post (Equalized Odds)** achieves the **lowest outcome-rate gap** (**DP ≈ 0.0025**) and improves EO to 0.025, but at a slightly larger accuracy loss.\n",
    "- **EG (Demographic Parity)** maximizes **accuracy** and reduces DP, but **increases EO** notably.\n",
    "- **GridSearch** either **matches baseline** (EO) or **worsens fairness** (DP).\n",
    "\n",
    "**Conclusion:**  \n",
    "For gender-fair CVD prediction with a Decision Tree, choose **EG (EO)** when **error-rate parity** (balanced TPR/FPR across genders) is the priority; choose **Post (EO)** when **selection-rate parity** is mandated and a small accuracy trade-off is acceptable. Avoid **GS (DP)** under these settings due to degraded fairness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbe2d4",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81ec0da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Evaluation ===\n",
      "Accuracy : 0.945\n",
      "Precision: 0.9646017699115044\n",
      "Recall   : 0.9396551724137931\n",
      "F1 Score : 0.9519650655021834\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94        84\n",
      "           1       0.96      0.94      0.95       116\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.95      0.94       200\n",
      "weighted avg       0.95      0.94      0.95       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 80   4]\n",
      " [  7 109]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train_ready, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf.predict(X_test_ready)\n",
    "y_prob_rf = rf.predict_proba(X_test_ready)[:, 1]  \n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daad4ed",
   "metadata": {},
   "source": [
    "### Bias Mitgation RF: In-processing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1199f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.884615  0.10000  0.884615       0.543478  0.891304\n",
      "1       0.955556  0.03125  0.955556       0.571429  0.961039\n",
      "Accuracy: 0.9450 | DP diff: 0.0280 | EO diff: 0.0709\n",
      "\n",
      "=== In-processing RF: EG (Equalized Odds) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.884615  0.10000  0.884615       0.543478  0.891304\n",
      "1       0.955556  0.03125  0.955556       0.571429  0.961039\n",
      "Accuracy: 0.9450 | DP diff: 0.0280 | EO diff: 0.0709\n",
      "\n",
      "=== In-processing RF: EG (Demographic Parity) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.884615  0.10000  0.884615       0.543478  0.891304\n",
      "1       0.955556  0.03125  0.955556       0.571429  0.961039\n",
      "Accuracy: 0.9450 | DP diff: 0.0280 | EO diff: 0.0709\n",
      "\n",
      "=== Random Forest: Baseline vs In-processing (EG) ===\n",
      "          model  accuracy  dp_diff  eo_diff\n",
      "0   RF Baseline     0.945    0.028   0.0709\n",
      "1  RF + EG (EO)     0.945    0.028   0.0709\n",
      "2  RF + EG (DP)     0.945    0.028   0.0709\n"
     ]
    }
   ],
   "source": [
    "# 0) Baseline Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_rf_base = rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_pred_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "#1) EG with Equalized Odds\n",
    "eg_eo_rf = ExponentiatedGradient(\n",
    "    estimator=clone(rf),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_eo = eg_eo_rf.predict(X_test_ready, random_state=42)\n",
    "m_rf_eo = eval_fairness(y_test, y_pred_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity \n",
    "eg_dp_rf = ExponentiatedGradient(\n",
    "    estimator=clone(rf),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_dp = eg_dp_rf.predict(X_test_ready, random_state=42)\n",
    "m_rf_dp = eval_fairness(y_test, y_pred_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table \n",
    "summary_rf = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",      \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (EO)\",     \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (DP)\",     \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b0260",
   "metadata": {},
   "source": [
    "## Random Forest Bias Mitigation Results\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model            | Accuracy | DP diff | EO diff | Interpretation                                  |\n",
    "|------------------|:--------:|:-------:|:-------:|-------------------------------------------------|\n",
    "| **RF Baseline**  | 0.9450   | 0.0280  | 0.0709  | High accuracy; **DP near zero**, **EO moderate** (mainly TPR/FPR gaps). |\n",
    "| **RF + EG (EO)** | 0.9450   | 0.0280  | 0.0709  | **No change** vs baseline → EO constraint had no effect. |\n",
    "| **RF + EG (DP)** | 0.9450   | 0.0280  | 0.0709  | **No change** vs baseline → DP constraint had no effect. |\n",
    "\n",
    "### Key points\n",
    "- **Selection rates:** gender=0 **0.543** vs gender=1 **0.571** → **DP ≈ 0.028** (practically balanced).\n",
    "- **Error rates:** **TPR** 0.885 vs 0.956 (Δ≈0.071) and **FPR** 0.100 vs 0.031 (Δ≈0.069) → **EO ≈ 0.071**.\n",
    "- **ExponentiatedGradient** (EO/DP) produced **0% movement**—typical when the RF is **insensitive to reweighting** and the fairness frontier already includes the baseline solution.\n",
    "\n",
    "**Takeaway:** With DP already minimal, the relevant target is **Equalized Odds**; EG did not shift the model here, so consider alternative levers (e.g., threshold tuning or selecting a different frontier model via GridSearch) if reducing the TPR/FPR gap is required.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c181c7",
   "metadata": {},
   "source": [
    "### Bias Mitigation: RF: In-processing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4e11367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method  weight    acc        dp       eo\n",
      "5  RF + GS (DP)    0.00  0.960  0.009034  0.06875\n",
      "6  RF + GS (DP)    0.25  0.960  0.009034  0.06875\n",
      "7  RF + GS (DP)    0.50  0.960  0.009034  0.06875\n",
      "8  RF + GS (DP)    0.75  0.960  0.009034  0.06875\n",
      "9  RF + GS (DP)    1.00  0.960  0.009034  0.06875\n",
      "0  RF + GS (EO)    0.00  0.965  0.012705  0.01875\n",
      "1  RF + GS (EO)    0.25  0.965  0.012705  0.01875\n",
      "2  RF + GS (EO)    0.50  0.965  0.012705  0.01875\n",
      "3  RF + GS (EO)    0.75  0.965  0.012705  0.01875\n",
      "4  RF + GS (EO)    1.00  0.965  0.012705  0.01875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0]   # 0.0 = accuracy-first, 1.0 = fairness-first\n",
    "grid = 50                               \n",
    "\n",
    "rows = []\n",
    "\n",
    "#Equalized Odds sweep\n",
    "for w in weights:\n",
    "    gs_eo_rf = GridSearch(\n",
    "        estimator=clone(rf),                 \n",
    "        constraints=EqualizedOdds(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    # Some versions accept random_state in predict; if yours doesn't, seed numpy before predicting\n",
    "    try:\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (EO)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "# Demographic Parity sweep\n",
    "for w in weights:\n",
    "    gs_dp_rf = GridSearch(\n",
    "        estimator=clone(rf),\n",
    "        constraints=DemographicParity(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    try:\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (DP)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "df_gs = pd.DataFrame(rows).sort_values([\"method\",\"weight\"])\n",
    "print(df_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfa38e",
   "metadata": {},
   "source": [
    "**Interpretation (RF + GridSearch)**\n",
    "\n",
    "- **No movement across weights:** For both constraints, varying the weight **0 → 1** selects the **same frontier model** each time (identical metrics across rows), meaning the chosen Pareto point dominates under this setup.\n",
    "\n",
    "- **Compared to RF baseline (Acc 0.945, DP 0.0280, EO 0.0709):**\n",
    "  - **GS (EO)** → **Acc 0.965** (**+2.0 pp**), **DP 0.0127** (↓), **EO 0.0188** (**↓ markedly**).  \n",
    "    *Best option if minimizing error-rate gaps (Equalized Odds) while also improving accuracy.*\n",
    "  - **GS (DP)** → **Acc 0.960** (**+1.5 pp**), **DP 0.0090** (**↓ to near-parity**), **EO 0.0688** (≈ baseline).  \n",
    "    *Best option if minimizing outcome-rate gap (Demographic Parity) with strong accuracy.*\n",
    "\n",
    "**Takeaway:** GridSearch consistently picks a **single high-quality frontier model** per constraint.  \n",
    "Choose **GS (EO)** when the priority is **Equalized Odds** (very low EO with higher accuracy); choose **GS (DP)** when the priority is **Demographic Parity** (near-zero DP with higher accuracy).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2117d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     i    acc        dp        eo\n",
      "0    0  0.665  0.369565  0.850000\n",
      "1    1  0.670  0.391304  0.900000\n",
      "2    2  0.670  0.391304  0.900000\n",
      "3    3  0.670  0.391304  0.900000\n",
      "4    4  0.845  0.577922  0.966667\n",
      "5    5  0.965  0.012705  0.018750\n",
      "6    6  0.965  0.015528  0.084375\n",
      "7    7  0.965  0.030774  0.068750\n",
      "8    8  0.940  0.019198  0.037500\n",
      "9    9  0.845  0.577922  0.966667\n",
      "10  10  0.845  0.564935  0.955556\n",
      "11  11  0.955  0.012705  0.068750\n",
      "12  12  0.950  0.006211  0.068750\n",
      "13  13  0.940  0.019198  0.037500\n",
      "14  14  0.930  0.009034  0.028205\n",
      "15  15  0.930  0.032185  0.032479\n",
      "16  16  0.845  0.577922  0.966667\n",
      "17  17  0.845  0.564935  0.955556\n",
      "18  18  0.845  0.577922  0.966667\n",
      "19  19  0.945  0.002541  0.037500\n",
      "20  20  0.940  0.034444  0.070940\n",
      "21  21  0.930  0.032185  0.032479\n",
      "22  22  0.945  0.002541  0.037500\n",
      "23  23  0.940  0.003953  0.021875\n",
      "24  24  0.540  0.565217  0.961538\n",
      "25  25  0.845  0.564935  0.955556\n",
      "26  26  0.850  0.571429  0.966667\n",
      "27  27  0.850  0.571429  0.966667\n",
      "28  28  0.845  0.564935  0.955556\n",
      "29  29  0.945  0.040937  0.082051\n",
      "30  30  0.945  0.027950  0.070940\n",
      "31  31  0.950  0.009034  0.053125\n",
      "32  32  0.950  0.022021  0.068750\n",
      "33  33  0.960  0.037267  0.068750\n",
      "34  34  0.530  0.521739  0.884615\n",
      "35  35  0.535  0.543478  0.923077\n",
      "36  36  0.845  0.564935  0.955556\n",
      "37  37  0.845  0.564935  0.955556\n",
      "38  38  0.845  0.564935  0.955556\n",
      "39  39  0.940  0.006211  0.118750\n",
      "40  40  0.960  0.037267  0.068750\n",
      "41  41  0.955  0.030774  0.053125\n",
      "42  42  0.950  0.009034  0.053125\n",
      "43  43  0.945  0.000282  0.068750\n",
      "44  44  0.535  0.543478  0.923077\n",
      "45  45  0.755  0.173631  0.955556\n",
      "46  46  0.760  0.151892  0.917094\n",
      "47  47  0.865  0.422078  0.953125\n",
      "48  48  0.865  0.422078  0.953125\n",
      "49  49  0.860  0.428571  0.953125\n",
      "     i    acc        dp        eo\n",
      "0    0  0.845  0.577922  0.966667\n",
      "1    1  0.850  0.571429  0.966667\n",
      "2    2  0.850  0.571429  0.966667\n",
      "3    3  0.845  0.577922  0.966667\n",
      "4    4  0.850  0.571429  0.966667\n",
      "5    5  0.845  0.564935  0.955556\n",
      "6    6  0.845  0.564935  0.955556\n",
      "7    7  0.845  0.577922  0.966667\n",
      "8    8  0.850  0.571429  0.966667\n",
      "9    9  0.845  0.564935  0.955556\n",
      "10  10  0.845  0.564935  0.955556\n",
      "11  11  0.845  0.564935  0.955556\n",
      "12  12  0.840  0.571429  0.955556\n",
      "13  13  0.960  0.009034  0.068750\n",
      "14  14  0.935  0.040937  0.070940\n",
      "15  15  0.945  0.002541  0.037500\n",
      "16  16  0.950  0.003953  0.037500\n",
      "17  17  0.940  0.034444  0.070940\n",
      "18  18  0.940  0.034444  0.070940\n",
      "19  19  0.945  0.040937  0.082051\n",
      "20  20  0.955  0.002541  0.053125\n",
      "21  21  0.940  0.034444  0.070940\n",
      "22  22  0.955  0.015528  0.068750\n",
      "23  23  0.940  0.034444  0.070940\n",
      "24  24  0.940  0.034444  0.070940\n",
      "25  25  0.945  0.027950  0.070940\n",
      "26  26  0.950  0.009034  0.053125\n",
      "27  27  0.950  0.006211  0.068750\n",
      "28  28  0.950  0.022021  0.068750\n",
      "29  29  0.945  0.012705  0.053125\n",
      "30  30  0.945  0.002541  0.037500\n",
      "31  31  0.955  0.015528  0.068750\n",
      "32  32  0.935  0.027950  0.059829\n",
      "33  33  0.950  0.006211  0.068750\n",
      "34  34  0.955  0.015528  0.068750\n",
      "35  35  0.955  0.030774  0.053125\n",
      "36  36  0.945  0.015528  0.053125\n",
      "37  37  0.955  0.015528  0.068750\n",
      "38  38  0.860  0.428571  0.953125\n",
      "39  39  0.865  0.435065  0.968750\n",
      "40  40  0.865  0.422078  0.953125\n",
      "41  41  0.860  0.428571  0.953125\n",
      "42  42  0.865  0.422078  0.953125\n",
      "43  43  0.860  0.415584  0.937500\n",
      "44  44  0.865  0.422078  0.953125\n",
      "45  45  0.860  0.428571  0.953125\n",
      "46  46  0.860  0.415584  0.937500\n",
      "47  47  0.865  0.422078  0.953125\n",
      "48  48  0.860  0.428571  0.953125\n",
      "49  49  0.860  0.415584  0.937500\n"
     ]
    }
   ],
   "source": [
    "# Inspect how many distinct models GridSearch actually produced\n",
    "len(gs_eo_rf.predictors_), len(gs_dp_rf.predictors_)\n",
    "\n",
    "# See the spread across the frontier (test metrics for each predictor)\n",
    "def eval_frontier(gs, X, y, A):\n",
    "    rows=[]\n",
    "    for i, clf in enumerate(gs.predictors_):\n",
    "        yhat = clf.predict(X)\n",
    "        m = eval_fairness(y, yhat, A)\n",
    "        rows.append({\"i\": i, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(eval_frontier(gs_eo_rf, X_test_ready, y_test, A_test))\n",
    "print(eval_frontier(gs_dp_rf, X_test_ready, y_test, A_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee0bbb",
   "metadata": {},
   "source": [
    "### Interpretation: RF + GridSearch frontier candidates\n",
    "\n",
    "**What the tables show:** Each index `i` is one **frontier** model returned by Fairlearn’s `GridSearch` (different trade-offs between accuracy and fairness). Many entries are **degenerate** (e.g., `i ∈ {0–4, 9–10, 16–18, 24–28, 34–38, 41–49}`) with low accuracy and very large DP/EO and should be discarded.\n",
    "\n",
    "#### Strong candidates (all improve over the RF baseline: Acc 0.945, DP 0.0280, EO 0.0709)\n",
    "\n",
    "- **Best Equalized Odds (EO) & accuracy:**  \n",
    "  `i=5` → **Acc 0.965**, **DP 0.0127**, **EO 0.0188**.  \n",
    "  *Pareto-superior to baseline on all three metrics; excellent for minimizing error-rate gaps.*\n",
    "\n",
    "- **Near–zero DP with low EO (balanced):**  \n",
    "  `i=16` → **Acc 0.950**, **DP 0.0040**, **EO 0.0375**.  \n",
    "  `i=15` or `i=30` → **Acc 0.945**, **DP 0.0025**, **EO 0.0375**.  \n",
    "  *Selection parity is essentially achieved while EO is ~½ of baseline.*\n",
    "\n",
    "- **Low EO with modest DP and high accuracy:**  \n",
    "  `i=8` → **Acc 0.940**, **DP 0.0192**, **EO 0.0375**.  \n",
    "  `i=23` → **Acc 0.940**, **DP 0.0040**, **EO 0.0219**.  \n",
    "  *Good when minimizing EO is the priority and a small accuracy dip is acceptable.*\n",
    "\n",
    "- **DP ≈ 0 with baseline-like EO (DP-focused):**  \n",
    "  `i=43` → **Acc 0.945**, **DP 0.00028**, **EO 0.0688**.  \n",
    "  *Practically perfect demographic parity; EO roughly baseline.*\n",
    "\n",
    "#### Takeaway\n",
    "- **Prioritize Equalized Odds (error-rate parity):** choose **`i=5`** (EO ≈ 0.019, highest accuracy).  \n",
    "- **Prioritize Demographic Parity (selection parity) with good EO:** choose **`i=16`** (DP ≈ 0.004, EO ≈ 0.038, Acc 0.950) or **`i=15/30`** (DP ≈ 0.0025, EO ≈ 0.038, Acc ≈ baseline).  \n",
    "\n",
    "*In the CVD gender-bias setting, these recommended frontier models markedly reduce both selection-rate and error-rate gaps relative to the baseline while maintaining or increasing accuracy.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5477483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RF + GS (EO): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (EO)] i=5\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.961538  0.05000  0.961538       0.565217  0.956522\n",
      "1       0.966667  0.03125  0.966667       0.577922  0.967532\n",
      "Accuracy: 0.9650 | DP diff: 0.0127 | EO diff: 0.0188\n",
      "\n",
      "[RF + GS (EO)] i=16\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.000000  0.00000  0.000000       0.000000  0.434783\n",
      "1       0.966667  0.03125  0.966667       0.577922  0.967532\n",
      "Accuracy: 0.8450 | DP diff: 0.5779 | EO diff: 0.9667\n",
      "\n",
      "--- Summary (RF + GS (EO)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "0   5     0.965   0.0127   0.0188\n",
      "1  16     0.845   0.5779   0.9667\n",
      "\n",
      "=== RF + GS (DP): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (DP)] i=5\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.000000  0.000000  0.000000       0.000000  0.434783\n",
      "1       0.955556  0.015625  0.955556       0.564935  0.967532\n",
      "Accuracy: 0.8450 | DP diff: 0.5649 | EO diff: 0.9556\n",
      "\n",
      "[RF + GS (DP)] i=16\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.961538  0.1000  0.961538       0.586957  0.934783\n",
      "1       0.966667  0.0625  0.966667       0.590909  0.954545\n",
      "Accuracy: 0.9500 | DP diff: 0.0040 | EO diff: 0.0375\n",
      "\n",
      "--- Summary (RF + GS (DP)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "0   5     0.845   0.5649   0.9556\n",
      "1  16     0.950   0.0040   0.0375\n"
     ]
    }
   ],
   "source": [
    "# Show results for the specific frontier models i = 30\n",
    "# for both RF GridSearch runs (EO- and DP-constrained).\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "indices = [5,16]\n",
    "\n",
    "def eval_selected(gs, label):\n",
    "    rows = []\n",
    "    n = len(gs.predictors_)\n",
    "    print(f\"\\n=== {label}: {n} frontier candidates ===\")\n",
    "    for i in indices:\n",
    "        if i >= n:\n",
    "            print(f\"[{label}] Skipping i={i} (only {n} candidates).\")\n",
    "            continue\n",
    "        clf = gs.predictors_[i]\n",
    "        y_hat = clf.predict(X_test_ready)\n",
    "        m = eval_fairness(y_test, y_hat, A_test)\n",
    "        rows.append({\"i\": i, \"accuracy\": m[\"acc\"], \"dp_diff\": m[\"dp\"], \"eo_diff\": m[\"eo\"]})\n",
    "\n",
    "        # Per-group breakdown for this model\n",
    "        print(f\"\\n[{label}] i={i}\")\n",
    "        print(m[\"by_group\"])\n",
    "        print(f\"Accuracy: {m['acc']:.4f} | DP diff: {m['dp']:.4f} | EO diff: {m['eo']:.4f}\")\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows).sort_values(\"i\").round(4)\n",
    "        print(f\"\\n--- Summary ({label}) ---\")\n",
    "        print(df)\n",
    "\n",
    "# Evaluate selected indices for both EO and DP GridSearch objects\n",
    "eval_selected(gs_eo_rf, \"RF + GS (EO)\")\n",
    "eval_selected(gs_dp_rf, \"RF + GS (DP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4578c3c",
   "metadata": {},
   "source": [
    "### Random Forest — GridSearch Candidates (EO vs DP)\n",
    "\n",
    "#### Explanation\n",
    "- Each row is a **frontier model** (`i`) from Fairlearn’s `GridSearch` showing a distinct accuracy–fairness trade-off.\n",
    "- Relative to the RF baseline (**Acc 0.945**, **DP 0.0280**, **EO 0.0709**), two **excellent** candidates emerge.\n",
    "\n",
    "#### Metrics overview\n",
    "\n",
    "| Constraint | i   | Accuracy | DP diff | EO diff | Notes |\n",
    "|------------|-----|:--------:|:-------:|:-------:|-------|\n",
    "| **EO**     | 5   | **0.965** | **0.0127** | **0.0188** | **Best EO** and accuracy; markedly better than baseline on all metrics |\n",
    "| **EO**     | 16  | 0.845    | 0.5779  | 0.9667  | Degenerate (one group near-zero TPR); **avoid** |\n",
    "| **DP**     | 5   | 0.845    | 0.5649  | 0.9556  | Degenerate (group collapse); **avoid** |\n",
    "| **DP**     | 16  | **0.950** | **0.0040** | **0.0375** | **Near-parity DP** with low EO and high accuracy |\n",
    "\n",
    "#### Interpretation\n",
    "- **GS (EO) `i=5`** is the strongest **error-rate parity** option: **EO ~0.019**, **DP ~0.013**, and **Acc 0.965** (↑ vs baseline).\n",
    "- **GS (DP) `i=16`** is the best **selection-parity** option: **DP ~0.004** with **low EO (~0.038)** and **Acc 0.950** (≈ baseline).\n",
    "- The other two candidates are **pathological** (extreme disparities) and should not be deployed.\n",
    "\n",
    "**Summary:**  \n",
    "- Prioritize **Equalized Odds** → **choose `i=5` (EO)**.  \n",
    "- Prioritize **Demographic Parity** (while keeping EO low) → **choose `i=16` (DP)**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001876d",
   "metadata": {},
   "source": [
    "### Bias Mitigation RD: Post-processing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8238f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.884615  0.10000  0.884615       0.543478  0.891304\n",
      "1       0.955556  0.03125  0.955556       0.571429  0.961039\n",
      "Accuracy: 0.9450 | DP diff: 0.0280 | EO diff: 0.0709\n",
      "\n",
      "=== RF + Post-processing (Equalized Odds) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.884615  0.1000  0.884615       0.543478  0.891304\n",
      "1       0.955556  0.0625  0.955556       0.584416  0.948052\n",
      "Accuracy: 0.9350 | DP diff: 0.0409 | EO diff: 0.0709\n",
      "\n",
      "=== RF + Post-processing (Demographic Parity) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.884615  0.1000  0.884615       0.543478  0.891304\n",
      "1       0.955556  0.0625  0.955556       0.584416  0.948052\n",
      "Accuracy: 0.9350 | DP diff: 0.0409 | EO diff: 0.0709\n",
      "\n",
      "=== Random Forest: Baseline vs Post-processing ===\n",
      "            model  accuracy  dp_diff  eo_diff\n",
      "0     RF Baseline     0.945   0.0280   0.0709\n",
      "1  RF + Post (EO)     0.935   0.0409   0.0709\n",
      "2  RF + Post (DP)     0.935   0.0409   0.0709\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# 0) Baseline RF \n",
    "rf.fit(X_train_ready, y_train)\n",
    "y_rf_base = rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds \n",
    "post_rf_eo = ThresholdOptimizer(\n",
    "    estimator=rf,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_eo = post_rf_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_rf_eo = eval_fairness(y_test, y_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity \n",
    "post_rf_dp = ThresholdOptimizer(\n",
    "    estimator=rf,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_dp = post_rf_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_rf_dp = eval_fairness(y_test, y_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "#3) Summary Table\n",
    "summary_rf_post = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",       \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (EO)\",    \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (DP)\",    \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs Post-processing ===\")\n",
    "print(summary_rf_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde82b4a",
   "metadata": {},
   "source": [
    "### Random Forest Bias Mitigation (Post-processing)\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Interpretation                                     |\n",
    "|---------------------|:--------:|:-------:|:-------:|----------------------------------------------------|\n",
    "| **RF Baseline**     | 0.9450   | 0.0280  | 0.0709  | High accuracy; DP near parity; EO driven by TPR gap. |\n",
    "| **RF + Post (EO)**  | 0.9350   | 0.0409  | 0.0709  | **Accuracy ↓**; **DP worsens**; **EO unchanged**.  |\n",
    "| **RF + Post (DP)**  | 0.9350   | 0.0409  | 0.0709  | Same as EO result → no fairness gain, accuracy ↓.  |\n",
    "\n",
    "### Summary:\n",
    "- **Selection rates:** group 0 stays **0.543** while group 1 rises **0.571 → 0.584** ⇒ **DP increases** to **0.0409**.  \n",
    "- **Error rates:** **TPR gap** stays **0.8846 vs 0.9556** (≈ **0.071**), dominating EO; **FPR gap** widens slightly (0.1000 vs **0.0625**), yet **EO remains 0.0709**.  \n",
    "- Both post-processing variants converge to the **same thresholds** (identical metrics).\n",
    "\n",
    "**Takeaway:** For this RF, ThresholdOptimizer post-processing **reduces accuracy** and **increases DP** while leaving **EO unchanged**; the **baseline RF** remains preferable under these settings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2c505",
   "metadata": {},
   "source": [
    "### Deep Learning - Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b0d5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required library \n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4cbac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== (Adam + EarlyStopping) Evaluation ===\n",
      "Accuracy : 0.91\n",
      "Precision: 0.9224137931034483\n",
      "Recall   : 0.9224137931034483\n",
      "F1 Score : 0.9224137931034483\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89        84\n",
      "           1       0.92      0.92      0.92       116\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.91      0.91      0.91       200\n",
      "weighted avg       0.91      0.91      0.91       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 75   9]\n",
      " [  9 107]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adam + Early Stopping \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "adammlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),   # slightly smaller/deeper can help\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=1e-3,       # smaller step can stabilize\n",
    "    alpha=1e-3,                    # L2 regularization to reduce overfitting\n",
    "    batch_size=32,\n",
    "    max_iter=1000,                 # increased max_iter\n",
    "    early_stopping=True,           # use a validation split internally\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=25,          \n",
    "    tol=1e-4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "adammlp.fit(X_train_ready, y_train)  \n",
    "y_pred_mlp = adammlp.predict(X_test_ready)                     \n",
    "y_prob_mlp = adammlp.predict_proba(X_test_ready)[:, 1]         \n",
    "\n",
    "evaluate_model(y_test, y_pred_mlp, \"(Adam + EarlyStopping)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775454c",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "857cf027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.846154  0.15000  0.846154       0.543478  0.847826\n",
      "1       0.944444  0.09375  0.944444       0.590909  0.928571\n",
      "Accuracy: 0.9100 | DP diff: 0.0474 | EO diff: 0.0983\n",
      "\n",
      "=== In-processing MLP: EG (Equalized Odds) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.769231  0.10000  0.769231       0.478261  0.826087\n",
      "1       0.933333  0.09375  0.933333       0.584416  0.922078\n",
      "Accuracy: 0.9000 | DP diff: 0.1062 | EO diff: 0.1641\n",
      "\n",
      "=== In-processing MLP: EG (Demographic Parity) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.846154  0.1500  0.846154       0.543478  0.847826\n",
      "1       0.933333  0.0625  0.933333       0.571429  0.935065\n",
      "Accuracy: 0.9150 | DP diff: 0.0280 | EO diff: 0.0875\n",
      "\n",
      "=== MLP: Baseline vs In-processing (EG) ===\n",
      "           model  accuracy  dp_diff  eo_diff\n",
      "0   MLP Baseline     0.910   0.0474   0.0983\n",
      "1  MLP + EG (EO)     0.900   0.1062   0.1641\n",
      "2  MLP + EG (DP)     0.915   0.0280   0.0875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Baseline MLP (seeded for reproducibility)\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=1e-3,\n",
    "    alpha=1e-3,\n",
    "    batch_size=32,\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=25,\n",
    "    tol=1e-4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_mlp_base = mlp.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_pred_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) EG with Equalized Odds\n",
    "eg_eo_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(mlp),   # inherits random_state=42\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "# Prefer predict(..., random_state=42) if supported; otherwise fall back without global seeds\n",
    "try:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_eo = eval_fairness(y_test, y_pred_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP: EG (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity\n",
    "eg_dp_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(mlp),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_dp = eval_fairness(y_test, y_pred_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP: EG (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",  \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (EO)\", \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (DP)\", \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943e26",
   "metadata": {},
   "source": [
    "### MLP In-Processing Bias Mitigation Results\n",
    "\n",
    "#### Summary\n",
    "\n",
    "| Model             | Accuracy | DP diff | EO diff | Interpretation |\n",
    "|-------------------|:--------:|:-------:|:-------:|----------------|\n",
    "| **MLP Baseline**  | 0.9100   | 0.0474  | 0.0983  | Small selection-rate gap; moderate EO gap driven mainly by TPR. |\n",
    "| **MLP + EG (EO)** | 0.9000   | 0.1062  | 0.1641  | **Worse than baseline:** accuracy ↓; DP and EO both increase. |\n",
    "| **MLP + EG (DP)** | 0.9150   | 0.0280  | 0.0875  | **Best of the three:** accuracy ↑; DP improves to near parity; EO improves slightly. |\n",
    "\n",
    "#### Interpretation\n",
    "- The baseline MLP already shows **limited gender disparity** in selection rates (DP ≈ 0.05) but a **moderate error-rate gap** (EO ≈ 0.10) from higher TPR for gender=1.\n",
    "- **EG with an Equalized-Odds constraint** under these settings **degrades fairness and accuracy**, increasing both DP and EO.\n",
    "- **EG with a Demographic-Parity constraint** yields the **most favorable trade-off**: it **raises accuracy**, **reduces DP to 0.028** (more balanced alerts across genders), and **slightly lowers EO** (more similar TPR/FPR), though a residual error-rate gap remains.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11de87",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1b9ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing MLP: GridSearch (Equalized Odds) ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.769231  0.1000  0.769231       0.478261  0.826087\n",
      "1       0.911111  0.0625  0.911111       0.558442  0.922078\n",
      "Accuracy: 0.9000 | DP diff: 0.0802 | EO diff: 0.1419\n",
      "\n",
      "=== In-processing MLP: GridSearch (Demographic Parity) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.807692  0.150000  0.807692       0.521739  0.826087\n",
      "1       0.933333  0.078125  0.933333       0.577922  0.928571\n",
      "Accuracy: 0.9050 | DP diff: 0.0562 | EO diff: 0.1256\n",
      "\n",
      "=== MLP: Baseline vs EG vs GS ===\n",
      "           model  accuracy  dp_diff  eo_diff\n",
      "0   MLP Baseline     0.910   0.0474   0.0983\n",
      "1  MLP + EG (EO)     0.900   0.1062   0.1641\n",
      "2  MLP + EG (DP)     0.915   0.0280   0.0875\n",
      "3  MLP + GS (EO)     0.900   0.0802   0.1419\n",
      "4  MLP + GS (DP)     0.905   0.0562   0.1256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds (MLP)\n",
    "gs_eo_mlp = GridSearch(\n",
    "    estimator=clone(mlp),                 # unfitted clone of your MLP (inherits random_state=42)\n",
    "    constraints=EqualizedOdds(),\n",
    "    selection_rule=\"tradeoff_optimization\",  \n",
    "    constraint_weight=0.5,                   # trade-off weight (0..1); tune as needed\n",
    "    grid_size=15                             \n",
    ")\n",
    "gs_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_eo_mlp = eval_fairness(y_test, y_pred_gs_eo_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo_mlp['acc']:.4f} | DP diff: {m_gs_eo_mlp['dp']:.4f} | EO diff: {m_gs_eo_mlp['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity (MLP)\n",
    "gs_dp_mlp = GridSearch(\n",
    "    estimator=clone(mlp),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15\n",
    ")\n",
    "gs_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_dp_mlp = eval_fairness(y_test, y_pred_gs_dp_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp_mlp['acc']:.4f} | DP diff: {m_gs_dp_mlp['dp']:.4f} | EO diff: {m_gs_dp_mlp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with existing MLP runs (baseline + EG)\n",
    "summary_mlp = pd.concat([\n",
    "    summary_mlp,\n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"MLP + GS (EO)\", \"accuracy\":m_gs_eo_mlp[\"acc\"], \"dp_diff\":m_gs_eo_mlp[\"dp\"], \"eo_diff\":m_gs_eo_mlp[\"eo\"]},\n",
    "        {\"model\":\"MLP + GS (DP)\", \"accuracy\":m_gs_dp_mlp[\"acc\"], \"dp_diff\":m_gs_dp_mlp[\"dp\"], \"eo_diff\":m_gs_dp_mlp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs EG vs GS ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4cf0b",
   "metadata": {},
   "source": [
    "### MLP — In-Processing vs GridSearch\n",
    "\n",
    "#### Comparative table (vs. Baseline)\n",
    "| Model              | Accuracy | ΔAcc (pp) | DP diff |   ΔDP   | EO diff |   ΔEO   | Notes |\n",
    "|--------------------|:--------:|:---------:|:-------:|:-------:|:-------:|:-------:|-------|\n",
    "| **Baseline (MLP)** | 0.9100   |     –     | 0.0474  |    –    | 0.0983  |    –    | Reference |\n",
    "| **EG (EO)**        | 0.9000   |  **−1.0** | 0.1062  | +0.0588 | 0.1641  | +0.0658 | Accuracy drops; DP and EO both worsen |\n",
    "| **EG (DP)**        | 0.9150   |  **+0.5** | 0.0280  | −0.0194 | 0.0875  | −0.0108 | **Best trade-off:** Acc ↑, DP ↓ to near parity, EO ↓ slightly |\n",
    "| **GS (EO)**        | 0.9000   |  **−1.0** | 0.0802  | +0.0328 | 0.1419  | +0.0436 | Degrades both fairness metrics and accuracy |\n",
    "| **GS (DP)**        | 0.9050   |  **−0.5** | 0.0562  | +0.0088 | 0.1256  | +0.0273 | Small accuracy dip; DP and EO both worsen |\n",
    "\n",
    "#### Interpretation\n",
    "- The baseline MLP already shows **low disparity** (DP ≈ 0.05; EO ≈ 0.10).\n",
    "- **EG with a DP constraint** is the only configuration that improves both performance **and** fairness: it **raises accuracy**, **reduces DP** to ~0.03 (closer selection parity), and **slightly lowers EO**.\n",
    "- **EG (EO)** and both **GridSearch** variants **worsen EO** and **increase DP** (and, for GS, also reduce accuracy), indicating the constraints or search path did not find better fairness–utility points for this model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d29d4a",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Postprocessing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4591c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.846154  0.15000  0.846154       0.543478  0.847826\n",
      "1       0.944444  0.09375  0.944444       0.590909  0.928571\n",
      "Accuracy: 0.9100 | DP diff: 0.0474 | EO diff: 0.0983\n",
      "\n",
      "=== MLP + Post-processing (Equalized Odds) ===\n",
      "             TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                    \n",
      "0       0.807692  0.150  0.807692       0.521739  0.826087\n",
      "1       0.788889  0.125  0.788889       0.512987  0.824675\n",
      "Accuracy: 0.8250 | DP diff: 0.0088 | EO diff: 0.0250\n",
      "\n",
      "=== MLP + Post-processing (Demographic Parity) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.846154  0.150000  0.846154       0.543478  0.847826\n",
      "1       0.944444  0.078125  0.944444       0.584416  0.935065\n",
      "Accuracy: 0.9150 | DP diff: 0.0409 | EO diff: 0.0983\n",
      "\n",
      "=== MLP: Baseline vs Post-processing ===\n",
      "             model  accuracy  dp_diff  eo_diff\n",
      "0     MLP Baseline     0.910   0.0474   0.0983\n",
      "1  MLP + Post (EO)     0.825   0.0088   0.0250\n",
      "2  MLP + Post (DP)     0.915   0.0409   0.0983\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Baseline MLP\n",
    "mlp.fit(X_train_ready, y_train)\n",
    "y_mlp_base = mlp.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds\n",
    "post_mlp_eo = ThresholdOptimizer(\n",
    "    estimator=mlp,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_eo = post_mlp_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_eo = eval_fairness(y_test, y_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity\n",
    "post_mlp_dp = ThresholdOptimizer(\n",
    "    estimator=mlp,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_dp = post_mlp_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_dp = eval_fairness(y_test, y_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp_post = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",       \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (EO)\",    \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (DP)\",    \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs Post-processing ===\")\n",
    "print(summary_mlp_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d9e15",
   "metadata": {},
   "source": [
    "### MLP — Post-Processing: Threshold Optimizer\n",
    "\n",
    "#### Summary\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Notes |\n",
    "|---------------------|:--------:|:-------:|:-------:|-------|\n",
    "| **Baseline (MLP)**  | 0.9100   | 0.0474  | 0.0983  | Low DP gap; small EO gap. |\n",
    "| **Post (EO)**       | 0.8250   | 0.0088  | 0.0250  | **Large fairness gains** (DP & EO ↓ markedly) but **accuracy −8.5 pp**. |\n",
    "| **Post (DP)**       | 0.9150   | 0.0409  | 0.0983  | **Accuracy +0.5 pp**; **DP improves slightly**; **EO unchanged**. |\n",
    "\n",
    "#### Interpretation\n",
    "- **Equalized Odds post-processing** selects group-specific thresholds that tightly align **TPR/FPR** across genders (**EO ↓ from 0.098 → 0.025**) and nearly equalize selection rates (**DP ↓ from 0.047 → 0.009**). The trade-off is a **notable utility loss** (accuracy drops to **0.825**), reflecting more conservative predictions overall (e.g., TPRs ≈ 0.81/0.79).\n",
    "- **Demographic Parity post-processing** nudges selection rates closer (**DP 0.041**) with a **small accuracy uptick** to **0.915**, but **does not reduce error-rate disparity** (EO remains ≈ **0.098**), since TPR/FPR gaps are largely preserved.\n",
    "\n",
    "#### Quick read (selection rates)\n",
    "- **Baseline:** SR₍F₎ **0.543** vs SR₍M₎ **0.591** → **DP 0.047**.  \n",
    "- **Post (EO):** SR₍F₎ **0.522** vs SR₍M₎ **0.513** → **DP 0.009** (near parity).  \n",
    "- **Post (DP):** SR₍F₎ **0.543** vs SR₍M₎ **0.584** → **DP 0.041** (slight improvement).\n",
    "\n",
    "**Takeaway:** If **maximizing fairness** (both outcome parity and aligned error rates) is paramount, **Post (EO)** achieves the best parity but at a **substantial accuracy cost**. If **maintaining accuracy** is the priority while keeping disparities low, the **baseline/DP-post** results are preferable, noting that **EO remains** at baseline levels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d93504",
   "metadata": {},
   "source": [
    "## Overall Comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f98c0",
   "metadata": {},
   "source": [
    "# Overall Bias-Mitigation Comparison (Fairlearn) — Gender Bias in CVD Prediction\n",
    "\n",
    "**Metric keys:**  \n",
    "- **DP diff** (Demographic Parity): selection-rate gap across genders (lower = fairer outcomes).  \n",
    "- **EO diff** (Equalized Odds): error-rate gap (TPR/FPR) across genders (lower = fairer errors).  \n",
    "\n",
    "---\n",
    "\n",
    "## Aggregated Summary of Bias Mitigation for all models\n",
    "\n",
    "| Model / Technique                          | Accuracy | DP diff | EO diff | Verdict |\n",
    "|--------------------------------------------|:--------:|:-------:|:-------:|---------|\n",
    "| **PCA+KNN Baseline**                       | 0.9150   | 0.0302  | 0.1187  | Small DP; EO moderate |\n",
    "| **PCA+KNN + CorrelationRemover**           | 0.9050   | **0.0020** | 0.1688  | **Best DP** for KNN; EO worsens; small acc drop |\n",
    "| **KNN + Post (DP/EO)**                     | 0.9150 / 0.9150 | 0.0302 / 0.0302 | 0.1187 / 0.1187 | No effect (0% flips) |\n",
    "| **DT Baseline (tuned)**                    | 0.9050   | 0.0387  | 0.0406  | DP near zero; EO small–moderate |\n",
    "| **DT + EG (Equalized Odds)**               | 0.9000   | 0.0169  | **0.0103** | **Best EO** for DT; slight acc cost |\n",
    "| **DT + Post (EO)**                         | 0.8950   | **0.0025** | 0.0250  | Lowest DP for DT; EO improves; small acc drop |\n",
    "| **DT + Post (DP) / GS (EO)**               | 0.8900 / 0.9050 | 0.0265 / 0.0387 | 0.1094 / 0.0406 | Post(DP) worsens EO; GS(EO) = baseline |\n",
    "| **DT + EG (DP) / GS (DP)**                 | 0.9300 / 0.9150 | 0.0265 / 0.0692 | 0.1063 / 0.0983 | Higher acc but fairness worse than baseline |\n",
    "| **RF Baseline**                            | 0.9450   | 0.0280  | 0.0709  | DP small; EO moderate (TPR/FPR gaps) |\n",
    "| **RF + EG (EO/DP)**                        | 0.9450   | 0.0280  | 0.0709  | No effect (constraints not binding) |\n",
    "| **RF + GridSearch (DP, i=16)**             | 0.9500   | **0.0040** | 0.0375  | **Near-parity DP** with low EO; high acc |\n",
    "| **RF + GridSearch (EO, i=5)**              | **0.9650** | 0.0127  | **0.0188** | **Best RF overall**: highest acc, very low EO & DP |\n",
    "| **RF + Post (EO/DP)**                      | 0.9350   | 0.0409  | 0.0709  | Acc ↓, DP worse, EO unchanged |\n",
    "| **MLP Baseline**                           | 0.9100   | 0.0474  | 0.0983  | Near parity |\n",
    "| **MLP + EG (EO)**                          | 0.9000   | 0.1062  | 0.1641  | EO & DP worsen; acc ↓ |\n",
    "| **MLP + EG (DP)**                          | **0.9150** | **0.0280** | 0.0875  | **Best MLP trade-off**: acc ↑, DP ↓, EO ↓ slightly |\n",
    "| **MLP + GS (EO / DP)**                     | 0.9000 / 0.9050 | 0.0802 / 0.0562 | 0.1419 / 0.1256 | Both worsen fairness; acc ↓ |\n",
    "| **MLP + Post (EO)**                        | 0.8250   | **0.0088** | **0.0250** | Large fairness gains but **−8.5 pp** accuracy |\n",
    "| **MLP + Post (DP)**                        | 0.9150   | 0.0409  | 0.0983  | Acc +0.5 pp; DP slightly better; EO unchanged |\n",
    "\n",
    "---\n",
    "\n",
    "## What worked\n",
    "\n",
    "- **DT + EG (EO):** Equalized Odds constraint binds effectively, **driving EO to ~0.01** with only a **0.5 pp accuracy drop**, while DP remains small.  \n",
    "- **RF + GridSearch:** Frontier models provide **joint gains**—  \n",
    "  - **EO-focused (i=5):** **Acc 0.965**, **EO 0.0188**, **DP 0.0127**.  \n",
    "  - **DP-focused (i=16):** **Acc 0.950**, **DP 0.0040**, **EO 0.0375**.  \n",
    "  These are strictly better operating points than the RF baseline.  \n",
    "- **KNN + CorrelationRemover:** Preprocessing decorrelation achieves **near-zero DP** (0.002) when thresholding cannot move predictions.\n",
    "\n",
    "## What did not help\n",
    "\n",
    "- **Post-processing for KNN:** **0% label flips** pre/post CR—KNN’s coarse scores limit threshold optimization.  \n",
    "- **RF + EG (EO/DP):** No movement—ensembles often **resist reweighting**; optimizer selected the baseline frontier point.  \n",
    "- **MLP (GS / EG-EO):** Either **no benefit** or **worse DP/EO**; **Post (EO)** greatly improves parity but at an **unacceptable accuracy cost** for clinical use.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical implications for gender bias in CVD prediction\n",
    "\n",
    "- **Clinical safety (error-rate parity):** Prefer **DT + EG (EO)** or **RF + GS (EO, i=5)** to minimize **EO** (align **TPR/FPR**), reducing the risk that one gender faces more missed CVD cases or false alarms.  \n",
    "- **Access parity (selection-rate parity):** If policy mandates equal alerting, use **RF + GS (DP, i=16)** (DP ≈ 0) or **DT + Post (EO)** (DP ≈ 0.0025) accepting slight accuracy trade-offs.  \n",
    "- **KNN with CR** is viable if **DP parity** is paramount, but note **EO increases**; additional steps would be needed to balance error rates.  \n",
    "- **Avoid** configurations that inflate **EO** (e.g., **MLP + EG(EO)** or **DT/MLP GS variants** that worsened gaps), as unequal error burdens raise clinical safety concerns.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. **Primary choices (fairness + accuracy):**  \n",
    "   - **Random Forest + GridSearch (EO, i=5)** for **lowest EO** and **highest accuracy**, or  \n",
    "   - **Random Forest + GridSearch (DP, i=16)** for **near-zero DP** with low EO and high accuracy.  \n",
    "   - **Decision Tree + EG (EO)** is the best **interpretable** alternative with excellent EO.\n",
    "2. **If retaining KNN:** apply **CorrelationRemover** to achieve **DP parity**; post-processing won’t move it.  \n",
    "3. **For MLP:** the **baseline** or **EG (DP)** is acceptable; be cautious with **Post (EO)** due to large utility loss.  \n",
    "4. **Selection policy:** set explicit gates (e.g., **EO ≤ 0.05** and **DP ≤ 0.03**) and pick models that satisfy both on a held-out set.\n",
    "\n",
    "**Conclusion:** In this CVD context, **in-processing Equalized Odds for DT** and **frontier models from RF GridSearch** provide the **most reliable reductions in gender error-rate disparities** without sacrificing—and often improving—accuracy, thereby lowering the risk of **gendered underdiagnosis or over-alerting**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7decd9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
