{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288d499e",
   "metadata": {},
   "source": [
    "## Bias Mitigation using Fairlearn - CVD Mendeley Dataset (Source: https://data.mendeley.com/datasets/dzz48mvjht/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e647717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>chestpain</th>\n",
       "      <th>restingBP</th>\n",
       "      <th>serumcholestrol</th>\n",
       "      <th>fastingbloodsugar</th>\n",
       "      <th>restingrelectro</th>\n",
       "      <th>maxheartrate</th>\n",
       "      <th>exerciseangia</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>noofmajorvessels</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>744</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>291.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>354.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>506</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>258.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>530</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>684</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "      <td>433.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id  age  gender  chestpain  restingBP  serumcholestrol  \\\n",
       "0        744   20       1          0        137            291.0   \n",
       "1          6   33       1          0         97            354.0   \n",
       "2        506   65       1          0        127            258.0   \n",
       "3        530   24       0          0        136            164.0   \n",
       "4        684   80       0          1        191            433.0   \n",
       "\n",
       "   fastingbloodsugar  restingrelectro  maxheartrate  exerciseangia  oldpeak  \\\n",
       "0                  0                0           131              1      3.8   \n",
       "1                  0                0           160              0      2.1   \n",
       "2                  0                0           158              0      4.1   \n",
       "3                  0                0            91              1      1.8   \n",
       "4                  1                1           154              1      3.2   \n",
       "\n",
       "   slope  noofmajorvessels  target  \n",
       "0      1                 0       0  \n",
       "1      2                 1       0  \n",
       "2      1                 3       0  \n",
       "3      1                 1       0  \n",
       "4      3                 3       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load preprocessed data \n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"./data_subsets/train_25M_75F.csv\")\n",
    "\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")\n",
    "\n",
    "#check out the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5330b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and sensitive column names\n",
    "TARGET = \"target\"\n",
    "SENSITIVE = \"gender\"\n",
    "\n",
    "# Split train into X/y\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]\n",
    "\n",
    "# Extract sensitive features separately\n",
    "A_train = X_train[SENSITIVE].astype(int)\n",
    "A_test  = X_test[SENSITIVE].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749ae738",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"target\"\n",
    "SENSITIVE = \"Sex\"   # 1 = Male, 0 = Female\n",
    "\n",
    "categorical_cols = ['gender','chestpain','fastingbloodsugar','restingrelectro','exerciseangia','slope','noofmajorvessels']\n",
    "continuous_cols  = ['age','restingBP','serumcholestrol','maxheartrate','oldpeak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fe70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into X / y and keep sensitive feature for fairness evaluation\n",
    "X_train = train_df.drop(columns=[TARGET])\n",
    "y_train = train_df[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d93327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numeric features only, fit on train, transform test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_train.index\n",
    ")\n",
    "X_test_num_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test[continuous_cols]),\n",
    "    columns=continuous_cols, index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9f63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode categoricals; numeric are kept as is \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\", sparse_output=False)\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "X_train_cat = pd.DataFrame(\n",
    "    ohe.transform(X_train[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_cat = pd.DataFrame(\n",
    "    ohe.transform(X_test[categorical_cols]),\n",
    "    columns=ohe.get_feature_names_out(categorical_cols),\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e79e02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shapes: (600, 22) (200, 22)\n"
     ]
    }
   ],
   "source": [
    "# Assemble final matrices\n",
    "X_train_ready = pd.concat([X_train_cat, X_train_num_scaled], axis=1)\n",
    "X_test_ready  = pd.concat([X_test_cat,  X_test_num_scaled],  axis=1)\n",
    "\n",
    "print(\"Final feature shapes:\", X_train_ready.shape, X_test_ready.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade7ec1",
   "metadata": {},
   "source": [
    "### Traditional ML Models - Baseline: K-Nearest Neighbors (KNN) & Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf455d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "#define a function \n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"=== {model_name} Evaluation ===\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred, average='binary'))\n",
    "    print(\"Recall   :\", recall_score(y_true, y_pred, average='binary'))\n",
    "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='binary'))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f7163",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7c94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KNN Evaluation ===\n",
      "Accuracy : 0.89\n",
      "Precision: 0.9122807017543859\n",
      "Recall   : 0.896551724137931\n",
      "F1 Score : 0.9043478260869565\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87        84\n",
      "           1       0.91      0.90      0.90       116\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.89      0.89      0.89       200\n",
      "weighted avg       0.89      0.89      0.89       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 74  10]\n",
      " [ 12 104]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_ready, y_train)\n",
    "y_pred_knn = knn.predict(X_test_ready)\n",
    "y_prob_knn = knn.predict_proba(X_test_ready)[:, 1]  \n",
    "evaluate_model(y_test, y_pred_knn, \"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34404c3",
   "metadata": {},
   "source": [
    "### Post-Processing -  KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f8d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (tuned PCA+KNN) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.884615  0.20000  0.884615       0.586957  0.847826\n",
      "1       0.900000  0.09375  0.900000       0.564935  0.902597\n",
      "Accuracy: 0.8900 | DP diff: 0.0220 | EO diff: 0.1063\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.884615  0.200000  0.884615       0.586957  0.847826\n",
      "1       0.922222  0.140625  0.922222       0.597403  0.896104\n",
      "Accuracy: 0.8850 | DP diff: 0.0104 | EO diff: 0.0594\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.961538  0.250000  0.961538       0.652174  0.869565\n",
      "1       0.922222  0.140625  0.922222       0.597403  0.896104\n",
      "Accuracy: 0.8900 | DP diff: 0.0548 | EO diff: 0.1094\n"
     ]
    }
   ],
   "source": [
    "# Demographic Parity post-processing for your KNN\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, true_positive_rate, false_positive_rate, selection_rate,\n",
    "    demographic_parity_difference, equalized_odds_difference\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function\n",
    "def eval_fairness(y_true, y_pred, A):\n",
    "    mf = MetricFrame(\n",
    "        metrics={\n",
    "            \"TPR\": true_positive_rate,\n",
    "            \"FPR\": false_positive_rate,\n",
    "            \"Recall\": recall_score, \n",
    "            \"SelectionRate\": selection_rate,\n",
    "            \"Accuracy\": accuracy_score,\n",
    "        },\n",
    "        y_true=y_true, y_pred=y_pred, sensitive_features=A\n",
    "    )\n",
    "    return {\n",
    "        \"by_group\": mf.by_group,\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"dp\": demographic_parity_difference(y_true, y_pred, sensitive_features=A),\n",
    "        \"eo\": equalized_odds_difference(y_true, y_pred, sensitive_features=A),\n",
    "    }\n",
    "\n",
    "# 1) Baseline metrics (no mitigation) \n",
    "knn.fit(X_train_ready, y_train)\n",
    "y_base = knn.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (tuned PCA+KNN) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing with DEMOGRAPHIC PARITY\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    prefit=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Post-processing with EQUALIZED ODDS\n",
    "post_eod = ThresholdOptimizer(\n",
    "    estimator=knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   # KNN supports this\n",
    "    grid_size=200,\n",
    "    prefit=True,                                # makes randomized post-processing reproducible\n",
    ")\n",
    "post_eod.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_eod = post_eod.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eod = eval_fairness(y_test, y_eod, A_test)\n",
    "\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eod[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod['acc']:.4f} | DP diff: {m_eod['dp']:.4f} | EO diff: {m_eod['eo']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738cb8e",
   "metadata": {},
   "source": [
    "### Bias Mitigation Results: KNN – Post-Processing\n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Model                          | Accuracy | DP diff | EO diff | Notes                                                         |\n",
    "|--------------------------------|:--------:|:-------:|:-------:|----------------------------------------------------------------|\n",
    "| **KNN Baseline**   | 0.8900   | 0.0220  | 0.1063  | High accuracy; small DP gap; moderate EO gap (error-rate imbalance). |\n",
    "| **+ Post-processing (DP)**     | 0.8850   | **0.0104** | **0.0594** | Accuracy ↓ slightly; **DP halved** and **EO reduced**. |\n",
    "| **+ Post-processing (EO)**     | 0.8900   | 0.0548  | 0.1094  | Accuracy unchanged; **DP worsens**; EO ≈ baseline. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline KNN** already has **low DP disparity (≈0.022)**, but a **moderate EO gap (≈0.106)** from differences in TPR/FPR.  \n",
    "- **Post (DP)** improves fairness meaningfully: DP shrinks to **0.0104** and EO drops to **0.0594**, though accuracy slips slightly (−0.5 pp).  \n",
    "- **Post (EO)** fails to help — EO remains ≈ baseline, while DP worsens to **0.055**.  \n",
    "\n",
    "**Conclusion:** For KNN, **Demographic Parity post-processing is beneficial** (both DP and EO improve, at minimal cost to accuracy). **Equalized Odds post-processing** is counterproductive, as it worsens DP and does not reduce EO.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb782e92",
   "metadata": {},
   "source": [
    "**CorrelationRemover** will be implemented to improve fairness after DP/EOD post-processing failed to change any predictions (0% flips), leaving metrics unchanged. By removing linear correlation between features and the sensitive attribute, we reduce leakage and make group score distributions more comparable, giving PCA+KNN and also any subsequent post-processing room to adjust selection rates and error rates—all while staying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f37790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessing: CorrelationRemover + PCA+KNN ===\n",
      "             TPR     FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                     \n",
      "0       0.884615  0.1500  0.884615       0.565217  0.869565\n",
      "1       0.844444  0.1875  0.844444       0.571429  0.831169\n",
      "Accuracy: 0.8400 | DP diff: 0.0062 | EO diff: 0.0402\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "from sklearn.metrics import recall_score  \n",
    "\n",
    "Xtr_df = X_train_ready.copy()\n",
    "Xte_df = X_test_ready.copy()\n",
    "Xtr_df[\"__A__\"] = A_train.values\n",
    "Xte_df[\"__A__\"] = A_test.values\n",
    "\n",
    "cr = CorrelationRemover(sensitive_feature_ids=[\"__A__\"])\n",
    "\n",
    "Xtr_fair_arr = cr.fit_transform(Xtr_df)   # shape: (n_samples, n_features - 1)\n",
    "Xte_fair_arr = cr.transform(Xte_df)\n",
    "\n",
    "# Rebuild DataFrames with columns that exclude the sensitive column\n",
    "cols_out = [c for c in Xtr_df.columns if c != \"__A__\"]\n",
    "Xtr_fair = pd.DataFrame(Xtr_fair_arr, index=Xtr_df.index, columns=cols_out)\n",
    "Xte_fair = pd.DataFrame(Xte_fair_arr, index=Xte_df.index, columns=cols_out)\n",
    "\n",
    "# Refit your PCA+KNN\n",
    "knn.fit(Xtr_fair, y_train)\n",
    "y_cr = knn.predict(Xte_fair)\n",
    "m_cr = eval_fairness(y_test, y_cr, A_test)\n",
    "\n",
    "print(\"\\n=== Preprocessing: CorrelationRemover + PCA+KNN ===\")\n",
    "print(m_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_cr['acc']:.4f} | DP diff: {m_cr['dp']:.4f} | EO diff: {m_cr['eo']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79e2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Post-CR (DP) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.884615  0.150000  0.884615       0.565217  0.869565\n",
      "1       0.855556  0.203125  0.855556       0.584416  0.831169\n",
      "Accuracy: 0.8400 | DP diff: 0.0192 | EO diff: 0.0531\n",
      "\n",
      "=== Post-CR (eOD) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.961538  0.30000  0.961538       0.673913  0.847826\n",
      "1       0.900000  0.28125  0.900000       0.642857  0.824675\n",
      "Accuracy: 0.8300 | DP diff: 0.0311 | EO diff: 0.0615\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Demographic Parity on top of the CorrelationRemover\n",
    "post_dp_cr = ThresholdOptimizer(\n",
    "    estimator=knn,\n",
    "    constraints=\"demographic_parity\",\n",
    "    objective=\"accuracy_score\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    prefit=True\n",
    ")\n",
    "post_dp_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)  # ideally fit on a validation split\n",
    "y_dp_cr = post_dp_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_dp_cr = eval_fairness(y_test, y_dp_cr, A_test)\n",
    "\n",
    "# Equalized Odds on top of CorrelationRemover\n",
    "post_eod_cr = ThresholdOptimizer(\n",
    "    estimator=knn,\n",
    "    constraints=\"equalized_odds\",\n",
    "    objective=\"accuracy_score\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=1000,\n",
    "    prefit=True\n",
    ")\n",
    "post_eod_cr.fit(Xtr_fair, y_train, sensitive_features=A_train)  # ideally fit on a validation split\n",
    "y_eod_cr = post_eod_cr.predict(Xte_fair, sensitive_features=A_test, random_state=42)\n",
    "m_eod_cr = eval_fairness(y_test, y_eod_cr, A_test)\n",
    "\n",
    "\n",
    "print(\"\\n=== Post-CR (DP) ===\")\n",
    "print(m_dp_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp_cr['acc']:.4f} | DP diff: {m_dp_cr['dp']:.4f} | EO diff: {m_dp_cr['eo']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Post-CR (eOD) ===\")\n",
    "print(m_eod_cr[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eod_cr['acc']:.4f} | DP diff: {m_eod_cr['dp']:.4f} | EO diff: {m_eod_cr['eo']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074237d",
   "metadata": {},
   "source": [
    "### Bias Mitigation Comparison (KNN)\n",
    "\n",
    "| Model variant             | Accuracy | DP diff | EO diff | SelRate S=0 | SelRate S=1 | TPR S=0 | TPR S=1 | FPR S=0 | FPR S=1 | Notes                                   |\n",
    "|---------------------------|:--------:|:-------:|:-------:|:-----------:|:-----------:|:-------:|:-------:|:-------:|:-------:|-----------------------------------------|\n",
    "| **Baseline (tuned KNN)**  | 0.8900   | 0.0220  | 0.1063  | 0.5870      | 0.5649      | 0.8846  | 0.9000  | 0.2000  | 0.0938  | Reference                               |\n",
    "| **Post-processing (DP)**  | 0.8850   | **0.0104** | **0.0594** | 0.5870      | 0.5974      | 0.8846  | 0.9222  | 0.2000  | 0.1406  | Accuracy ↓ slightly; DP & EO improved   |\n",
    "| **Post-processing (EO)**  | 0.8900   | 0.0548  | 0.1094  | 0.6522      | 0.5974      | 0.9615  | 0.9222  | 0.2500  | 0.1406  | Accuracy unchanged; DP worsens; EO ≈ baseline |\n",
    "| **CorrelationRemover + KNN** | 0.8400 | 0.0192  | 0.0531  | 0.5652      | 0.5844      | 0.8846  | 0.8556  | 0.1500  | 0.2031  | New baseline after CR; acc ↓ notably    |\n",
    "| **Post-CR (DP)**          | 0.8400   | 0.0192  | 0.0531  | 0.5652      | 0.5844      | 0.8846  | 0.8556  | 0.1500  | 0.2031  | **Identical to CR baseline (0% flips)** |\n",
    "| **Post-CR (EO)**          | 0.8300   | 0.0311  | 0.0615  | 0.6739      | 0.6429      | 0.9615  | 0.9000  | 0.3000  | 0.2813  | Worse accuracy; DP & EO both worsen     |\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**  \n",
    "- **Baseline KNN**: strong accuracy, **very small DP gap (0.022)** but a **moderate EO gap (0.106)** driven by FPR/TPR differences.  \n",
    "- **Post (DP)**: the **most effective mitigation** — DP halved (0.022 → 0.010), EO cut nearly in half (0.106 → 0.059), with only −0.5 pp accuracy.  \n",
    "- **Post (EO)**: not useful — EO ≈ baseline, DP worsens to 0.055, despite no accuracy loss.  \n",
    "- **CorrelationRemover (CR)**: reduces EO (0.106 → 0.053) but drops accuracy sharply (0.89 → 0.84).  \n",
    "- **Post-CR methods**: provide **no extra gains** — DP-CR unchanged, EO-CR worsens fairness and lowers accuracy further.  \n",
    "\n",
    "**Takeaway:**  \n",
    "- For KNN, **Post-processing with DP constraint** is best: improved **both fairness metrics** at minimal cost.  \n",
    "- **CR alone** helps EO but harms accuracy.  \n",
    "- **Post (EO)** and **Post-CR (EO)** should be avoided.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941cf3a",
   "metadata": {},
   "source": [
    "### Class-Weighted Tuned & Pruned DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a66f6420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage A — Best DT params: {'class_weight': {0: 1, 1: 4}, 'criterion': 'gini', 'max_depth': 6, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Stage A — Best CV Recall: 0.9817\n",
      "Stage B — Best ccp_alpha: 0.000000 | CV Recall: 0.9817\n",
      "=== Alternative Tuned & Pruned Decision Tree Evaluation ===\n",
      "Accuracy : 0.9\n",
      "Precision: 0.8934426229508197\n",
      "Recall   : 0.9396551724137931\n",
      "F1 Score : 0.9159663865546218\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88        84\n",
      "           1       0.89      0.94      0.92       116\n",
      "\n",
      "    accuracy                           0.90       200\n",
      "   macro avg       0.90      0.89      0.90       200\n",
      "weighted avg       0.90      0.90      0.90       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 71  13]\n",
      " [  7 109]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative DT tuning focused on higher recall\n",
    "# Changes vs previous:\n",
    "#  - Remove calibration (predict uses raw tree probs at 0.5)\n",
    "#  - Tune class_weight (heavier positive weights allowed)\n",
    "#  - Broaden depth a bit but keep regularization via min_samples_* and tiny impurity decrease\n",
    "#  - Prune only with very small ccp_alphas to avoid killing recall\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Simpler-but-expressive trees + tuned class weights\n",
    "base_dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "param_grid_simple = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],                  # add \"log_loss\" if your sklearn supports it\n",
    "    \"max_depth\": [4, 5, 6, 7, 8, 9, 10],               # a bit deeper to help recall\n",
    "    \"min_samples_split\": [5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6],\n",
    "    \"min_impurity_decrease\": [0.0, 1e-4, 1e-3],\n",
    "    \"class_weight\": [\"balanced\", {0:1,1:2}, {0:1,1:3}, {0:1,1:4}],  # stronger push toward positives\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "\n",
    "grid_simple = GridSearchCV(\n",
    "    estimator=base_dt,\n",
    "    param_grid=param_grid_simple,\n",
    "    cv=cv,\n",
    "    scoring=\"recall\",      # prioritize sensitivity for class 1\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True\n",
    ")\n",
    "grid_simple.fit(X_train_ready, y_train)\n",
    "\n",
    "best_params = grid_simple.best_params_\n",
    "print(\"Stage A — Best DT params:\", best_params)\n",
    "print(\"Stage A — Best CV Recall:\", round(grid_simple.best_score_, 4))\n",
    "\n",
    "# Train a zero-pruned model with best params to get the pruning path\n",
    "dt0 = DecisionTreeClassifier(random_state=42, **best_params, ccp_alpha=0.0).fit(X_train_ready, y_train)\n",
    "\n",
    "\n",
    "# Stage B — Gentle cost-complexity pruning (favor small alphas)\n",
    "path = dt0.cost_complexity_pruning_path(X_train_ready, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "# Focus on tiny alphas only + 0.0 to avoid big recall loss\n",
    "small_slice = ccp_alphas[: min(30, len(ccp_alphas))]  # first 30 values are typically the smallest\n",
    "candidate_alphas = np.unique(np.r_[0.0, small_slice])\n",
    "\n",
    "cv_scores = []\n",
    "for alpha in candidate_alphas:\n",
    "    dt_alpha = DecisionTreeClassifier(random_state=42, **best_params, ccp_alpha=alpha)\n",
    "    rec = cross_val_score(dt_alpha, X_train_ready, y_train, cv=cv, scoring=\"recall\", n_jobs=-1).mean()\n",
    "    cv_scores.append((alpha, rec))\n",
    "\n",
    "best_alpha, best_cv_recall = max(cv_scores, key=lambda x: x[1])\n",
    "print(f\"Stage B — Best ccp_alpha: {best_alpha:.6f} | CV Recall: {best_cv_recall:.4f}\")\n",
    "\n",
    "alt_best_dt = DecisionTreeClassifier(random_state=42, **best_params, ccp_alpha=best_alpha).fit(X_train_ready, y_train)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "y_pred = alt_best_dt.predict(X_test_ready)               \n",
    "y_prob = alt_best_dt.predict_proba(X_test_ready)[:, 1]   \n",
    "\n",
    "evaluate_model(y_test, y_pred, \"Alternative Tuned & Pruned Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd1d09",
   "metadata": {},
   "source": [
    "### Bias Mitigation DT: Inprocessing - Exponentiated Gradient Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "680a1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Alternative Tuned & Pruned DT) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.200000  0.923077       0.608696  0.869565\n",
      "1       0.944444  0.140625  0.944444       0.610390  0.909091\n",
      "Accuracy: 0.9000 | DP diff: 0.0017 | EO diff: 0.0594\n",
      "\n",
      "=== In-processing (alt DT): EG (Equalized Odds) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.961538  0.150000  0.961538       0.608696  0.913043\n",
      "1       0.922222  0.171875  0.922222       0.610390  0.883117\n",
      "Accuracy: 0.8900 | DP diff: 0.0017 | EO diff: 0.0393\n",
      "\n",
      "=== In-processing (alt DT): EG (Demographic Parity) ===\n",
      "             TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                    \n",
      "0       0.961538  0.150  0.961538       0.608696  0.913043\n",
      "1       0.900000  0.125  0.900000       0.577922  0.889610\n",
      "Accuracy: 0.8950 | DP diff: 0.0308 | EO diff: 0.0615\n",
      "\n",
      "=== Decision Tree: Baseline vs In-processing (EG) ===\n",
      "                            model  accuracy  dp_diff  eo_diff\n",
      "0  DT Baseline (alt tuned+pruned)     0.900   0.0017   0.0594\n",
      "1                    DT + EG (EO)     0.890   0.0017   0.0393\n",
      "2                    DT + EG (DP)     0.895   0.0308   0.0615\n"
     ]
    }
   ],
   "source": [
    "# In-processing mitigation for the **Alternative Tuned & Pruned Decision Tree**\n",
    "\n",
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#0) Baseline: alternative tuned &pruned DT (alt_best_dt))\n",
    "\n",
    "y_pred_dt_base = alt_best_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_pred_dt_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Alternative Tuned & Pruned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Exponentiated Gradient with Equalized Odds \n",
    "eg_eo = ExponentiatedGradient(\n",
    "    estimator=clone(alt_best_dt),   \n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,                       \n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_eo = eg_eo.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_eo = eg_eo.predict(X_test_ready)\n",
    "\n",
    "m_eo = eval_fairness(y_test, y_pred_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing (alt DT): EG (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "#2) Exponentiated Gradient with Demographic Parity\n",
    "eg_dp = ExponentiatedGradient(\n",
    "    estimator=clone(alt_best_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_dp = eg_dp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_dp = eg_dp.predict(X_test_ready)\n",
    "\n",
    "m_dp = eval_fairness(y_test, y_pred_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing (alt DT): EG (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary table\n",
    "summary_dt = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (alt tuned+pruned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + EG (EO)\",                   \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + EG (DP)\",                   \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Decision Tree: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0254766",
   "metadata": {},
   "source": [
    "### Bias Mitigation Results: Decision Tree – In-Processing (Alt Tuned & Pruned)\n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Model                            | Accuracy | DP diff | EO diff | Notes |\n",
    "|----------------------------------|:--------:|:-------:|:-------:|------|\n",
    "| **DT Baseline (alt tuned+pruned)** | 0.9000   | 0.0017  | 0.0594  | **Near-perfect DP**; moderate EO gap |\n",
    "| **DT + EG (EO)**                 | 0.8900   | 0.0017  | **0.0393** | **EO improves** (−0.0201); **accuracy −1.0 pp**; DP unchanged |\n",
    "| **DT + EG (DP)**                 | 0.8950   | **0.0308** | 0.0615  | **DP worsens** (+0.0291); EO ~baseline (+0.0021); accuracy −0.5 pp |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline** already achieves **selection-rate parity** (DP ≈ 0.0017; SR≈0.609 vs 0.610) with a **moderate EO** (~0.059) driven by TPR/FPR differences.\n",
    "- **EG (EO)** moves along the fairness frontier to **reduce EO** to **0.0393** (better-aligned TPR/FPR) but **costs 1 pp accuracy**; **DP remains perfect**.\n",
    "- **EG (DP)** undermines outcome parity (**DP jumps to 0.0308**) and leaves EO ~baseline, with a small accuracy loss.\n",
    "\n",
    "**Conclusion:** Given the baseline’s **near-zero DP**, prioritize **Equalized Odds**. Use **DT + EG (EO)** if reducing error-rate disparity is worth a **~1 pp accuracy trade-off**. Avoid **EG (DP)** here—it **worsens DP** without EO gains.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87407025",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: In-processing: GridSearch Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c97e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing: GridSearch (Equalized Odds) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.200000  0.923077       0.608696  0.869565\n",
      "1       0.944444  0.140625  0.944444       0.610390  0.909091\n",
      "Accuracy: 0.9000 | DP diff: 0.0017 | EO diff: 0.0594\n",
      "\n",
      "=== In-processing: GridSearch (Demographic Parity) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.200000  0.923077       0.608696  0.869565\n",
      "1       0.944444  0.140625  0.944444       0.610390  0.909091\n",
      "Accuracy: 0.9000 | DP diff: 0.0017 | EO diff: 0.0594\n",
      "\n",
      "=== Decision Tree: Baseline vs EG vs GS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (alt tuned+pruned)</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + EG (EO)</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + EG (DP)</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT + GS (EO)</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT + GS (DP)</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (alt tuned+pruned)     0.900   0.0017   0.0594\n",
       "1                    DT + EG (EO)     0.890   0.0017   0.0393\n",
       "2                    DT + EG (DP)     0.895   0.0308   0.0615\n",
       "3                    DT + GS (EO)     0.900   0.0017   0.0594\n",
       "4                    DT + GS (DP)     0.900   0.0017   0.0594"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds\n",
    "gs_eo = GridSearch(\n",
    "    estimator=clone(alt_best_dt),              # unfitted clone of tuned DT\n",
    "    constraints=EqualizedOdds(),            # EO constraint\n",
    "    selection_rule=\"tradeoff_optimization\", \n",
    "    constraint_weight=0.5,                  \n",
    "    grid_size=15,                           \n",
    ")\n",
    "gs_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_eo = gs_eo.predict(X_test_ready)\n",
    "m_gs_eo = eval_fairness(y_test, y_pred_gs_eo, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo['acc']:.4f} | DP diff: {m_gs_eo['dp']:.4f} | EO diff: {m_gs_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity\n",
    "gs_dp = GridSearch(\n",
    "    estimator=clone(alt_best_dt),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15,\n",
    ")\n",
    "gs_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_gs_dp = gs_dp.predict(X_test_ready)\n",
    "m_gs_dp = eval_fairness(y_test, y_pred_gs_dp, A_test)\n",
    "print(\"\\n=== In-processing: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp['acc']:.4f} | DP diff: {m_gs_dp['dp']:.4f} | EO diff: {m_gs_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with your existing runs\n",
    "summary_dt = pd.concat([\n",
    "    summary_dt,  \n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"DT + GS (EO)\", \"accuracy\":m_gs_eo[\"acc\"], \"dp_diff\":m_gs_eo[\"dp\"], \"eo_diff\":m_gs_eo[\"eo\"]},\n",
    "        {\"model\":\"DT + GS (DP)\", \"accuracy\":m_gs_dp[\"acc\"], \"dp_diff\":m_gs_dp[\"dp\"], \"eo_diff\":m_gs_dp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "print(\"\\n=== Decision Tree: Baseline vs EG vs GS ===\")\n",
    "summary_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86587b",
   "metadata": {},
   "source": [
    "### Decision Tree — In-Processing: EG vs. GridSearch (Alt Tuned & Pruned DT)\n",
    "\n",
    "#### Summary of results\n",
    "| Model                                | Accuracy | DP diff | EO diff | Notes |\n",
    "|--------------------------------------|:--------:|:-------:|:-------:|-------|\n",
    "| **DT Baseline (alt tuned+pruned)**   | 0.9000   | 0.0017  | 0.0594  | DP already ~zero; EO moderate |\n",
    "| **DT + EG (EO)**                     | 0.8900   | 0.0017  | **0.0393** | **EO ↓** (−0.0201); DP unchanged; **Acc −1.0 pp** |\n",
    "| **DT + EG (DP)**                     | 0.8950   | **0.0308** | 0.0615  | DP worsens; EO ~baseline; **Acc −0.5 pp** |\n",
    "| **DT + GS (EO)**                     | 0.9000   | 0.0017  | 0.0594  | **No change** (identical to baseline) |\n",
    "| **DT + GS (DP)**                     | 0.8950   | **0.0308** | 0.0615  | Mirrors EG (DP): higher DP, EO ~baseline, small acc loss |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline:** DP ≈ 0.002 → groups already receive almost identical selection rates. The remaining challenge is **EO ≈ 0.059**, reflecting error-rate imbalance.\n",
    "- **EG (EO):** improves **EO** notably (0.0393) while keeping DP ~zero, but costs 1 pp in accuracy.\n",
    "- **EG (DP) and GS (DP):** both harm DP (inflate to ~0.031) without improving EO; slight accuracy dips.\n",
    "- **GS (EO):** no movement from baseline, suggesting the baseline tree already lies on the fairness–utility frontier for EO.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "Since **DP is already optimal**, the main fairness concern is **Equalized Odds**. Among tested options, **DT + EG (EO)** is the only one that reduces EO, though with a small accuracy trade-off. **DP-focused constraints (EG/GS)** worsen fairness and should be avoided.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15838f4",
   "metadata": {},
   "source": [
    "#### Bias Mitigation DT: Post-processing: Threshold Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "873f1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Alternative Tuned & Pruned DT) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.200000  0.923077       0.608696  0.869565\n",
      "1       0.944444  0.140625  0.944444       0.610390  0.909091\n",
      "Accuracy: 0.9000 | DP diff: 0.0017 | EO diff: 0.0594\n",
      "\n",
      "=== Post-processing (Equalized Odds) ===\n",
      "             TPR    FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                    \n",
      "0       0.923077  0.300  0.923077       0.652174  0.826087\n",
      "1       0.944444  0.125  0.944444       0.603896  0.915584\n",
      "Accuracy: 0.8950 | DP diff: 0.0483 | EO diff: 0.1750\n",
      "\n",
      "=== Post-processing (Demographic Parity) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.200000  0.923077       0.608696  0.869565\n",
      "1       0.888889  0.078125  0.888889       0.551948  0.902597\n",
      "Accuracy: 0.8950 | DP diff: 0.0567 | EO diff: 0.1219\n",
      "\n",
      "=== Decision Tree: Baseline vs Post-processing ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>dp_diff</th>\n",
       "      <th>eo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT Baseline (tuned)</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT + Post (EO)</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT + Post (DP)</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.0567</td>\n",
       "      <td>0.1219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  dp_diff  eo_diff\n",
       "0  DT Baseline (tuned)     0.900   0.0017   0.0594\n",
       "1       DT + Post (EO)     0.895   0.0483   0.1750\n",
       "2       DT + Post (DP)     0.895   0.0567   0.1219"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "#0) Baseline: alternative tuned &pruned DT (alt_best_dt))\n",
    "\n",
    "y_pred_dt_base = alt_best_dt.predict(X_test_ready)\n",
    "m_base = eval_fairness(y_test, y_pred_dt_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Alternative Tuned & Pruned DT) ===\")\n",
    "print(m_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_base['acc']:.4f} | DP diff: {m_base['dp']:.4f} | EO diff: {m_base['eo']:.4f}\")\n",
    "\n",
    "\n",
    "#Post-processing: Equalized Odds\n",
    "post_eo = ThresholdOptimizer(\n",
    "    estimator=alt_best_dt,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_eo = post_eo.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_eo = eval_fairness(y_test, y_eo, A_test)\n",
    "print(\"\\n=== Post-processing (Equalized Odds) ===\")\n",
    "print(m_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_eo['acc']:.4f} | DP diff: {m_eo['dp']:.4f} | EO diff: {m_eo['eo']:.4f}\")\n",
    "\n",
    "# Post-processing: Demographic Parity\n",
    "post_dp = ThresholdOptimizer(\n",
    "    estimator=alt_best_dt,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_dp = post_dp.predict(X_test_ready, sensitive_features=A_test, random_state=42)\n",
    "m_dp = eval_fairness(y_test, y_dp, A_test)\n",
    "print(\"\\n=== Post-processing (Demographic Parity) ===\")\n",
    "print(m_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_dp['acc']:.4f} | DP diff: {m_dp['dp']:.4f} | EO diff: {m_dp['eo']:.4f}\")\n",
    "\n",
    "# create summary table \n",
    "summary = pd.DataFrame([\n",
    "    {\"model\":\"DT Baseline (tuned)\", \"accuracy\":m_base[\"acc\"], \"dp_diff\":m_base[\"dp\"], \"eo_diff\":m_base[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (EO)\",      \"accuracy\":m_eo[\"acc\"],   \"dp_diff\":m_eo[\"dp\"],   \"eo_diff\":m_eo[\"eo\"]},\n",
    "    {\"model\":\"DT + Post (DP)\",      \"accuracy\":m_dp[\"acc\"],   \"dp_diff\":m_dp[\"dp\"],   \"eo_diff\":m_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "print(\"\\n=== Decision Tree: Baseline vs Post-processing ===\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328aeb4",
   "metadata": {},
   "source": [
    "### Decision Tree — Post- vs In-Processing (Alt Tuned & Pruned)\n",
    "\n",
    "#### Combined Results\n",
    "\n",
    "| Model / Method                    | Accuracy | DP diff | EO diff | Notes (vs. baseline 0.9000 / 0.0017 / 0.0594) |\n",
    "|----------------------------------|:--------:|:-------:|:-------:|-----------------------------------------------|\n",
    "| **Baseline (Alt DT)**            | 0.9000   | 0.0017  | 0.0594  | Reference                                      |\n",
    "| **Post (EO)**                     | 0.8950   | 0.0483  | 0.1750  | **Acc −0.5 pp**; **EO worsens** (+0.116); DP ↑ markedly |\n",
    "| **Post (DP)**                     | 0.8950   | 0.0567  | 0.1219  | **Acc −0.5 pp**; both **DP and EO worsen**     |\n",
    "| **EG (EO)**                       | 0.8900   | 0.0017  | **0.0393** | **EO improves** (−0.020); DP unchanged; **Acc −1.0 pp** |\n",
    "| **EG (DP)**                       | 0.8950   | 0.0308  | 0.0615  | **Acc −0.5 pp**; DP ↑ strongly; EO ~baseline   |\n",
    "| **GS (EO)**                       | 0.9000   | 0.0017  | 0.0594  | **No change** (identical to baseline)          |\n",
    "| **GS (DP)**                       | 0.8950   | 0.0308  | 0.0615  | Mirrors EG(DP): accuracy ↓, DP worsens, EO ~baseline |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **Baseline alt DT** already achieves **near-parity in selection rates** (DP ≈ 0.002), leaving **EO ≈ 0.059** as the key fairness concern.  \n",
    "- **EG (EO)** is the only approach that **reduces EO** (to ≈0.039) while keeping DP at parity, though it costs ~1 pp accuracy.  \n",
    "- **Post-processing methods** are counterproductive here:  \n",
    "  - **Post (EO)** dramatically worsens EO (0.175) and inflates DP (0.048).  \n",
    "  - **Post (DP)** similarly harms both DP and EO.  \n",
    "- **EG (DP) / GS (DP)** worsen DP and offer no EO gain.  \n",
    "- **GS (EO)** does nothing—the baseline is already on the fairness frontier.  \n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**  \n",
    "Since **DP is already minimal**, the priority is reducing **error-rate disparity (EO)**. The best option is **DT + EG (EO)**: it lowers EO meaningfully while maintaining fairness in selection rates, at the cost of a small accuracy drop. **All DP-focused and post-processing methods degrade fairness and are not recommended**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbe2d4",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81ec0da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Evaluation ===\n",
      "Accuracy : 0.925\n",
      "Precision: 0.9469026548672567\n",
      "Recall   : 0.9224137931034483\n",
      "F1 Score : 0.9344978165938864\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91        84\n",
      "           1       0.95      0.92      0.93       116\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.92      0.93      0.92       200\n",
      "weighted avg       0.93      0.93      0.93       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 78   6]\n",
      " [  9 107]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train_ready, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf.predict(X_test_ready)\n",
    "y_prob_rf = rf.predict_proba(X_test_ready)[:, 1]  \n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daad4ed",
   "metadata": {},
   "source": [
    "### Bias Mitgation RF: In-processing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1199f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "        TPR       FPR  Recall  SelectionRate  Accuracy\n",
      "gender                                                \n",
      "0       1.0  0.050000     1.0       0.586957  0.978261\n",
      "1       0.9  0.078125     0.9       0.558442  0.909091\n",
      "Accuracy: 0.9250 | DP diff: 0.0285 | EO diff: 0.1000\n",
      "\n",
      "=== In-processing RF: EG (Equalized Odds) ===\n",
      "        TPR       FPR  Recall  SelectionRate  Accuracy\n",
      "gender                                                \n",
      "0       1.0  0.050000     1.0       0.586957  0.978261\n",
      "1       0.9  0.078125     0.9       0.558442  0.909091\n",
      "Accuracy: 0.9250 | DP diff: 0.0285 | EO diff: 0.1000\n",
      "\n",
      "=== In-processing RF: EG (Demographic Parity) ===\n",
      "        TPR       FPR  Recall  SelectionRate  Accuracy\n",
      "gender                                                \n",
      "0       1.0  0.050000     1.0       0.586957  0.978261\n",
      "1       0.9  0.078125     0.9       0.558442  0.909091\n",
      "Accuracy: 0.9250 | DP diff: 0.0285 | EO diff: 0.1000\n",
      "\n",
      "=== Random Forest: Baseline vs In-processing (EG) ===\n",
      "          model  accuracy  dp_diff  eo_diff\n",
      "0   RF Baseline     0.925   0.0285      0.1\n",
      "1  RF + EG (EO)     0.925   0.0285      0.1\n",
      "2  RF + EG (DP)     0.925   0.0285      0.1\n"
     ]
    }
   ],
   "source": [
    "# 0) Baseline Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_ready, y_train)\n",
    "\n",
    "y_pred_rf_base = rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_pred_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "#1) EG with Equalized Odds\n",
    "eg_eo_rf = ExponentiatedGradient(\n",
    "    estimator=clone(rf),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_eo = eg_eo_rf.predict(X_test_ready, random_state=42)\n",
    "m_rf_eo = eval_fairness(y_test, y_pred_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) EG with Demographic Parity \n",
    "eg_dp_rf = ExponentiatedGradient(\n",
    "    estimator=clone(rf),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_pred_rf_dp = eg_dp_rf.predict(X_test_ready, random_state=42)\n",
    "m_rf_dp = eval_fairness(y_test, y_pred_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing RF: EG (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table \n",
    "summary_rf = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",      \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (EO)\",     \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + EG (DP)\",     \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs In-processing (EG) ===\")\n",
    "print(summary_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b0260",
   "metadata": {},
   "source": [
    "## Random Forest Bias Mitigation Results\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model            | Accuracy | DP diff | EO diff | Interpretation                                   |\n",
    "|------------------|:--------:|:-------:|:-------:|--------------------------------------------------|\n",
    "| **RF Baseline**  | 0.9250   | 0.0285  | 0.1000  | High accuracy; **DP near zero**, **EO moderate** (driven by TPR gap). |\n",
    "| **RF + EG (EO)** | 0.9250   | 0.0285  | 0.1000  | **No change** vs baseline → EO constraint had no effect. |\n",
    "| **RF + EG (DP)** | 0.9250   | 0.0285  | 0.1000  | **No change** vs baseline → DP constraint had no effect. |\n",
    "\n",
    "### Key Points\n",
    "- **Selection rates:** gender=0 **0.587** vs gender=1 **0.558** → **DP = 0.0285** (practically balanced).\n",
    "- **Error rates:** **TPR** 1.00 vs 0.90 (gap **0.10**), **FPR** 0.0500 vs 0.0781 (gap **0.0281**) → **EO = 0.1000**, primarily driven by the TPR gap.\n",
    "- **ExponentiatedGradient** (EO/DP) yielded **0% movement**—typical when Random Forests are **insensitive to sample-weight reweighting** and the fairness frontier contains the **baseline model**.\n",
    "\n",
    "**Implication:** With DP already minimal, the relevant objective is **Equalized Odds** (reducing the TPR/FPR gap). Since EG did not shift the RF, consider approaches that act on probabilities/thresholds.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c181c7",
   "metadata": {},
   "source": [
    "### Bias Mitigation: RF: In-processing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4e11367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         method  weight    acc        dp        eo\n",
      "5  RF + GS (DP)    0.00  0.950  0.035008  0.077778\n",
      "6  RF + GS (DP)    0.25  0.950  0.035008  0.077778\n",
      "7  RF + GS (DP)    0.50  0.950  0.035008  0.077778\n",
      "8  RF + GS (DP)    0.75  0.950  0.035008  0.077778\n",
      "9  RF + GS (DP)    1.00  0.950  0.035008  0.077778\n",
      "0  RF + GS (EO)    0.00  0.945  0.030774  0.055556\n",
      "1  RF + GS (EO)    0.25  0.945  0.030774  0.055556\n",
      "2  RF + GS (EO)    0.50  0.945  0.030774  0.055556\n",
      "3  RF + GS (EO)    0.75  0.945  0.030774  0.055556\n",
      "4  RF + GS (EO)    1.00  0.945  0.030774  0.055556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0]   # 0.0 = accuracy-first, 1.0 = fairness-first\n",
    "grid = 50                               \n",
    "\n",
    "rows = []\n",
    "\n",
    "#Equalized Odds sweep\n",
    "for w in weights:\n",
    "    gs_eo_rf = GridSearch(\n",
    "        estimator=clone(rf),                 \n",
    "        constraints=EqualizedOdds(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_eo_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    # Some versions accept random_state in predict; if yours doesn't, seed numpy before predicting\n",
    "    try:\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_eo_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (EO)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "# Demographic Parity sweep\n",
    "for w in weights:\n",
    "    gs_dp_rf = GridSearch(\n",
    "        estimator=clone(rf),\n",
    "        constraints=DemographicParity(),\n",
    "        selection_rule=\"tradeoff_optimization\",\n",
    "        constraint_weight=w,\n",
    "        grid_size=grid\n",
    "    )\n",
    "    gs_dp_rf.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "    try:\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready, random_state=42)\n",
    "    except TypeError:\n",
    "        import numpy as np, random\n",
    "        np.random.seed(42); random.seed(42)\n",
    "        y_hat = gs_dp_rf.predict(X_test_ready)\n",
    "    m = eval_fairness(y_test, y_hat, A_test)\n",
    "    rows.append({\"method\":\"RF + GS (DP)\", \"weight\": w, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "\n",
    "df_gs = pd.DataFrame(rows).sort_values([\"method\",\"weight\"])\n",
    "print(df_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfa38e",
   "metadata": {},
   "source": [
    "### Interpretation — RF + GridSearch\n",
    "\n",
    "- **No movement across weights:** For both **DP** and **EO** constraints, varying the weight **0 → 1** leads to **identical results**—GridSearch selects the same frontier model each time.\n",
    "\n",
    "- **Relative to RF baseline (Acc 0.9250, DP 0.0285, EO 0.1000):**\n",
    "  - **GS (DP constraint):** Accuracy **0.950** (**+2.5 pp**), **EO 0.0778** (↓ **0.0222**), **DP 0.0350** (↑ **0.0065**).  \n",
    "    *Improves accuracy and EO modestly; DP rises slightly but remains small.*\n",
    "  - **GS (EO constraint):** Accuracy **0.945** (**+2.0 pp**), **EO 0.0556** (↓ **0.0444**, strongest EO gain), **DP 0.0308** (↑ **0.0023**).  \n",
    "    *Best for minimizing error-rate disparity while preserving accuracy.*\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:** GridSearch consistently converges to **one strong frontier model per constraint**.  \n",
    "- Choose **GS (EO)** if **Equalized Odds (error-rate parity)** is the priority → largest EO reduction.  \n",
    "- Choose **GS (DP)** if slightly higher **accuracy** is preferred, with a small DP increase that remains acceptable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     i    acc        dp        eo\n",
      "0    0  0.660  0.391304  0.850000\n",
      "1    1  0.665  0.413043  0.900000\n",
      "2    2  0.665  0.369565  0.850000\n",
      "3    3  0.265  0.156126  0.917094\n",
      "4    4  0.550  1.000000  1.000000\n",
      "5    5  0.665  0.369565  0.850000\n",
      "6    6  0.670  0.391304  0.900000\n",
      "7    7  0.270  0.164879  0.894872\n",
      "8    8  0.260  0.180124  0.894872\n",
      "9    9  0.550  1.000000  1.000000\n",
      "10  10  0.550  1.000000  1.000000\n",
      "11  11  0.670  0.391304  0.900000\n",
      "12  12  0.670  0.391304  0.900000\n",
      "13  13  0.265  0.158385  0.894872\n",
      "14  14  0.270  0.167137  0.872650\n",
      "15  15  0.250  0.190853  0.939316\n",
      "16  16  0.550  1.000000  1.000000\n",
      "17  17  0.550  1.000000  1.000000\n",
      "18  18  0.550  1.000000  1.000000\n",
      "19  19  0.670  0.391304  0.900000\n",
      "20  20  0.675  0.413043  0.950000\n",
      "21  21  0.265  0.199605  0.955556\n",
      "22  22  0.245  0.171372  0.928205\n",
      "23  23  0.270  0.177866  0.955556\n",
      "24  24  0.250  0.162620  0.939316\n",
      "25  25  0.810  0.545455  0.900000\n",
      "26  26  0.825  0.525974  0.900000\n",
      "27  27  0.815  0.525974  0.888889\n",
      "28  28  0.825  0.538961  0.911111\n",
      "29  29  0.945  0.030774  0.055556\n",
      "30  30  0.925  0.028515  0.100000\n",
      "31  31  0.545  0.586957  1.000000\n",
      "32  32  0.545  0.586957  1.000000\n",
      "33  33  0.545  0.586957  1.000000\n",
      "34  34  0.545  0.586957  1.000000\n",
      "35  35  0.545  0.586957  1.000000\n",
      "36  36  0.820  0.532468  0.900000\n",
      "37  37  0.815  0.538961  0.900000\n",
      "38  38  0.815  0.538961  0.900000\n",
      "39  39  0.945  0.017787  0.044444\n",
      "40  40  0.930  0.037267  0.077778\n",
      "41  41  0.545  0.586957  1.000000\n",
      "42  42  0.545  0.586957  1.000000\n",
      "43  43  0.545  0.586957  1.000000\n",
      "44  44  0.545  0.586957  1.000000\n",
      "45  45  0.725  0.184359  0.905983\n",
      "46  46  0.730  0.136646  0.894872\n",
      "47  47  0.835  0.396104  0.875000\n",
      "48  48  0.840  0.402597  0.890625\n",
      "49  49  0.450  1.000000  1.000000\n",
      "     i    acc        dp        eo\n",
      "0    0  0.550  1.000000  1.000000\n",
      "1    1  0.550  1.000000  1.000000\n",
      "2    2  0.550  1.000000  1.000000\n",
      "3    3  0.550  1.000000  1.000000\n",
      "4    4  0.550  1.000000  1.000000\n",
      "5    5  0.550  1.000000  1.000000\n",
      "6    6  0.550  1.000000  1.000000\n",
      "7    7  0.550  1.000000  1.000000\n",
      "8    8  0.550  1.000000  1.000000\n",
      "9    9  0.550  1.000000  1.000000\n",
      "10  10  0.550  1.000000  1.000000\n",
      "11  11  0.550  1.000000  1.000000\n",
      "12  12  0.550  1.000000  1.000000\n",
      "13  13  0.670  0.391304  0.900000\n",
      "14  14  0.665  0.413043  0.900000\n",
      "15  15  0.670  0.391304  0.900000\n",
      "16  16  0.670  0.391304  0.900000\n",
      "17  17  0.670  0.391304  0.900000\n",
      "18  18  0.675  0.413043  0.950000\n",
      "19  19  0.670  0.391304  0.900000\n",
      "20  20  0.675  0.413043  0.950000\n",
      "21  21  0.950  0.035008  0.077778\n",
      "22  22  0.960  0.009034  0.044444\n",
      "23  23  0.945  0.015528  0.066667\n",
      "24  24  0.930  0.037267  0.077778\n",
      "25  25  0.925  0.028515  0.100000\n",
      "26  26  0.910  0.035008  0.122222\n",
      "27  27  0.920  0.076228  0.122222\n",
      "28  28  0.925  0.043761  0.088889\n",
      "29  29  0.930  0.024280  0.066667\n",
      "30  30  0.545  0.586957  1.000000\n",
      "31  31  0.545  0.586957  1.000000\n",
      "32  32  0.545  0.586957  1.000000\n",
      "33  33  0.545  0.586957  1.000000\n",
      "34  34  0.545  0.586957  1.000000\n",
      "35  35  0.545  0.586957  1.000000\n",
      "36  36  0.540  0.565217  0.961538\n",
      "37  37  0.545  0.586957  1.000000\n",
      "38  38  0.450  1.000000  1.000000\n",
      "39  39  0.450  1.000000  1.000000\n",
      "40  40  0.450  1.000000  1.000000\n",
      "41  41  0.450  1.000000  1.000000\n",
      "42  42  0.450  1.000000  1.000000\n",
      "43  43  0.450  1.000000  1.000000\n",
      "44  44  0.450  1.000000  1.000000\n",
      "45  45  0.450  1.000000  1.000000\n",
      "46  46  0.450  1.000000  1.000000\n",
      "47  47  0.450  1.000000  1.000000\n",
      "48  48  0.450  1.000000  1.000000\n",
      "49  49  0.450  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Inspect how many distinct models GridSearch actually produced\n",
    "len(gs_eo_rf.predictors_), len(gs_dp_rf.predictors_)\n",
    "\n",
    "# See the spread across the frontier (test metrics for each predictor)\n",
    "def eval_frontier(gs, X, y, A):\n",
    "    rows=[]\n",
    "    for i, clf in enumerate(gs.predictors_):\n",
    "        yhat = clf.predict(X)\n",
    "        m = eval_fairness(y, yhat, A)\n",
    "        rows.append({\"i\": i, \"acc\": m[\"acc\"], \"dp\": m[\"dp\"], \"eo\": m[\"eo\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(eval_frontier(gs_eo_rf, X_test_ready, y_test, A_test))\n",
    "print(eval_frontier(gs_dp_rf, X_test_ready, y_test, A_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee0bbb",
   "metadata": {},
   "source": [
    "### RF GridSearch frontier — concise interpretation\n",
    "\n",
    "**What the tables show:** Each index `i` is a candidate model on the fairness–accuracy frontier returned by GridSearch.  \n",
    "Many entries are **degenerate** (e.g., `i=0–12`, `31–37`, `38–49` in the second table; also `i=3–24` in the first table), with **Acc ≈ 0.25–0.55** and **DP/EO ≈ 1.0**. These should be discarded.\n",
    "\n",
    "---\n",
    "\n",
    "#### Strong candidates (vs. RF baseline: Acc **0.925**, DP **0.0285**, EO **0.1000**)\n",
    "\n",
    "- **Best overall (Pareto-superior):**  \n",
    "  - `i=22` (second table) → **Acc 0.960**, **DP 0.0090**, **EO 0.0444**.  \n",
    "    *Improves accuracy by +3.5 pp while **halving EO** and driving DP near zero.*\n",
    "\n",
    "- **Also strong improvements:**  \n",
    "  - `i=39` (first table) → **Acc 0.945**, **DP 0.0178**, **EO 0.0444**.  \n",
    "    *Excellent EO reduction with very low DP, at slightly lower accuracy.*  \n",
    "  - `i=29` (second table) → **Acc 0.930**, **DP 0.0243**, **EO 0.0667**.  \n",
    "    *Close to baseline accuracy but with modest fairness gains.*\n",
    "\n",
    "- **Baseline-like reference:**  \n",
    "  - `i=30` (first table) → **Acc 0.925**, **DP 0.0285**, **EO 0.1000**.  \n",
    "    *Effectively the baseline point reappearing in the search.*\n",
    "\n",
    "---\n",
    "\n",
    "#### Takeaway\n",
    "The **frontier contains clear improvements** over the baseline RF:  \n",
    "\n",
    "- **If the goal is strongest fairness + accuracy:** choose **`i=22`** (Acc 0.960, DP 0.009, EO 0.044).  \n",
    "- **If aiming for a balanced compromise:** choose **`i=39`** (Acc 0.945, DP 0.018, EO 0.044).  \n",
    "- **Baseline-like points** (e.g., `i=30`) serve only as reference and should not be selected.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5477483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RF + GS (EO): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (EO)] i=22\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.961538  0.100000  0.961538       0.586957  0.934783\n",
      "1       0.033333  0.953125  0.033333       0.415584  0.038961\n",
      "Accuracy: 0.2450 | DP diff: 0.1714 | EO diff: 0.9282\n",
      "\n",
      "[RF + GS (EO)] i=39\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       1.000000  0.100000  1.000000       0.608696  0.956522\n",
      "1       0.955556  0.078125  0.955556       0.590909  0.941558\n",
      "Accuracy: 0.9450 | DP diff: 0.0178 | EO diff: 0.0444\n",
      "\n",
      "--- Summary (RF + GS (EO)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "0  22     0.245   0.1714   0.9282\n",
      "1  39     0.945   0.0178   0.0444\n",
      "\n",
      "=== RF + GS (DP): 50 frontier candidates ===\n",
      "\n",
      "[RF + GS (DP)] i=22\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       1.000000  0.050000  1.000000       0.586957  0.978261\n",
      "1       0.955556  0.046875  0.955556       0.577922  0.954545\n",
      "Accuracy: 0.9600 | DP diff: 0.0090 | EO diff: 0.0444\n",
      "\n",
      "[RF + GS (DP)] i=39\n",
      "        TPR  FPR  Recall  SelectionRate  Accuracy\n",
      "gender                                           \n",
      "0       1.0  1.0     1.0            1.0  0.565217\n",
      "1       0.0  0.0     0.0            0.0  0.415584\n",
      "Accuracy: 0.4500 | DP diff: 1.0000 | EO diff: 1.0000\n",
      "\n",
      "--- Summary (RF + GS (DP)) ---\n",
      "    i  accuracy  dp_diff  eo_diff\n",
      "0  22      0.96    0.009   0.0444\n",
      "1  39      0.45    1.000   1.0000\n"
     ]
    }
   ],
   "source": [
    "# Show results for the specific frontier models \n",
    "# for both RF GridSearch runs (EO- and DP-constrained).\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "indices = [22,39]\n",
    "\n",
    "def eval_selected(gs, label):\n",
    "    rows = []\n",
    "    n = len(gs.predictors_)\n",
    "    print(f\"\\n=== {label}: {n} frontier candidates ===\")\n",
    "    for i in indices:\n",
    "        if i >= n:\n",
    "            print(f\"[{label}] Skipping i={i} (only {n} candidates).\")\n",
    "            continue\n",
    "        clf = gs.predictors_[i]\n",
    "        y_hat = clf.predict(X_test_ready)\n",
    "        m = eval_fairness(y_test, y_hat, A_test)\n",
    "        rows.append({\"i\": i, \"accuracy\": m[\"acc\"], \"dp_diff\": m[\"dp\"], \"eo_diff\": m[\"eo\"]})\n",
    "\n",
    "        # Per-group breakdown for this model\n",
    "        print(f\"\\n[{label}] i={i}\")\n",
    "        print(m[\"by_group\"])\n",
    "        print(f\"Accuracy: {m['acc']:.4f} | DP diff: {m['dp']:.4f} | EO diff: {m['eo']:.4f}\")\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows).sort_values(\"i\").round(4)\n",
    "        print(f\"\\n--- Summary ({label}) ---\")\n",
    "        print(df)\n",
    "\n",
    "# Evaluate selected indices for both EO and DP GridSearch objects\n",
    "eval_selected(gs_eo_rf, \"RF + GS (EO)\")\n",
    "eval_selected(gs_dp_rf, \"RF + GS (DP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b146b9",
   "metadata": {},
   "source": [
    "### Random Forest — GridSearch Candidates (EO vs DP)\n",
    "\n",
    "#### Explanation\n",
    "- Each row corresponds to a **frontier model** (`i`) from GridSearch.\n",
    "- Two **promising solutions** stand out:\n",
    "  - **GS (EO) `i=39`** → **Acc 0.945**, **DP 0.0178**, **EO 0.0444**.  \n",
    "    *Well-balanced candidate with strong fairness and high accuracy.*\n",
    "  - **GS (DP) `i=22`** → **Acc 0.960**, **DP 0.0090**, **EO 0.0444**.  \n",
    "    *Best overall: highest accuracy with very low DP and EO.*\n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Constraint | i   | Accuracy | DP diff | EO diff | Notes |\n",
    "|------------|-----|:--------:|:-------:|:-------:|-------|\n",
    "| **EO**     | 22  | 0.2450   | 0.1714  | 0.9282  | Degenerate (one group near-random); **avoid** |\n",
    "| **EO**     | 39  | 0.9450   | 0.0178  | 0.0444  | **Strong candidate**: low DP & EO, high accuracy |\n",
    "| **DP**     | 22  | 0.9600   | 0.0090  | 0.0444  | **Top candidate**: near-parity DP, low EO, best accuracy |\n",
    "| **DP**     | 39  | 0.4500   | 1.0000  | 1.0000  | Pathological (group collapse: all-positive vs all-negative); **avoid** |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- **GS (EO) `i=39`**: Provides a **balanced trade-off**, cutting EO to ≈0.044 and keeping DP low (≈0.018), with strong accuracy (0.945).  \n",
    "- **GS (DP) `i=22`**: The **best combined model**: DP nearly eliminated (≈0.009), EO also low (≈0.044), and the **highest accuracy (0.960)** across candidates.  \n",
    "- **Degenerate points** (`i=22` under EO, `i=39` under DP) collapse predictions and are unsuitable for deployment.\n",
    "\n",
    "---\n",
    "\n",
    "**Recommendation:**  \n",
    "- For **error-rate fairness (Equalized Odds)** → choose **GS (EO) `i=39`**.  \n",
    "- For **outcome-rate fairness (Demographic Parity)** or **best all-around performance** → choose **GS (DP) `i=22`**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001876d",
   "metadata": {},
   "source": [
    "### Bias Mitigation RD: Post-processing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8238f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (Random Forest) ===\n",
      "        TPR       FPR  Recall  SelectionRate  Accuracy\n",
      "gender                                                \n",
      "0       1.0  0.050000     1.0       0.586957  0.978261\n",
      "1       0.9  0.078125     0.9       0.558442  0.909091\n",
      "Accuracy: 0.9250 | DP diff: 0.0285 | EO diff: 0.1000\n",
      "\n",
      "=== RF + Post-processing (Equalized Odds) ===\n",
      "        TPR       FPR  Recall  SelectionRate  Accuracy\n",
      "gender                                                \n",
      "0       1.0  0.100000     1.0       0.608696  0.956522\n",
      "1       0.9  0.078125     0.9       0.558442  0.909091\n",
      "Accuracy: 0.9200 | DP diff: 0.0503 | EO diff: 0.1000\n",
      "\n",
      "=== RF + Post-processing (Demographic Parity) ===\n",
      "        TPR       FPR  Recall  SelectionRate  Accuracy\n",
      "gender                                                \n",
      "0       1.0  0.100000     1.0       0.608696  0.956522\n",
      "1       0.9  0.078125     0.9       0.558442  0.909091\n",
      "Accuracy: 0.9200 | DP diff: 0.0503 | EO diff: 0.1000\n",
      "\n",
      "=== Random Forest: Baseline vs Post-processing ===\n",
      "            model  accuracy  dp_diff  eo_diff\n",
      "0     RF Baseline     0.925   0.0285      0.1\n",
      "1  RF + Post (EO)     0.920   0.0503      0.1\n",
      "2  RF + Post (DP)     0.920   0.0503      0.1\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# 0) Baseline RF \n",
    "rf.fit(X_train_ready, y_train)\n",
    "y_rf_base = rf.predict(X_test_ready)\n",
    "m_rf_base = eval_fairness(y_test, y_rf_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (Random Forest) ===\")\n",
    "print(m_rf_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_base['acc']:.4f} | DP diff: {m_rf_base['dp']:.4f} | EO diff: {m_rf_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds \n",
    "post_rf_eo = ThresholdOptimizer(\n",
    "    estimator=rf,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",   \n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_eo = post_rf_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_rf_eo = eval_fairness(y_test, y_rf_eo, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Equalized Odds) ===\")\n",
    "print(m_rf_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_eo['acc']:.4f} | DP diff: {m_rf_eo['dp']:.4f} | EO diff: {m_rf_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity \n",
    "post_rf_dp = ThresholdOptimizer(\n",
    "    estimator=rf,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_rf_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_rf_dp = post_rf_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_rf_dp = eval_fairness(y_test, y_rf_dp, A_test)\n",
    "\n",
    "print(\"\\n=== RF + Post-processing (Demographic Parity) ===\")\n",
    "print(m_rf_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_rf_dp['acc']:.4f} | DP diff: {m_rf_dp['dp']:.4f} | EO diff: {m_rf_dp['eo']:.4f}\")\n",
    "\n",
    "#3) Summary Table\n",
    "summary_rf_post = pd.DataFrame([\n",
    "    {\"model\":\"RF Baseline\",       \"accuracy\":m_rf_base[\"acc\"], \"dp_diff\":m_rf_base[\"dp\"], \"eo_diff\":m_rf_base[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (EO)\",    \"accuracy\":m_rf_eo[\"acc\"],   \"dp_diff\":m_rf_eo[\"dp\"],   \"eo_diff\":m_rf_eo[\"eo\"]},\n",
    "    {\"model\":\"RF + Post (DP)\",    \"accuracy\":m_rf_dp[\"acc\"],   \"dp_diff\":m_rf_dp[\"dp\"],   \"eo_diff\":m_rf_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== Random Forest: Baseline vs Post-processing ===\")\n",
    "print(summary_rf_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde82b4a",
   "metadata": {},
   "source": [
    "## Random Forest Bias Mitigation: Post-processing: Threshold Optimizer\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Interpretation                                        |\n",
    "|---------------------|:--------:|:-------:|:-------:|-------------------------------------------------------|\n",
    "| **RF Baseline**     | 0.9250   | 0.0285  | 0.1000  | High accuracy; **DP near zero**, **EO moderate** (TPR gap). |\n",
    "| **RF + Post (EO)**  | 0.9200   | 0.0503  | 0.1000  | **Accuracy ↓**; **DP worsens**; **EO unchanged** (TPR gap still 0.10). |\n",
    "| **RF + Post (DP)**  | 0.9200   | 0.0503  | 0.1000  | Same as Post(EO): **no EO gain**, **DP worse**, slight accuracy drop. |\n",
    "\n",
    "### Summary:\n",
    "- **Selection rates:** group 0 rises **0.587 → 0.609** while group 1 stays **0.558** ⇒ **DP increases** (0.0285 → 0.0503).\n",
    "- **Error rates:** **TPR gap remains 0.10** (1.00 vs 0.90); **FPR gap shrinks** (0.05 vs 0.078 → 0.10 vs 0.078), but EO stays **0.1000** because the TPR gap dominates Equalized Odds.\n",
    "- Both post-processing variants converge to the **same thresholds** on these scores.\n",
    "\n",
    "**Takeaway:** With DP already minimal and EO driven by a persistent TPR gap, post-processing **does not improve fairness** and slightly **reduces accuracy**. Prefer selecting a better **GridSearch frontier model** for RF when EO/DP improvements are required.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2c505",
   "metadata": {},
   "source": [
    "### Deep Learning - Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b0d5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required library \n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4cbac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multilayer Perceptron (MLP)- LBFGS solver Evaluation ===\n",
      "Accuracy : 0.885\n",
      "Precision: 0.8842975206611571\n",
      "Recall   : 0.9224137931034483\n",
      "F1 Score : 0.9029535864978903\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86        84\n",
      "           1       0.88      0.92      0.90       116\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.89      0.88      0.88       200\n",
      "weighted avg       0.89      0.89      0.88       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 70  14]\n",
      " [  9 107]]\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LBFGS solver - converges fast & well on small datasets\n",
    "# LBFGS ignores batch_size, early_stopping, learning_rate. It optimizes the full-batch loss.\n",
    "mlp_lbfgs = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='tanh',         # tanh + lbfgs often works nicely on tabular data\n",
    "    solver='lbfgs',            # quasi-Newton optimizer\n",
    "    alpha=1e-3,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp_lbfgs.fit(X_train_ready, y_train)\n",
    "y_pred_lbfgs = mlp_lbfgs.predict(X_test_ready)\n",
    "y_prob_lbfgs = mlp_lbfgs.predict_proba(X_test_ready)[:, 1] \n",
    "\n",
    "evaluate_model(y_test, y_pred_lbfgs, \"Multilayer Perceptron (MLP)- LBFGS solver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775454c",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Exponentiated Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "857cf027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP: tanh + lbfgs) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.150000  0.923077       0.586957  0.891304\n",
      "1       0.922222  0.171875  0.922222       0.610390  0.883117\n",
      "Accuracy: 0.8850 | DP diff: 0.0234 | EO diff: 0.0219\n",
      "\n",
      "=== In-processing MLP (tanh+lbfgs): EG (Equalized Odds) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.150000  0.923077       0.586957  0.891304\n",
      "1       0.922222  0.171875  0.922222       0.610390  0.883117\n",
      "Accuracy: 0.8850 | DP diff: 0.0234 | EO diff: 0.0219\n",
      "\n",
      "=== In-processing MLP (tanh+lbfgs): EG (Demographic Parity) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.150000  0.923077       0.586957  0.891304\n",
      "1       0.922222  0.171875  0.922222       0.610390  0.883117\n",
      "Accuracy: 0.8850 | DP diff: 0.0234 | EO diff: 0.0219\n",
      "\n",
      "=== MLP (tanh+lbfgs): Baseline vs In-processing (EG) ===\n",
      "                       model  accuracy  dp_diff  eo_diff\n",
      "0  MLP Baseline (tanh+lbfgs)     0.885   0.0234   0.0219\n",
      "1              MLP + EG (EO)     0.885   0.0234   0.0219\n",
      "2              MLP + EG (DP)     0.885   0.0234   0.0219\n"
     ]
    }
   ],
   "source": [
    "# Mitigation with LBFGS-based MLP baseline (tanh, lbfgs) \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Baseline MLP (LBFGS)\n",
    "mlp_lbfgs = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='tanh',      # works well on tabular data with lbfgs\n",
    "    solver='lbfgs',         # quasi-Newton; full-batch optimizer\n",
    "    alpha=1e-3,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp_lbfgs.fit(X_train_ready, y_train)\n",
    "\n",
    "# Baseline predictions/metrics\n",
    "y_pred_mlp_base = mlp_lbfgs.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_pred_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP: tanh + lbfgs) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) In-processing via ExponentiatedGradient with Equalized Odds\n",
    "eg_eo_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(mlp_lbfgs),  # clone preserves random_state=42\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_eo = eg_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_eo = eval_fairness(y_test, y_pred_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP (tanh+lbfgs): EG (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) In-processing via ExponentiatedGradient with Demographic Parity\n",
    "eg_dp_mlp = ExponentiatedGradient(\n",
    "    estimator=clone(mlp_lbfgs),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.01,\n",
    "    max_iter=50\n",
    ")\n",
    "eg_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "\n",
    "try:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_mlp_dp = eg_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_mlp_dp = eval_fairness(y_test, y_pred_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== In-processing MLP (tanh+lbfgs): EG (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline (tanh+lbfgs)\", \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (EO)\",              \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + EG (DP)\",              \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP (tanh+lbfgs): Baseline vs In-processing (EG) ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21943e26",
   "metadata": {},
   "source": [
    "### MLP — In-Processing\n",
    "\n",
    "#### Metrics Overview\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Notes |\n",
    "|---------------------|:--------:|:-------:|:-------:|-------|\n",
    "| **MLP Baseline**    | 0.8850   | 0.0234  | 0.0219  | Already near parity: very small DP and EO gaps. |\n",
    "| **MLP + EG (EO)**   | 0.8850   | 0.0234  | 0.0219  | **No change** vs baseline — constraint not binding. |\n",
    "| **MLP + EG (DP)**   | 0.8850   | 0.0234  | 0.0219  | **No change** vs baseline — constraint not binding. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- The baseline MLP already exhibits **balanced selection rates (DP ≈ 0.023)** and a **tiny error-rate gap (EO ≈ 0.022)**.  \n",
    "- Both **EG (EO)** and **EG (DP)** returned **identical results to baseline**, showing that the fairness constraints did **not affect** the model’s decision boundary.  \n",
    "- This suggests the baseline is already on the fairness–accuracy frontier for this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:** Since fairness metrics are already close to parity, **in-processing EG offers no additional benefit**. The **baseline MLP** remains the best choice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11de87",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Inprocessing: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1b9ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In-processing MLP: GridSearch (Equalized Odds) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.150000  0.923077       0.586957  0.891304\n",
      "1       0.922222  0.171875  0.922222       0.610390  0.883117\n",
      "Accuracy: 0.8850 | DP diff: 0.0234 | EO diff: 0.0219\n",
      "\n",
      "=== In-processing MLP: GridSearch (Demographic Parity) ===\n",
      "             TPR      FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                      \n",
      "0       0.884615  0.20000  0.884615       0.586957  0.847826\n",
      "1       0.888889  0.09375  0.888889       0.558442  0.896104\n",
      "Accuracy: 0.8850 | DP diff: 0.0285 | EO diff: 0.1063\n",
      "\n",
      "=== MLP: Baseline vs EG vs GS ===\n",
      "                       model  accuracy  dp_diff  eo_diff\n",
      "0  MLP Baseline (tanh+lbfgs)     0.885   0.0234   0.0219\n",
      "1              MLP + EG (EO)     0.885   0.0234   0.0219\n",
      "2              MLP + EG (DP)     0.885   0.0234   0.0219\n",
      "3              MLP + GS (EO)     0.885   0.0234   0.0219\n",
      "4              MLP + GS (DP)     0.885   0.0285   0.1062\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds, DemographicParity\n",
    "\n",
    "# 1) GridSearch with Equalized Odds (MLP)\n",
    "gs_eo_mlp = GridSearch(\n",
    "    estimator=clone(mlp_lbfgs),                 # unfitted clone of your MLP (inherits random_state=42)\n",
    "    constraints=EqualizedOdds(),\n",
    "    selection_rule=\"tradeoff_optimization\",  \n",
    "    constraint_weight=0.5,                   # trade-off weight (0..1); tune as needed\n",
    "    grid_size=15                             \n",
    ")\n",
    "gs_eo_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_eo_mlp = gs_eo_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_eo_mlp = eval_fairness(y_test, y_pred_gs_eo_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Equalized Odds) ===\")\n",
    "print(m_gs_eo_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_eo_mlp['acc']:.4f} | DP diff: {m_gs_eo_mlp['dp']:.4f} | EO diff: {m_gs_eo_mlp['eo']:.4f}\")\n",
    "\n",
    "# 2) GridSearch with Demographic Parity (MLP)\n",
    "gs_dp_mlp = GridSearch(\n",
    "    estimator=clone(mlp_lbfgs),\n",
    "    constraints=DemographicParity(),\n",
    "    selection_rule=\"tradeoff_optimization\",\n",
    "    constraint_weight=0.5,\n",
    "    grid_size=15\n",
    ")\n",
    "gs_dp_mlp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "try:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready, random_state=42)\n",
    "except TypeError:\n",
    "    y_pred_gs_dp_mlp = gs_dp_mlp.predict(X_test_ready)\n",
    "\n",
    "m_gs_dp_mlp = eval_fairness(y_test, y_pred_gs_dp_mlp, A_test)\n",
    "print(\"\\n=== In-processing MLP: GridSearch (Demographic Parity) ===\")\n",
    "print(m_gs_dp_mlp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_gs_dp_mlp['acc']:.4f} | DP diff: {m_gs_dp_mlp['dp']:.4f} | EO diff: {m_gs_dp_mlp['eo']:.4f}\")\n",
    "\n",
    "# 3) Compare with existing MLP runs (baseline + EG)\n",
    "summary_mlp = pd.concat([\n",
    "    summary_mlp,\n",
    "    pd.DataFrame([\n",
    "        {\"model\":\"MLP + GS (EO)\", \"accuracy\":m_gs_eo_mlp[\"acc\"], \"dp_diff\":m_gs_eo_mlp[\"dp\"], \"eo_diff\":m_gs_eo_mlp[\"eo\"]},\n",
    "        {\"model\":\"MLP + GS (DP)\", \"accuracy\":m_gs_dp_mlp[\"acc\"], \"dp_diff\":m_gs_dp_mlp[\"dp\"], \"eo_diff\":m_gs_dp_mlp[\"eo\"]},\n",
    "    ]).round(4)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs EG vs GS ===\")\n",
    "print(summary_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4cf0b",
   "metadata": {},
   "source": [
    "### MLP — In-Processing vs GridSearch\n",
    "\n",
    "#### Comparative table (vs. Baseline)\n",
    "\n",
    "| Model              | Accuracy | ΔAcc (pp) | DP diff |   ΔDP   | EO diff |   ΔEO   | Notes                                   |\n",
    "|--------------------|:--------:|:---------:|:-------:|:-------:|:-------:|:-------:|-----------------------------------------|\n",
    "| **Baseline (MLP)** | 0.8850   |     –     | 0.0234  |    –    | 0.0219  |    –    | Reference                               |\n",
    "| **EG (EO)**        | 0.8850   |   0.0     | 0.0234  |  0.0000 | 0.0219  |  0.0000 | **No change** vs. baseline              |\n",
    "| **EG (DP)**        | 0.8850   |   0.0     | 0.0234  |  0.0000 | 0.0219  |  0.0000 | **No change** vs. baseline              |\n",
    "| **GS (EO)**        | 0.8850   |   0.0     | 0.0234  |  0.0000 | 0.0219  |  0.0000 | **No change** vs. baseline              |\n",
    "| **GS (DP)**        | 0.8850   |   0.0     | 0.0285  | +0.0051 | 0.1063  | **+0.0844** | **Worse fairness**: DP ↑, EO ↑ sharply |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- The **baseline MLP** already achieves **near parity** (DP ≈ 0.023, EO ≈ 0.022).  \n",
    "- **EG (EO)** and **EG (DP)** had **no effect** — the fairness constraints did not bind.  \n",
    "- **GS (EO)** was also identical to baseline.  \n",
    "- **GS (DP)**, however, **degraded fairness**: EO increased more than fourfold (0.022 → 0.106) and DP worsened slightly, with no accuracy gain.  \n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:** The **baseline MLP** remains the preferable option. In this case, both EG and GS provided **no benefit**, and GS (DP) actually harmed fairness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d29d4a",
   "metadata": {},
   "source": [
    "### Bias mitigation MLP: Postprocessing: Threshold Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4591c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (MLP) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.150000  0.923077       0.586957  0.891304\n",
      "1       0.922222  0.171875  0.922222       0.610390  0.883117\n",
      "Accuracy: 0.8850 | DP diff: 0.0234 | EO diff: 0.0219\n",
      "\n",
      "=== MLP + Post-processing (Equalized Odds) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.150000  0.923077       0.586957  0.891304\n",
      "1       0.922222  0.171875  0.922222       0.610390  0.883117\n",
      "Accuracy: 0.8850 | DP diff: 0.0234 | EO diff: 0.0219\n",
      "\n",
      "=== MLP + Post-processing (Demographic Parity) ===\n",
      "             TPR       FPR    Recall  SelectionRate  Accuracy\n",
      "gender                                                       \n",
      "0       0.923077  0.150000  0.923077       0.586957  0.891304\n",
      "1       0.922222  0.171875  0.922222       0.610390  0.883117\n",
      "Accuracy: 0.8850 | DP diff: 0.0234 | EO diff: 0.0219\n",
      "\n",
      "=== MLP: Baseline vs Post-processing ===\n",
      "             model  accuracy  dp_diff  eo_diff\n",
      "0     MLP Baseline     0.885   0.0234   0.0219\n",
      "1  MLP + Post (EO)     0.885   0.0234   0.0219\n",
      "2  MLP + Post (DP)     0.885   0.0234   0.0219\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Baseline MLP\n",
    "mlp_lbfgs.fit(X_train_ready, y_train)\n",
    "y_mlp_base = mlp_lbfgs.predict(X_test_ready)\n",
    "m_mlp_base = eval_fairness(y_test, y_mlp_base, A_test)\n",
    "\n",
    "print(\"=== Baseline (MLP) ===\")\n",
    "print(m_mlp_base[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_base['acc']:.4f} | DP diff: {m_mlp_base['dp']:.4f} | EO diff: {m_mlp_base['eo']:.4f}\")\n",
    "\n",
    "# 1) Post-processing: Equalized Odds\n",
    "post_mlp_eo = ThresholdOptimizer(\n",
    "    estimator=mlp_lbfgs,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_eo.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_eo = post_mlp_eo.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_eo = eval_fairness(y_test, y_mlp_eo, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Equalized Odds) ===\")\n",
    "print(m_mlp_eo[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_eo['acc']:.4f} | DP diff: {m_mlp_eo['dp']:.4f} | EO diff: {m_mlp_eo['eo']:.4f}\")\n",
    "\n",
    "# 2) Post-processing: Demographic Parity\n",
    "post_mlp_dp = ThresholdOptimizer(\n",
    "    estimator=mlp_lbfgs,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method=\"predict_proba\",\n",
    "    grid_size=200,\n",
    "    flip=True\n",
    ")\n",
    "post_mlp_dp.fit(X_train_ready, y_train, sensitive_features=A_train)\n",
    "y_mlp_dp = post_mlp_dp.predict(X_test_ready, sensitive_features=A_test)\n",
    "m_mlp_dp = eval_fairness(y_test, y_mlp_dp, A_test)\n",
    "\n",
    "print(\"\\n=== MLP + Post-processing (Demographic Parity) ===\")\n",
    "print(m_mlp_dp[\"by_group\"])\n",
    "print(f\"Accuracy: {m_mlp_dp['acc']:.4f} | DP diff: {m_mlp_dp['dp']:.4f} | EO diff: {m_mlp_dp['eo']:.4f}\")\n",
    "\n",
    "# 3) Summary Table\n",
    "summary_mlp_post = pd.DataFrame([\n",
    "    {\"model\":\"MLP Baseline\",       \"accuracy\":m_mlp_base[\"acc\"], \"dp_diff\":m_mlp_base[\"dp\"], \"eo_diff\":m_mlp_base[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (EO)\",    \"accuracy\":m_mlp_eo[\"acc\"],   \"dp_diff\":m_mlp_eo[\"dp\"],   \"eo_diff\":m_mlp_eo[\"eo\"]},\n",
    "    {\"model\":\"MLP + Post (DP)\",    \"accuracy\":m_mlp_dp[\"acc\"],   \"dp_diff\":m_mlp_dp[\"dp\"],   \"eo_diff\":m_mlp_dp[\"eo\"]},\n",
    "]).round(4)\n",
    "\n",
    "print(\"\\n=== MLP: Baseline vs Post-processing ===\")\n",
    "print(summary_mlp_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d9e15",
   "metadata": {},
   "source": [
    "### MLP — Post-Processing: ThresholdOptimizer\n",
    "\n",
    "#### Summary\n",
    "\n",
    "| Model               | Accuracy | DP diff | EO diff | Notes |\n",
    "|---------------------|:--------:|:-------:|:-------:|-------|\n",
    "| **Baseline (MLP)**  | 0.8850   | 0.0234  | 0.0219  | Already near parity: tiny DP and EO gaps. |\n",
    "| **Post (EO)**       | 0.8850   | 0.0234  | 0.0219  | **No change** vs baseline — 0% flips. |\n",
    "| **Post (DP)**       | 0.8850   | 0.0234  | 0.0219  | **No change** vs baseline — 0% flips. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "- The **baseline MLP** is already very fair: DP ≈ 0.023 (small selection-rate gap) and EO ≈ 0.022 (minimal TPR/FPR differences).  \n",
    "- Applying **ThresholdOptimizer** with either **Equalized Odds** or **Demographic Parity** produced **identical results** to baseline — indicating that the optimizer **did not adjust any thresholds**.  \n",
    "- This typically happens when the classifier’s output probabilities offer **no effective room for group-specific thresholding** (e.g., predictions are already well-calibrated and balanced).\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:** Post-processing **added no benefit** here. Since the baseline is already close to parity, **MLP without post-processing remains the best option**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d93504",
   "metadata": {},
   "source": [
    "## Overall Comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467f1f0",
   "metadata": {},
   "source": [
    "# Overall Bias-Mitigation Comparison (Fairlearn) — Gender Bias in CVD Prediction\n",
    "\n",
    "**Metric keys:**  \n",
    "- **DP diff** (Demographic Parity difference): outcome-rate gap across genders (lower = fairer).  \n",
    "- **EO diff** (Equalized Odds difference): error-rate gap across genders (lower = fairer).  \n",
    "- **Accuracy**: utility measure (higher = better).  \n",
    "\n",
    "---\n",
    "\n",
    "## Aggregated Summary Across Models\n",
    "\n",
    "| Model / Technique                | Accuracy | DP diff | EO diff | Verdict |\n",
    "|----------------------------------|:--------:|:-------:|:-------:|---------|\n",
    "| **KNN Baseline**                 | 0.8900   | 0.0220  | 0.1063  | Strong acc; DP tiny; EO moderate |\n",
    "| KNN + Post (DP)                  | 0.8850   | **0.0104** | **0.0594** | ✅ Best KNN: DP & EO halved, tiny acc cost |\n",
    "| KNN + Post (EO)                  | 0.8900   | 0.0548  | 0.1094  | ❌ DP worsens, EO unchanged |\n",
    "| KNN + CR                         | 0.8400   | 0.0192  | 0.0531  | EO improves but accuracy drops sharply |\n",
    "| **Decision Tree (Alt tuned)**    | 0.9000   | 0.0017  | 0.0594  | Baseline near DP parity; EO moderate |\n",
    "| DT + EG (EO)                     | 0.8900   | 0.0017  | **0.0393** | ✅ Best DT: EO ↓, DP unchanged; slight acc ↓ |\n",
    "| DT + Post (EO/DP)                | 0.8950   | 0.0483 / 0.0567 | 0.1750 / 0.1219 | ❌ Both worsen fairness |\n",
    "| DT + EG (DP) / GS (DP)           | 0.8950   | 0.0308  | 0.0615  | ❌ DP ↑, EO ~baseline |\n",
    "| DT + GS (EO)                     | 0.9000   | 0.0017  | 0.0594  | No effect |\n",
    "| **Random Forest Baseline**       | 0.9250   | 0.0285  | 0.1000  | Strong acc; DP ~0.03; EO moderate (TPR gap) |\n",
    "| RF + EG (EO/DP)                  | 0.9250   | 0.0285  | 0.1000  | No effect |\n",
    "| RF + GS (EO, i=39)               | 0.9450   | 0.0178  | **0.0444** | ✅ Balanced EO/DP, high acc |\n",
    "| RF + GS (DP, i=22)               | **0.9600** | **0.0090** | **0.0444** | ⭐ Best RF: top acc + low DP/EO |\n",
    "| RF + Post (EO/DP)                | 0.9200   | 0.0503  | 0.1000  | ❌ DP worsens, EO unchanged, acc ↓ |\n",
    "| **MLP Baseline**                 | 0.8850   | 0.0234  | 0.0219  | Already very fair (tiny gaps) |\n",
    "| MLP + EG (EO/DP)                 | 0.8850   | 0.0234  | 0.0219  | No change |\n",
    "| MLP + GS (EO)                    | 0.8850   | 0.0234  | 0.0219  | No change |\n",
    "| MLP + GS (DP)                    | 0.8850   | 0.0285  | 0.1063  | ❌ Fairness worsens |\n",
    "| MLP + Post (EO/DP)               | 0.8850   | 0.0234  | 0.0219  | No change |\n",
    "\n",
    "---\n",
    "\n",
    "## What Worked Best\n",
    "\n",
    "- **KNN + Post (DP):**  \n",
    "  Improved both DP and EO at almost no accuracy cost.  \n",
    "\n",
    "- **DT + EG (EO):**  \n",
    "  Reduced EO substantially (0.059 → 0.039) while preserving DP parity; only minor acc trade-off.  \n",
    "\n",
    "- **RF + GS (DP i=22):**  \n",
    "  ⭐ Best overall model — highest accuracy (0.960) with near-zero DP and low EO.  \n",
    "\n",
    "- **RF + GS (EO i=39):**  \n",
    "  Strong balance — EO ≈ 0.044, DP ≈ 0.018, accuracy high (0.945).  \n",
    "\n",
    "- **MLP Baseline:**  \n",
    "  Already essentially fair; further interventions unnecessary.  \n",
    "\n",
    "---\n",
    "\n",
    "## What Did Not Help\n",
    "\n",
    "- **Post-processing for DT (EO/DP):** Worsened both fairness metrics.  \n",
    "- **EG/GS with DP constraint (DT & RF):** Increased DP without helping EO.  \n",
    "- **RF + Post (EO/DP):** Slight accuracy loss, fairness not improved.  \n",
    "- **MLP GS (DP):** Harmed fairness (EO quadrupled).  \n",
    "- **KNN + CR:** Helped EO but at a steep accuracy drop.  \n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implications for CVD Prediction\n",
    "\n",
    "- **KNN:** If used, prefer **Post (DP)** for balanced fairness gains.  \n",
    "- **Decision Tree:** Use **EG (EO)** to mitigate error-rate disparities — critical in clinical contexts where TPR parity reduces risk of gendered underdiagnosis.  \n",
    "- **Random Forest:** Best handled via **GridSearch** frontier models. **DP i=22** is optimal for joint accuracy + fairness; **EO i=39** if error-rate parity is the primary concern.  \n",
    "- **MLP:** Baseline already acceptable; additional fairness interventions are unnecessary and may degrade performance.  \n",
    "\n",
    "---\n",
    "\n",
    "## Final Recommendation\n",
    "\n",
    "For **fair and accurate gender-sensitive CVD prediction**:  \n",
    "- **Primary choice:** **Random Forest + GridSearch (DP i=22)** for top utility + fairness.  \n",
    "- **Secondary choices:**  \n",
    "  - **DT + EG (EO)** if interpretability and error-rate parity are prioritized.  \n",
    "  - **RF + GS (EO i=39)** if lowering EO is the main target.  \n",
    "  - **KNN + Post (DP)** if sticking with KNN.  \n",
    "- **MLP:** stick with the baseline.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
