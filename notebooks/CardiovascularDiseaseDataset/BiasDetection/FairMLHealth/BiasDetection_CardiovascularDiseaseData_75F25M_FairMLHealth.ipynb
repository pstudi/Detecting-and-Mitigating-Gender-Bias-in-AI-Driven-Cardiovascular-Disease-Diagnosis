{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba0b2b9",
   "metadata": {},
   "source": [
    "### Bias Detection and Fairness Evaluation on Cardiovascular Disease (Kaggle) using FairMLhealth\n",
    "(Source: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c29a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load X_test set\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57b21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup successful\n"
     ]
    }
   ],
   "source": [
    "import fairmlhealth\n",
    "import aif360\n",
    "print(\"Environment setup successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f126c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fairmlhealth\n",
      "Version: 1.0.2\n",
      "Summary: Health-centered variation analysis\n",
      "Home-page: https://github.com/KenSciResearch/fairMLHealth\n",
      "Author: Christine Allen\n",
      "Author-email: ca.magallen@gmail.com\n",
      "License: \n",
      "Location: c:\\users\\patri\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: aif360, ipython, jupyter, numpy, pandas, requests, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#have a look at the details of fairmlhealth - especially the version\n",
    "!pip show fairmlhealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76186324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "#have a look at the modules that are within fairmlhealth\n",
    "\n",
    "print(dir(fairmlhealth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44657ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "#load necessary modules \n",
    "\n",
    "#import module measure to use measure.summary for bias detection\n",
    "from fairmlhealth import measure\n",
    "\n",
    "#import module for investigation of individual cohorts \n",
    "from fairmlhealth.__utils import iterate_cohorts\n",
    "\n",
    "#import FairRanges to flag high values\n",
    "from fairmlhealth.__utils import FairRanges\n",
    "\n",
    "# Wrap the fairness summary function for cohort-wise analysis\n",
    "@iterate_cohorts\n",
    "def cohort_summary(**kwargs):\n",
    "    return measure.summary(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb6c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"inFairness\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"AdversarialDebiasing will be unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a099295",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models - KNN & DT\n",
    "\n",
    "#### K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d817251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true    y_prob  y_pred\n",
      "0       0       0  0.344828       0\n",
      "1       0       0  0.862069       1\n",
      "2       1       0  0.379310       0\n",
      "3       0       0  0.379310       0\n",
      "4       0       0  0.275862       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "knn_df = pd.read_csv(\"CVDKaggleData_75F25M__tunedKNN_predictions.csv\")\n",
    "\n",
    "print(knn_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871aa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_knn = knn_df[\"y_true\"].values\n",
    "y_prob_knn = knn_df[\"y_prob\"].values\n",
    "y_pred_knn = knn_df[\"y_pred\"].values\n",
    "gender_knn = knn_df[\"gender\"].values\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_knn = gender_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad78836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0146\n",
      "               Balanced Accuracy Difference            0.0138\n",
      "               Balanced Accuracy Ratio                 1.0198\n",
      "               Disparate Impact Ratio                  1.0999\n",
      "               Equal Odds Difference                   0.0564\n",
      "               Equal Odds Ratio                        1.1090\n",
      "               Positive Predictive Parity Difference   0.0023\n",
      "               Positive Predictive Parity Ratio        1.0033\n",
      "               Statistical Parity Difference           0.0458\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "knn_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_true_knn,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn,\n",
    "    prtc_attr=protected_attr_knn,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,   # skip inconsistency metrics that cause NearestNeighbors error\n",
    "    skip_performance=True\n",
    ")\n",
    "\n",
    "print(knn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ba3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Custom scenario oriented bounds\n",
    "\n",
    "custom_ranges = {\n",
    "    \"tpr diff\": (-0.03, 0.03),\n",
    "    \"fpr diff\": (-0.03, 0.03),\n",
    "    \"equal odds difference\": (-0.04, 0.04),\n",
    "    \"statistical parity difference\": (-0.05, 0.05),\n",
    "    \"disparate impact ratio\": (0.9, 1.1),\n",
    "    \"selection ratio\": (0.9, 1.1),\n",
    "    \"auc difference\": (-0.02, 0.02),\n",
    "    \"balanced accuracy difference\": (-0.02, 0.02),\n",
    "}\n",
    "\n",
    "bounds = FairRanges().load_fair_ranges(custom_ranges=custom_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14d2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  restore Styler.set_precision to adjust the highlighting color in the styled table\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "Styler = type(pd.DataFrame({\"_\":[0]}).style)  \n",
    "\n",
    "if not hasattr(Styler, \"set_precision\"):\n",
    "    def _set_precision(self, precision=4):\n",
    "        try:\n",
    "            return self.format(precision=precision)\n",
    "        except TypeError:\n",
    "            return self.format(formatter=lambda x:\n",
    "                f\"{x:.{precision}g}\" if isinstance(x, (int, float, np.floating)) else x\n",
    "            )\n",
    "    setattr(Styler, \"set_precision\", _set_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90435472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f619b_row4_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f619b\">\n",
       "  <caption>KNN Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f619b_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f619b_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_f619b_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_f619b_row0_col0\" class=\"data row0 col0\" >0.0146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f619b_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_f619b_row1_col0\" class=\"data row1 col0\" >0.0138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f619b_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_f619b_row2_col0\" class=\"data row2 col0\" >1.0198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f619b_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_f619b_row3_col0\" class=\"data row3 col0\" >1.0999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f619b_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_f619b_row4_col0\" class=\"data row4 col0\" >0.0564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f619b_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_f619b_row5_col0\" class=\"data row5 col0\" >1.1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f619b_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_f619b_row6_col0\" class=\"data row6 col0\" >0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f619b_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_f619b_row7_col0\" class=\"data row7 col0\" >1.0033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f619b_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_f619b_row8_col0\" class=\"data row8 col0\" >0.0458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f619b_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_f619b_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_f619b_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x275455c3280>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "from fairmlhealth.__utils import Flagger\n",
    "\n",
    "class MyFlagger(Flagger):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.flag_color = \"#491ee6\"   \n",
    "        self.flag_type = \"background-color\"\n",
    "\n",
    "styled_knn = MyFlagger().apply_flag(\n",
    "    df=knn_bias,\n",
    "    caption=\"KNN Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e135b8e",
   "metadata": {},
   "source": [
    "### Interpretation of KNN Fairness Metrics by Gender\n",
    "\n",
    "The table reports fairness metrics for the **K-Nearest Neighbors (KNN) model**, using gender as the sensitive attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0146):**  \n",
    "  → Very small; the model’s ranking performance is almost identical across genders.\n",
    "\n",
    "- **Balanced Accuracy Difference (0.0138)** and **Ratio (1.0198):**  \n",
    "  → Shows a small advantage for one gender, but the ratio remains close to 1, indicating that balanced accuracy is fairly consistent across groups.\n",
    "\n",
    "- **Disparate Impact Ratio (1.0999):**  \n",
    "  → Indicates that the likelihood of receiving a positive prediction is slightly higher for one group.  \n",
    "  → Still well within the commonly accepted fairness threshold (0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (0.0564)** and **Equal Odds Ratio (1.1090):**  \n",
    "  → This is the **largest disparity observed**. It means that the true positive rate and false positive rate differ more noticeably across genders compared to other metrics, although the difference (≈5.6%) is still moderate.\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0023)** and **Ratio (1.0033):**  \n",
    "  → Precision is nearly equal across genders, with an almost negligible difference.\n",
    "\n",
    "- **Statistical Parity Difference (0.0458):**  \n",
    "  → Indicates a slight imbalance in the probability of being predicted positive, with one gender being favored slightly more.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → The privileged group makes up 35% of the dataset. While the imbalance could affect outcomes, disparities remain relatively small.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The KNN model demonstrates **mostly balanced fairness performance across genders**, with the following nuances:\n",
    "- **Most metrics (AUC, balanced accuracy, predictive parity)** show only minor disparities.  \n",
    "- **Equal Odds Difference (0.0564)** stands out as the largest gap, suggesting that error rates (TPR/FPR) differ more noticeably between genders.  \n",
    "- **Statistical parity and disparate impact** remain within acceptable ranges, meaning that both groups receive positive outcomes at fairly similar rates.\n",
    "\n",
    "In summary, the KNN model is **reasonably fair**, though it introduces a **slight imbalance in error distribution** across gender groups compared to other fairness aspects.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0eeae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - KNN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0138</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>-0.0288</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>-0.0458</td>\n",
       "      <td>0.9092</td>\n",
       "      <td>-0.0564</td>\n",
       "      <td>0.9211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>1.0198</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>1.1090</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>1.0033</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>1.0999</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>1.0856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0138   \n",
       "1       gender             1                        0.0138   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9806   -0.0288     0.9017   -0.0023     0.9967   \n",
       "1                   1.0198    0.0288     1.1090    0.0023     1.0033   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0         -0.0458           0.9092   -0.0564     0.9211  \n",
       "1          0.0458           1.0999    0.0564     1.0856  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - KNN\")\n",
    "measure.bias(X_test, y_test, y_pred_knn, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b169e",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – KNN (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **K-Nearest Neighbors (KNN) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **-0.0138**, Ratio = **0.9806**  \n",
    "- **Male (1):** Difference = **0.0138**, Ratio = **1.0198**  \n",
    "➡️ Males benefit from slightly higher balanced accuracy, while females are at a small disadvantage. The gap is minor (~1–2%).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **-0.0288**, Ratio = **0.9017**  \n",
    "- **Male (1):** FPR Diff = **0.0288**, Ratio = **1.1090**  \n",
    "➡️ Females experience a **lower false positive rate**, meaning fewer incorrect positive classifications.  \n",
    "➡️ Males face a somewhat higher false positive rate.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0023**, Ratio = **0.9967**  \n",
    "- **Male (1):** PPV Diff = **0.0023**, Ratio = **1.0033**  \n",
    "➡️ Precision is **almost identical** across genders, with negligible differences.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **-0.0458**, Ratio = **0.9092**  \n",
    "- **Male (1):** Selection Diff = **0.0458**, Ratio = **1.0999**  \n",
    "➡️ Males are **more likely to be predicted positive** than females.  \n",
    "➡️ Females have a slightly lower chance of being classified as positive cases.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **-0.0564**, Ratio = **0.9211**  \n",
    "- **Male (1):** TPR Diff = **0.0564**, Ratio = **1.0856**  \n",
    "➡️ Males benefit from a **higher recall**, meaning fewer false negatives.  \n",
    "➡️ Females have a somewhat lower recall, missing more true positives.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):**  \n",
    "  - Advantage: Lower false positive rate.  \n",
    "  - Disadvantage: Lower recall (more missed true positives) and lower selection rate.  \n",
    "\n",
    "- **Males (1):**  \n",
    "  - Advantage: Higher recall and higher chance of being predicted positive.  \n",
    "  - Disadvantage: Higher false positive rate.  \n",
    "\n",
    "The KNN model shows a **trade-off**:  \n",
    "- Females are more protected from false alarms but risk missing more true cases.  \n",
    "- Males are detected more often (higher recall, more positives predicted) but also face more false positives.  \n",
    "\n",
    "Overall, disparities are moderate, with the **largest gap in recall (TPR difference ~0.056)**, suggesting a **slight imbalance in error distribution** across genders.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fdeff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.4886</td>\n",
       "      <td>0.7064</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>0.2833</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7087</td>\n",
       "      <td>0.7607</td>\n",
       "      <td>0.6959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.7109</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.2934</td>\n",
       "      <td>0.2533</td>\n",
       "      <td>0.7095</td>\n",
       "      <td>0.7661</td>\n",
       "      <td>0.7153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.4587</td>\n",
       "      <td>0.6977</td>\n",
       "      <td>0.6821</td>\n",
       "      <td>0.2645</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7072</td>\n",
       "      <td>0.7515</td>\n",
       "      <td>0.6588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.4886   \n",
       "1        gender             0   7362.0       0.5004           0.5045   \n",
       "2        gender             1   3894.0       0.4923           0.4587   \n",
       "\n",
       "   Accuracy  F1-Score     FPR  PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7064    0.7023  0.2833       —     0.7087   0.7607  0.6959  \n",
       "1    0.7109    0.7124  0.2934  0.2533     0.7095   0.7661  0.7153  \n",
       "2    0.6977    0.6821  0.2645       —     0.7072   0.7515  0.6588  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fairmlhealth import measure\n",
    "import pandas as pd\n",
    "from IPython.display import display  \n",
    "\n",
    "# convert gender into DataFrame with a clear column name to get a nice table as output\n",
    "gender_df = pd.DataFrame({\"gender\": X_test[\"gender\"].astype(int)})\n",
    "\n",
    "\n",
    "# Get the stratified table\n",
    "perf_table_knn = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_knn = perf_table_knn.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ea758",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – KNN (by Gender)\n",
    "\n",
    "The table shows performance results for the **K-Nearest Neighbors (KNN) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7064  \n",
    "- **F1-Score:** 0.7023  \n",
    "- **Precision:** 0.7087  \n",
    "- **ROC AUC:** 0.7607  \n",
    "- **TPR (Recall):** 0.6959  \n",
    "→ The model performs at a **moderate level**, with a good trade-off between precision and recall.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.7109 (slightly higher than males)  \n",
    "- **F1-Score:** 0.7124 (better balance of precision and recall)  \n",
    "- **FPR:** 0.2934 (higher than males → more false positives)  \n",
    "- **Precision:** 0.7095 (almost identical to males)  \n",
    "- **ROC AUC:** 0.7661 (better discrimination ability than males)  \n",
    "- **TPR (Recall):** 0.7153 (higher recall than males)  \n",
    "\n",
    "➡️ Females benefit from **higher accuracy, F1-score, recall, and ROC AUC**, but at the cost of a **higher false positive rate**.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.6977 (lower than females)  \n",
    "- **F1-Score:** 0.6821 (lower than females)  \n",
    "- **FPR:** 0.2645 (lower than females → fewer false positives)  \n",
    "- **Precision:** 0.7072 (very similar to females)  \n",
    "- **ROC AUC:** 0.7515 (slightly worse than females)  \n",
    "- **TPR (Recall):** 0.6588 (lower recall → more missed positives)  \n",
    "\n",
    "➡️ Males experience **lower recall and overall accuracy**, but benefit from a **lower false positive rate**.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Better overall performance (accuracy, F1, recall, ROC AUC), but face more false positives.  \n",
    "- **Males (1):** Slightly worse overall performance, with fewer false positives but more missed positives.  \n",
    "\n",
    "The KNN model shows a **trade-off in fairness**:  \n",
    "- It favors **females in terms of detection ability (higher recall, better accuracy)**.  \n",
    "- It favors **males in terms of error protection (lower false positives)**.  \n",
    "\n",
    "Overall, disparities are modest, suggesting the model is **reasonably fair but with small gender-specific trade-offs**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5db1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.7153\n",
      "  False Positive Rate (FPR): 0.2934\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.6588\n",
      "  False Positive Rate (FPR): 0.2645\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#group specific error analysis\n",
    "\n",
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_knn == 0)  # unprivileged group (female)\n",
    "male_mask   = (protected_attr_knn == 1)  # privileged group (male)\n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74986e6d",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – KNN Model\n",
    "\n",
    "This section evaluates the classification performance of the KNN model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.7153  | 0.2934  |\n",
    "| Male (Privileged = 1)        | 0.6588  | 0.2645  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Achieve a **higher TPR (71.53%)**, meaning the model detects more true positives for this group.  \n",
    "  - However, they also experience a **higher FPR (29.34%)**, meaning they receive more false positives.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Achieve a **lower TPR (65.88%)**, so the model misses more true positives compared to females.  \n",
    "  - At the same time, they have a **lower FPR (26.45%)**, meaning they are less likely to be incorrectly classified as positive.\n",
    "\n",
    "#### Conclusion\n",
    "The KNN model reveals a **sensitivity–specificity trade-off across genders**:  \n",
    "- **Females** benefit from higher sensitivity (better recall of true positives) but face more false alarms.  \n",
    "- **Males** are protected against false positives but suffer from lower sensitivity, missing more true cases.  \n",
    "\n",
    "Overall, the differences are moderate, suggesting the model is **reasonably fair but slightly imbalanced** in how it distributes errors between genders.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb317ca",
   "metadata": {},
   "source": [
    "### Decision Tree - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adbaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_dt    y_prob\n",
      "0       0       0          0  0.256610\n",
      "1       0       0          1  0.842593\n",
      "2       1       0          0  0.417021\n",
      "3       0       0          0  0.335329\n",
      "4       0       0          0  0.354173\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "dt_df = pd.read_csv(\"CVDKaggleData_75F25M_DT_tuned_predictions.csv\")\n",
    "\n",
    "print(dt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06530601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract common columns\n",
    "y_true_dt = dt_df[\"y_true\"].values\n",
    "y_prob_dt = dt_df[\"y_prob\"].values\n",
    "y_pred_dt = dt_df[\"y_pred_dt\"].values\n",
    "gender_dt = dt_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_dt = gender_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b83e2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Decision Tree Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0153\n",
      "               Balanced Accuracy Difference           -0.0006\n",
      "               Balanced Accuracy Ratio                 0.9992\n",
      "               Disparate Impact Ratio                  0.9925\n",
      "               Equal Odds Difference                  -0.0078\n",
      "               Equal Odds Ratio                        0.9774\n",
      "               Positive Predictive Parity Difference   0.0092\n",
      "               Positive Predictive Parity Ratio        1.0130\n",
      "               Statistical Parity Difference          -0.0038\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Gender Bias Report\n",
    "print(\"\\n--- Decision Tree Gender Bias Report ---\")\n",
    "\n",
    "dt_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt,\n",
    "    prtc_attr=protected_attr_dt,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,  \n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(dt_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc2d88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_292f0\">\n",
       "  <caption>Decision Tree Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_292f0_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_292f0_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_292f0_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_292f0_row0_col0\" class=\"data row0 col0\" >0.0153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_292f0_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_292f0_row1_col0\" class=\"data row1 col0\" >-0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_292f0_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_292f0_row2_col0\" class=\"data row2 col0\" >0.9992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_292f0_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_292f0_row3_col0\" class=\"data row3 col0\" >0.9925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_292f0_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_292f0_row4_col0\" class=\"data row4 col0\" >-0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_292f0_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_292f0_row5_col0\" class=\"data row5 col0\" >0.9774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_292f0_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_292f0_row6_col0\" class=\"data row6 col0\" >0.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_292f0_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_292f0_row7_col0\" class=\"data row7 col0\" >1.0130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_292f0_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_292f0_row8_col0\" class=\"data row8 col0\" >-0.0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_292f0_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_292f0_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_292f0_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x275455c26b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "styled_dt = MyFlagger().apply_flag(\n",
    "    df=dt_bias,\n",
    "    caption=\"Decision Tree Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2830b1e",
   "metadata": {},
   "source": [
    "### Interpretation of Decision Tree Fairness Metrics by Gender\n",
    "\n",
    "The table reports fairness metrics for the **Decision Tree model**, using gender as the sensitive attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0153):**  \n",
    "  → Very small; the model’s ranking ability is nearly identical across genders.\n",
    "\n",
    "- **Balanced Accuracy Difference (-0.0006)** and **Ratio (0.9992):**  \n",
    "  → Essentially no disparity in balanced accuracy; both genders are treated equally well.\n",
    "\n",
    "- **Disparate Impact Ratio (0.9925):**  \n",
    "  → Very close to 1, meaning positive prediction rates are almost equal for both genders. This value lies comfortably within the fairness range (0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (-0.0078)** and **Ratio (0.9774):**  \n",
    "  → Very small difference in error rates (TPR and FPR). The negative sign suggests a **slight advantage for females**, but the magnitude is negligible.\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0092)** and **Ratio (1.0130):**  \n",
    "  → Precision is nearly identical between genders, with a marginal advantage for males.\n",
    "\n",
    "- **Statistical Parity Difference (-0.0038):**  \n",
    "  → Virtually no difference in the probability of receiving a positive prediction. Slightly favors females, but the difference is negligible.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → The privileged group (males) makes up 35% of the dataset, but this imbalance does not translate into fairness violations.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The Decision Tree model demonstrates **excellent fairness across gender groups**:  \n",
    "- All differences are extremely small (well below common fairness concern thresholds).  \n",
    "- Positive outcomes, precision, recall, and error rates are distributed almost equally between genders.  \n",
    "- If anything, there is a **tiny advantage for females** (unprivileged group), but it is so small that it does not indicate systematic bias.\n",
    "\n",
    "In summary, the Decision Tree is **highly fair across genders**, making it one of the most balanced models in terms of fairness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "631b7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - DT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>1.0008</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>1.0231</td>\n",
       "      <td>-0.0092</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>1.0076</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>1.0110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>-0.0066</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>1.0130</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>-0.0078</td>\n",
       "      <td>0.9891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                        0.0006   \n",
       "1       gender             1                       -0.0006   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   1.0008    0.0066     1.0231   -0.0092     0.9871   \n",
       "1                   0.9992   -0.0066     0.9774    0.0092     1.0130   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0038           1.0076    0.0078     1.0110  \n",
       "1         -0.0038           0.9925   -0.0078     0.9891  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - DT\")\n",
    "measure.bias(X_test, y_test, y_pred_dt, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b884a10",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – Decision Tree (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **Decision Tree model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **0.0006**, Ratio = **1.0008**  \n",
    "- **Male (1):** Difference = **-0.0006**, Ratio = **0.9992**  \n",
    "➡️ Balanced accuracy is nearly identical across genders, with differences close to zero.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **0.0066**, Ratio = **1.0231**  \n",
    "- **Male (1):** FPR Diff = **-0.0066**, Ratio = **0.9774**  \n",
    "➡️ Females experience a **slightly higher false positive rate**, while males benefit from slightly fewer false positives.  \n",
    "The difference is very small (~0.6%).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0092**, Ratio = **0.9871**  \n",
    "- **Male (1):** PPV Diff = **0.0092**, Ratio = **1.0130**  \n",
    "➡️ Precision is slightly better for males, but the difference is negligible (<1%).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **0.0038**, Ratio = **1.0076**  \n",
    "- **Male (1):** Selection Diff = **-0.0038**, Ratio = **0.9925**  \n",
    "➡️ Females are slightly **more likely to be predicted positive** than males, but the difference is almost negligible.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **0.0078**, Ratio = **1.0110**  \n",
    "- **Male (1):** TPR Diff = **-0.0078**, Ratio = **0.9891**  \n",
    "➡️ Females achieve a **slightly higher recall**, while males experience slightly more false negatives.  \n",
    "The difference is marginal (~0.8%).\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Small advantages in recall (TPR) and likelihood of being predicted positive, but slightly higher false positives and slightly lower precision.  \n",
    "- **Males (1):** Benefit from fewer false positives and slightly higher precision, but with a minor disadvantage in recall and prediction rate.  \n",
    "\n",
    "All disparities are **extremely small (≤ 1%)**, meaning the Decision Tree model demonstrates **very strong fairness across gender groups**, with no meaningful bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1acccc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.4983</td>\n",
       "      <td>0.7113</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>0.7106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.4970</td>\n",
       "      <td>0.7111</td>\n",
       "      <td>0.7103</td>\n",
       "      <td>0.2858</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.7709</td>\n",
       "      <td>0.7079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.5008</td>\n",
       "      <td>0.7116</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>0.2924</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7036</td>\n",
       "      <td>0.7556</td>\n",
       "      <td>0.7157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.4983   \n",
       "1        gender             0   7362.0       0.5004           0.4970   \n",
       "2        gender             1   3894.0       0.4923           0.5008   \n",
       "\n",
       "   Accuracy  F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7113    0.7101  0.2881      —     0.7096   0.7657  0.7106  \n",
       "1    0.7111    0.7103  0.2858      —     0.7128   0.7709  0.7079  \n",
       "2    0.7116    0.7096  0.2924      —     0.7036   0.7556  0.7157  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_dt = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_dt = perf_table_dt.fillna(\"—\")\n",
    "\n",
    "# Display pretty table\n",
    "display(perf_table_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a06dc",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – Decision Tree (by Gender)\n",
    "\n",
    "The table shows performance results for the **Decision Tree model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7113  \n",
    "- **F1-Score:** 0.7101  \n",
    "- **Precision:** 0.7096  \n",
    "- **ROC AUC:** 0.7657  \n",
    "- **TPR (Recall):** 0.7106  \n",
    "→ The model achieves **solid and balanced predictive performance**, with a good balance of precision and recall.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.7111 (almost the same as males)  \n",
    "- **F1-Score:** 0.7103 (nearly identical)  \n",
    "- **FPR:** 0.2858 (slightly lower than males → fewer false positives)  \n",
    "- **Precision:** 0.7128 (slightly higher than males)  \n",
    "- **ROC AUC:** 0.7709 (slightly better discrimination ability)  \n",
    "- **TPR (Recall):** 0.7079 (very close to males)  \n",
    "\n",
    "➡️ Females show **slightly better precision, ROC AUC, and fewer false positives**, though recall is marginally lower.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.7116 (almost the same as females)  \n",
    "- **F1-Score:** 0.7096 (virtually identical)  \n",
    "- **FPR:** 0.2924 (slightly higher than females → more false positives)  \n",
    "- **Precision:** 0.7036 (slightly lower than females)  \n",
    "- **ROC AUC:** 0.7556 (slightly lower discrimination ability)  \n",
    "- **TPR (Recall):** 0.7157 (slightly higher than females)  \n",
    "\n",
    "➡️ Males achieve **slightly higher recall**, meaning fewer missed positives, but at the cost of **lower precision and higher false positive rate**.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Small advantage in precision, ROC AUC, and fewer false positives.  \n",
    "- **Males (1):** Small advantage in recall, detecting slightly more true positives, but with more false alarms.  \n",
    "\n",
    "The Decision Tree model demonstrates **very balanced performance across genders**, with differences so small that they indicate **no meaningful gender bias**, only minor trade-offs between recall and precision.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53556439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.7079\n",
      "  False Positive Rate (FPR): 0.2858\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.7157\n",
      "  False Positive Rate (FPR): 0.2924\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_dt == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_dt == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e7685",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Decision Tree Model\n",
    "\n",
    "This section evaluates the classification performance of the Decision Tree model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.7079  | 0.2858  |\n",
    "| Male (Privileged = 1)        | 0.7157  | 0.2924  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Achieve a **TPR of 70.79%**, which is slightly lower than males.  \n",
    "  - Benefit from a **lower FPR (28.58%)**, meaning they receive fewer false positives.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Achieve a **TPR of 71.57%**, slightly higher than females, meaning more true positives are detected.  \n",
    "  - However, they also have a **higher FPR (29.24%)**, meaning they are more often misclassified as positive.\n",
    "\n",
    "#### Conclusion\n",
    "The Decision Tree model demonstrates a **small trade-off in error distribution**:  \n",
    "- **Males** gain slightly higher sensitivity (recall) but also face more false alarms.  \n",
    "- **Females** avoid more false positives but miss slightly more true cases.  \n",
    "\n",
    "The differences are **minimal**, indicating the model maintains **strong gender fairness** with only negligible performance trade-offs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5168d",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaa2a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_rf    y_prob\n",
      "0       0       0          0  0.284528\n",
      "1       0       0          1  0.804516\n",
      "2       1       0          0  0.347408\n",
      "3       0       0          0  0.272444\n",
      "4       0       0          1  0.537651\n"
     ]
    }
   ],
   "source": [
    "rf_df = pd.read_csv(\"CVDKaggleData_75F25M_RF_tuned_predictions.csv\")\n",
    "print(rf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "278af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_rf = rf_df[\"y_true\"].values\n",
    "y_pred_rf = rf_df[\"y_pred_rf\"].values\n",
    "y_prob_rf = rf_df[\"y_prob\"].values\n",
    "gender_rf = rf_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_rf = gender_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "689f0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0131\n",
      "               Balanced Accuracy Difference            0.0099\n",
      "               Balanced Accuracy Ratio                 1.0142\n",
      "               Disparate Impact Ratio                  0.9795\n",
      "               Equal Odds Difference                  -0.0228\n",
      "               Equal Odds Ratio                        0.9159\n",
      "               Positive Predictive Parity Difference   0.0235\n",
      "               Positive Predictive Parity Ratio        1.0332\n",
      "               Statistical Parity Difference          -0.0096\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Gender Bias Report\n",
    "print(\"\\n--- Random Forest Gender Bias Report ---\")\n",
    "\n",
    "rf_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf,\n",
    "    prtc_attr=protected_attr_rf,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(rf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7a8e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_48ba7\">\n",
       "  <caption>Random Forest Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_48ba7_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_48ba7_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_48ba7_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_48ba7_row0_col0\" class=\"data row0 col0\" >0.0131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48ba7_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_48ba7_row1_col0\" class=\"data row1 col0\" >0.0099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48ba7_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_48ba7_row2_col0\" class=\"data row2 col0\" >1.0142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48ba7_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_48ba7_row3_col0\" class=\"data row3 col0\" >0.9795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48ba7_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_48ba7_row4_col0\" class=\"data row4 col0\" >-0.0228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48ba7_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_48ba7_row5_col0\" class=\"data row5 col0\" >0.9159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48ba7_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_48ba7_row6_col0\" class=\"data row6 col0\" >0.0235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48ba7_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_48ba7_row7_col0\" class=\"data row7 col0\" >1.0332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48ba7_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_48ba7_row8_col0\" class=\"data row8 col0\" >-0.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48ba7_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_48ba7_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_48ba7_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x275457b1420>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for Random Forest\n",
    "styled_rf = MyFlagger().apply_flag(\n",
    "    df=rf_bias,\n",
    "    caption=\"Random Forest Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f00c6",
   "metadata": {},
   "source": [
    "### Interpretation of Random Forest Fairness Metrics by Gender\n",
    "\n",
    "The table reports fairness metrics for the **Random Forest (RF) model**, using gender as the sensitive attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0131):**  \n",
    "  → Very small; the model’s ranking ability is nearly the same across genders.\n",
    "\n",
    "- **Balanced Accuracy Difference (0.0099)** and **Ratio (1.0142):**  \n",
    "  → Indicates a very slight advantage for one gender in balanced accuracy, though the difference is minimal.\n",
    "\n",
    "- **Disparate Impact Ratio (0.9795):**  \n",
    "  → Close to 1, suggesting that both genders have nearly equal chances of receiving a positive prediction. This value lies well within the acceptable fairness range (0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (-0.0228)** and **Equal Odds Ratio (0.9159):**  \n",
    "  → Shows some disparity in error rates (TPR and FPR). The negative difference indicates a slight advantage for females, but the difference is still relatively small (~2%).\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0235)** and **Ratio (1.0332):**  \n",
    "  → Precision is slightly higher for males, but the difference remains minor.\n",
    "\n",
    "- **Statistical Parity Difference (-0.0096):**  \n",
    "  → Almost negligible difference in the rate of positive predictions, slightly favoring females.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → The privileged group (males) represents 35% of the dataset. This imbalance does not translate into major fairness concerns in the metrics.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The Random Forest model demonstrates **fairness performance that is strong overall but slightly less balanced than Decision Tree or MLP**:  \n",
    "- Most differences are small and within acceptable thresholds.  \n",
    "- **Females** seem to have a slight advantage in terms of error rates (Equal Odds), while **males** show a small advantage in precision.  \n",
    "- The **largest disparity** appears in **Equal Odds Difference (-0.0228)**, but this is still relatively minor.\n",
    "\n",
    "Overall, the Random Forest model is **reasonably fair**, with no strong systematic gender bias, but with slightly more noticeable differences than the most balanced models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47479386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - RF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>0.9860</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>1.0918</td>\n",
       "      <td>-0.0235</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>1.0210</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.0044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>1.0142</td>\n",
       "      <td>-0.0228</td>\n",
       "      <td>0.9159</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>1.0332</td>\n",
       "      <td>-0.0096</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.9956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0099   \n",
       "1       gender             1                        0.0099   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9860    0.0228     1.0918   -0.0235     0.9679   \n",
       "1                   1.0142   -0.0228     0.9159    0.0235     1.0332   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0096           1.0210     0.003     1.0044  \n",
       "1         -0.0096           0.9795    -0.003     0.9956  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - RF\")\n",
    "measure.bias(X_test, y_test, y_pred_rf, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da34894",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – Random Forest (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **Random Forest model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **-0.0099**, Ratio = **0.9860**  \n",
    "- **Male (1):** Difference = **0.0099**, Ratio = **1.0142**  \n",
    "➡️ Males have a slight advantage in balanced accuracy (~1%), while females are at a small disadvantage.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **0.0228**, Ratio = **1.0918**  \n",
    "- **Male (1):** FPR Diff = **-0.0228**, Ratio = **0.9159**  \n",
    "➡️ Females face a **slightly higher false positive rate**, while males benefit from fewer false positives.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0235**, Ratio = **0.9679**  \n",
    "- **Male (1):** PPV Diff = **0.0235**, Ratio = **1.0332**  \n",
    "➡️ Males have a **small precision advantage**, meaning their positive predictions are slightly more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **0.0096**, Ratio = **1.0210**  \n",
    "- **Male (1):** Selection Diff = **-0.0096**, Ratio = **0.9795**  \n",
    "➡️ Females are slightly **more likely to be predicted positive** than males.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **0.003**, Ratio = **1.0044**  \n",
    "- **Male (1):** TPR Diff = **-0.003**, Ratio = **0.9956**  \n",
    "➡️ Recall is nearly identical across genders, with females having a marginally higher TPR.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Slight advantages in **recall** and **selection rate**, but at the cost of a **higher false positive rate** and **slightly lower precision**.  \n",
    "- **Males (1):** Benefit from **fewer false positives** and **higher precision**, but have a marginally lower recall and are slightly less likely to be predicted positive.  \n",
    "\n",
    "The disparities are **very small (mostly around 1–2%)**, meaning the Random Forest model shows **reasonably balanced fairness across gender groups**, with no major bias but some **minor trade-offs** in error distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6573254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.4631</td>\n",
       "      <td>0.7075</td>\n",
       "      <td>0.6956</td>\n",
       "      <td>0.2568</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7215</td>\n",
       "      <td>0.7651</td>\n",
       "      <td>0.6715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.4598</td>\n",
       "      <td>0.7108</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.2488</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7297</td>\n",
       "      <td>0.7697</td>\n",
       "      <td>0.6705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.4694</td>\n",
       "      <td>0.7013</td>\n",
       "      <td>0.6895</td>\n",
       "      <td>0.2716</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7062</td>\n",
       "      <td>0.7566</td>\n",
       "      <td>0.6734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.4631   \n",
       "1        gender             0   7362.0       0.5004           0.4598   \n",
       "2        gender             1   3894.0       0.4923           0.4694   \n",
       "\n",
       "   Accuracy  F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7075    0.6956  0.2568      —     0.7215   0.7651  0.6715  \n",
       "1    0.7108    0.6988  0.2488      —     0.7297   0.7697  0.6705  \n",
       "2    0.7013    0.6895  0.2716      —     0.7062   0.7566  0.6734  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_rf = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_rf = perf_table_rf.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae69fb",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – Random Forest (by Gender)\n",
    "\n",
    "The table shows performance results for the **Random Forest model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7075  \n",
    "- **F1-Score:** 0.6956  \n",
    "- **Precision:** 0.7215  \n",
    "- **ROC AUC:** 0.7651  \n",
    "- **TPR (Recall):** 0.6715  \n",
    "→ The Random Forest achieves **solid predictive performance**, balancing precision and recall effectively.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.7108 (slightly higher than males)  \n",
    "- **F1-Score:** 0.6988 (slightly higher than males)  \n",
    "- **FPR:** 0.2488 (lower false positive rate than males)  \n",
    "- **Precision:** 0.7297 (higher than males)  \n",
    "- **ROC AUC:** 0.7697 (slightly better discrimination ability)  \n",
    "- **TPR (Recall):** 0.6705 (very close to males)  \n",
    "\n",
    "➡️ Females achieve **better overall performance** in accuracy, F1, precision, and ROC AUC, while also benefiting from a **lower false positive rate**. Recall is nearly identical to males.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.7013 (lower than females)  \n",
    "- **F1-Score:** 0.6895 (lower than females)  \n",
    "- **FPR:** 0.2716 (higher false positive rate)  \n",
    "- **Precision:** 0.7062 (lower than females)  \n",
    "- **ROC AUC:** 0.7566 (slightly lower discrimination ability)  \n",
    "- **TPR (Recall):** 0.6734 (almost identical to females)  \n",
    "\n",
    "➡️ Males perform **slightly worse across most metrics**, with more false positives and lower precision. Recall remains essentially the same as for females.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Stronger performance across accuracy, F1, precision, and ROC AUC, plus fewer false positives.  \n",
    "- **Males (1):** Weaker across most metrics, with higher false positives and lower precision, but nearly equal recall.  \n",
    "\n",
    "The Random Forest model shows a **small but consistent performance advantage for females**, while maintaining **fairly balanced recall across genders**. This indicates **no severe bias**, though the model leans slightly in favor of females in terms of predictive reliability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a52946a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6705\n",
      "  False Positive Rate (FPR): 0.2488\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.6734\n",
      "  False Positive Rate (FPR): 0.2716\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_rf == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_rf == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31192e69",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Random Forest Model\n",
    "\n",
    "This section evaluates the classification performance of the Random Forest model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.6705  | 0.2488  |\n",
    "| Male (Privileged = 1)        | 0.6734  | 0.2716  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Achieve a **TPR of 67.05%**, almost identical to males.  \n",
    "  - Benefit from a **lower FPR (24.88%)**, meaning fewer false positives compared to males.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Achieve a **TPR of 67.34%**, slightly higher than females, meaning marginally better recall.  \n",
    "  - However, they also experience a **higher FPR (27.16%)**, meaning they are more often incorrectly classified as positive.\n",
    "\n",
    "#### Conclusion\n",
    "The Random Forest model achieves **very balanced recall across genders**:  \n",
    "- **Males** gain a very small advantage in sensitivity (recall).  \n",
    "- **Females** benefit from fewer false alarms (lower FPR).  \n",
    "\n",
    "The differences are **minimal** (≈0.3% in TPR, ≈2.3% in FPR), suggesting the model is **highly fair across gender groups** with only negligible trade-offs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136de6",
   "metadata": {},
   "source": [
    "### Deep Learning Model - Feed Forward Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c69d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred    y_prob\n",
      "0       0       0       0  0.332016\n",
      "1       0       0       1  0.861841\n",
      "2       1       0       0  0.411947\n",
      "3       0       0       0  0.338920\n",
      "4       0       0       0  0.319818\n"
     ]
    }
   ],
   "source": [
    "mlp_df = pd.read_csv(\"CVDKaggleData_75F25M_MLP_adamtuned_predictions.csv\")\n",
    "print(mlp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f65a498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns \n",
    "y_true_mlp = mlp_df[\"y_true\"].values \n",
    "y_prob_mlp = mlp_df[\"y_prob\"].values\n",
    "y_pred_mlp = mlp_df[\"y_pred\"].values\n",
    "gender_mlp = mlp_df[\"gender\"].values \n",
    "\n",
    "# Use gender_mlp as the protected attribute\n",
    "protected_attr_mlp = gender_mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6969bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0133\n",
      "               Balanced Accuracy Difference            0.0099\n",
      "               Balanced Accuracy Ratio                 1.0141\n",
      "               Disparate Impact Ratio                  0.9429\n",
      "               Equal Odds Difference                  -0.0421\n",
      "               Equal Odds Ratio                        0.8616\n",
      "               Positive Predictive Parity Difference   0.0308\n",
      "               Positive Predictive Parity Ratio        1.0444\n",
      "               Statistical Parity Difference          -0.0289\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "#Run fairmlhealth bias detection for MLP \n",
    "\n",
    "mlp_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp,\n",
    "    prtc_attr=protected_attr_mlp,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(mlp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbf7eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e27c2_row4_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e27c2\">\n",
       "  <caption>MLP Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e27c2_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e27c2_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_e27c2_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_e27c2_row0_col0\" class=\"data row0 col0\" >0.0133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e27c2_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_e27c2_row1_col0\" class=\"data row1 col0\" >0.0099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e27c2_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_e27c2_row2_col0\" class=\"data row2 col0\" >1.0141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e27c2_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_e27c2_row3_col0\" class=\"data row3 col0\" >0.9429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e27c2_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_e27c2_row4_col0\" class=\"data row4 col0\" >-0.0421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e27c2_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_e27c2_row5_col0\" class=\"data row5 col0\" >0.8616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e27c2_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_e27c2_row6_col0\" class=\"data row6 col0\" >0.0308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e27c2_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_e27c2_row7_col0\" class=\"data row7 col0\" >1.0444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e27c2_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_e27c2_row8_col0\" class=\"data row8 col0\" >-0.0289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e27c2_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_e27c2_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_e27c2_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x27545804670>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for MLP\n",
    "styled_mlp = MyFlagger().apply_flag(\n",
    "    df=mlp_bias,\n",
    "    caption=\"MLP Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8a2e5",
   "metadata": {},
   "source": [
    "### Interpretation of MLP Fairness Metrics by Gender\n",
    "\n",
    "The table reports fairness metrics for the **Multilayer Perceptron (MLP) model**, using gender as the sensitive attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0133):**  \n",
    "  → Very small; ranking performance is nearly identical across genders.\n",
    "\n",
    "- **Balanced Accuracy Difference (0.0099)** and **Ratio (1.0141):**  \n",
    "  → Balanced accuracy is almost equal across genders, with only a ~1% difference.\n",
    "\n",
    "- **Disparate Impact Ratio (0.9429):**  \n",
    "  → Slightly below 1, suggesting that females are somewhat less likely to receive positive predictions than males.  \n",
    "  → Still within the generally acceptable fairness range (0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (-0.0421)** and **Equal Odds Ratio (0.8616):**  \n",
    "  → This is the **largest disparity** observed. The negative value indicates that females have a slight advantage in error rates (TPR/FPR).  \n",
    "  → However, the ratio being closer to 0.86 shows some imbalance in how errors are distributed.\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0308)** and **Ratio (1.0444):**  \n",
    "  → Precision is marginally higher for males, meaning their positive predictions are a bit more reliable.\n",
    "\n",
    "- **Statistical Parity Difference (-0.0289):**  \n",
    "  → Indicates females are predicted positive at a slightly lower rate than males, but the difference is modest.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → Males make up 35% of the dataset, but this imbalance does not fully account for the fairness gaps.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The MLP model achieves **reasonably balanced fairness**, but with **some small disparities**:  \n",
    "- **Females**: Slight advantage in error rates (equal odds difference negative), but are **less likely to be predicted positive** overall.  \n",
    "- **Males**: Small advantage in precision and positive prediction rate.  \n",
    "\n",
    "The most notable metric is **Equal Odds Difference (-0.0421)**, suggesting some imbalance in error distribution. Still, all disparities remain **relatively minor**, keeping the model within **acceptable fairness bounds**, though less balanced than the Decision Tree model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aa66097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - MLP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0099</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>1.1606</td>\n",
       "      <td>-0.0308</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>1.0606</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>1.0322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>1.0141</td>\n",
       "      <td>-0.0421</td>\n",
       "      <td>0.8616</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>1.0444</td>\n",
       "      <td>-0.0289</td>\n",
       "      <td>0.9429</td>\n",
       "      <td>-0.0223</td>\n",
       "      <td>0.9688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0099   \n",
       "1       gender             1                        0.0099   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9861    0.0421     1.1606   -0.0308     0.9575   \n",
       "1                   1.0141   -0.0421     0.8616    0.0308     1.0444   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0289           1.0606    0.0223     1.0322  \n",
       "1         -0.0289           0.9429   -0.0223     0.9688  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - MLP\")\n",
    "measure.bias(X_test, y_test, y_pred_mlp, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efef5a5",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – MLP (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **Multilayer Perceptron (MLP) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **-0.0099**, Ratio = **0.9861**  \n",
    "- **Male (1):** Difference = **0.0099**, Ratio = **1.0141**  \n",
    "➡️ Males show a slight advantage in balanced accuracy (~1%), while females are marginally disadvantaged.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **0.0421**, Ratio = **1.1606**  \n",
    "- **Male (1):** FPR Diff = **-0.0421**, Ratio = **0.8616**  \n",
    "➡️ Females experience a **higher false positive rate**, meaning they are more often incorrectly classified as positive.  \n",
    "➡️ Males benefit from fewer false positives.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0308**, Ratio = **0.9575**  \n",
    "- **Male (1):** PPV Diff = **0.0308**, Ratio = **1.0444**  \n",
    "➡️ Males achieve **higher precision**, meaning their positive predictions are more reliable.  \n",
    "➡️ Females show a small disadvantage in precision.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **0.0289**, Ratio = **1.0606**  \n",
    "- **Male (1):** Selection Diff = **-0.0289**, Ratio = **0.9429**  \n",
    "➡️ Females are **slightly more likely to be predicted positive** compared to males.  \n",
    "➡️ Males are less frequently predicted positive.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **0.0223**, Ratio = **1.0322**  \n",
    "- **Male (1):** TPR Diff = **-0.0223**, Ratio = **0.9688**  \n",
    "➡️ Females achieve **slightly higher recall**, detecting more true positives.  \n",
    "➡️ Males experience slightly more false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):**  \n",
    "  - Advantages: Slightly **higher recall** and **higher selection rate**.  \n",
    "  - Disadvantages: **Higher false positive rate** and **lower precision**.  \n",
    "\n",
    "- **Males (1):**  \n",
    "  - Advantages: **Fewer false positives** and **higher precision**.  \n",
    "  - Disadvantages: Slightly **lower recall** and **less likely to be predicted positive**.  \n",
    "\n",
    "The disparities are **modest (mostly 2–4%)**, indicating the MLP model is **fair overall** but introduces a **small trade-off**:  \n",
    "- **Females** are detected more often but at the cost of more false alarms.  \n",
    "- **Males** enjoy more reliable predictions but risk missing more true positives.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbc2caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.4868</td>\n",
       "      <td>0.7107</td>\n",
       "      <td>0.7061</td>\n",
       "      <td>0.2771</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7140</td>\n",
       "      <td>0.7679</td>\n",
       "      <td>0.6984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>0.7142</td>\n",
       "      <td>0.7075</td>\n",
       "      <td>0.2624</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7251</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.5056</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.7036</td>\n",
       "      <td>0.3045</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6943</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.7131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.4868   \n",
       "1        gender             0   7362.0       0.5004           0.4768   \n",
       "2        gender             1   3894.0       0.4923           0.5056   \n",
       "\n",
       "   Accuracy  F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7107    0.7061  0.2771      —     0.7140   0.7679  0.6984  \n",
       "1    0.7142    0.7075  0.2624      —     0.7251   0.7725  0.6908  \n",
       "2    0.7042    0.7036  0.3045      —     0.6943   0.7592  0.7131  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_mlp = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_mlp = perf_table_mlp.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7c382",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – MLP (by Gender)\n",
    "\n",
    "The table shows performance results for the **Multilayer Perceptron (MLP) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7107  \n",
    "- **F1-Score:** 0.7061  \n",
    "- **Precision:** 0.7140  \n",
    "- **ROC AUC:** 0.7679  \n",
    "- **TPR (Recall):** 0.6984  \n",
    "→ The MLP achieves **solid performance**, balancing recall and precision with good discrimination ability.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.7142 (slightly higher than males)  \n",
    "- **F1-Score:** 0.7075 (slightly higher)  \n",
    "- **FPR:** 0.2624 (lower false positive rate)  \n",
    "- **Precision:** 0.7251 (higher precision)  \n",
    "- **ROC AUC:** 0.7725 (better discrimination ability)  \n",
    "- **TPR (Recall):** 0.6908 (slightly lower than males)  \n",
    "\n",
    "➡️ Females benefit from **better accuracy, precision, and ROC AUC**, and fewer false positives. Their recall is slightly lower compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.7042 (lower than females)  \n",
    "- **F1-Score:** 0.7036 (slightly lower)  \n",
    "- **FPR:** 0.3045 (higher false positive rate)  \n",
    "- **Precision:** 0.6943 (lower precision)  \n",
    "- **ROC AUC:** 0.7592 (weaker discrimination ability)  \n",
    "- **TPR (Recall):** 0.7131 (slightly higher than females)  \n",
    "\n",
    "➡️ Males achieve **better recall (TPR)**, meaning fewer missed positives, but at the cost of **higher false positives and lower precision**.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Stronger performance in precision, accuracy, ROC AUC, and fewer false alarms.  \n",
    "- **Males (1):** Advantage in recall, detecting more positives, but suffer from more false positives and less reliable predictions.  \n",
    "\n",
    "The MLP model demonstrates a **small performance trade-off**:  \n",
    "- **Females** → more reliable predictions (higher precision, fewer false positives).  \n",
    "- **Males** → better sensitivity (higher recall), but with more false alarms.  \n",
    "\n",
    "The differences are **moderate but not extreme**, suggesting the MLP remains **reasonably fair across genders**, with minor imbalances in error distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379dbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6908\n",
      "  False Positive Rate (FPR): 0.2624\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.7131\n",
      "  False Positive Rate (FPR): 0.3045\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_mlp == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_mlp == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abb196",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – MLP Model\n",
    "\n",
    "This section evaluates the classification performance of the MLP model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.6908  | 0.2624  |\n",
    "| Male (Privileged = 1)        | 0.7131  | 0.3045  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Achieve a **TPR of 69.08%**, slightly lower than males.  \n",
    "  - Benefit from a **lower FPR (26.24%)**, meaning fewer false positives.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Achieve a **TPR of 71.31%**, slightly higher than females, meaning more true positives are detected.  \n",
    "  - However, they also suffer from a **higher FPR (30.45%)**, meaning more false positives occur.\n",
    "\n",
    "#### Conclusion\n",
    "The MLP model reveals a **balanced but trade-off relationship**:  \n",
    "- **Males** → higher sensitivity (better recall), but at the cost of more false alarms.  \n",
    "- **Females** → fewer false positives, but at the cost of slightly reduced recall.  \n",
    "\n",
    "The differences (≈2.2% in TPR, ≈4.2% in FPR) are **small**, suggesting the model maintains **fair performance across genders** with only minor imbalances in error distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4a1b0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
