{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba0b2b9",
   "metadata": {},
   "source": [
    "### Bias Detection and Fairness Evaluation on Cardiovascular Disease (Kaggle) using FairMLhealth\n",
    "(Source: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c29a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load X_test set\n",
    "X_test = pd.read_csv(\"./data_splits/X_test.csv\")\n",
    "y_test = pd.read_csv(\"./data_splits/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57b21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup successful\n"
     ]
    }
   ],
   "source": [
    "import fairmlhealth\n",
    "import aif360\n",
    "print(\"Environment setup successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f126c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fairmlhealth\n",
      "Version: 1.0.2\n",
      "Summary: Health-centered variation analysis\n",
      "Home-page: https://github.com/KenSciResearch/fairMLHealth\n",
      "Author: Christine Allen\n",
      "Author-email: ca.magallen@gmail.com\n",
      "License: \n",
      "Location: c:\\users\\patri\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: aif360, ipython, jupyter, numpy, pandas, requests, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#have a look at the details of fairmlhealth - especially the version\n",
    "!pip show fairmlhealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76186324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "#have a look at the modules that are within fairmlhealth\n",
    "\n",
    "print(dir(fairmlhealth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44657ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "#load necessary modules \n",
    "\n",
    "#import module measure to use measure.summary for bias detection\n",
    "from fairmlhealth import measure\n",
    "\n",
    "#import module for investigation of individual cohorts \n",
    "from fairmlhealth.__utils import iterate_cohorts\n",
    "\n",
    "#import FairRanges to flag high values\n",
    "from fairmlhealth.__utils import FairRanges\n",
    "\n",
    "# Wrap the fairness summary function for cohort-wise analysis\n",
    "@iterate_cohorts\n",
    "def cohort_summary(**kwargs):\n",
    "    return measure.summary(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb6c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"inFairness\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"AdversarialDebiasing will be unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a099295",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models - KNN & DT\n",
    "\n",
    "#### K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d817251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_prob  y_pred\n",
      "0       0       0    0.40       0\n",
      "1       0       0    0.75       1\n",
      "2       1       0    0.20       0\n",
      "3       0       0    0.40       0\n",
      "4       0       0    1.00       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "knn_df = pd.read_csv(\"CVDKaggleData_75M25F_PCAKNN_predictions.csv\")\n",
    "\n",
    "print(knn_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871aa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_knn = knn_df[\"y_true\"].values\n",
    "y_prob_knn = knn_df[\"y_prob\"].values\n",
    "y_pred_knn = knn_df[\"y_pred\"].values\n",
    "gender_knn = knn_df[\"gender\"].values\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_knn = gender_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad78836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0064\n",
      "               Balanced Accuracy Difference            0.0095\n",
      "               Balanced Accuracy Ratio                 1.0141\n",
      "               Disparate Impact Ratio                  1.0780\n",
      "               Equal Odds Difference                   0.0425\n",
      "               Equal Odds Ratio                        1.0817\n",
      "               Positive Predictive Parity Difference   0.0040\n",
      "               Positive Predictive Parity Ratio        1.0059\n",
      "               Statistical Parity Difference           0.0358\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "knn_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_true_knn,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn,\n",
    "    prtc_attr=protected_attr_knn,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,   # skip inconsistency metrics that cause NearestNeighbors error\n",
    "    skip_performance=True\n",
    ")\n",
    "\n",
    "print(knn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ba3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Custom scenario oriented bounds\n",
    "\n",
    "custom_ranges = {\n",
    "    \"tpr diff\": (-0.03, 0.03),\n",
    "    \"fpr diff\": (-0.03, 0.03),\n",
    "    \"equal odds difference\": (-0.04, 0.04),\n",
    "    \"statistical parity difference\": (-0.05, 0.05),\n",
    "    \"disparate impact ratio\": (0.9, 1.1),\n",
    "    \"selection ratio\": (0.9, 1.1),\n",
    "    \"auc difference\": (-0.02, 0.02),\n",
    "    \"balanced accuracy difference\": (-0.02, 0.02),\n",
    "}\n",
    "\n",
    "bounds = FairRanges().load_fair_ranges(custom_ranges=custom_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14d2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  restore Styler.set_precision to adjust the highlighting color in the styled table\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "Styler = type(pd.DataFrame({\"_\":[0]}).style)  \n",
    "\n",
    "if not hasattr(Styler, \"set_precision\"):\n",
    "    def _set_precision(self, precision=4):\n",
    "        try:\n",
    "            return self.format(precision=precision)\n",
    "        except TypeError:\n",
    "            return self.format(formatter=lambda x:\n",
    "                f\"{x:.{precision}g}\" if isinstance(x, (int, float, np.floating)) else x\n",
    "            )\n",
    "    setattr(Styler, \"set_precision\", _set_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90435472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_65fea_row4_col0 {\n",
       "  background-color: #491ee6;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_65fea\">\n",
       "  <caption>KNN Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_65fea_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_65fea_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_65fea_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_65fea_row0_col0\" class=\"data row0 col0\" >0.0064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_65fea_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_65fea_row1_col0\" class=\"data row1 col0\" >0.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_65fea_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_65fea_row2_col0\" class=\"data row2 col0\" >1.0141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_65fea_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_65fea_row3_col0\" class=\"data row3 col0\" >1.0780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_65fea_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_65fea_row4_col0\" class=\"data row4 col0\" >0.0425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_65fea_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_65fea_row5_col0\" class=\"data row5 col0\" >1.0817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_65fea_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_65fea_row6_col0\" class=\"data row6 col0\" >0.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_65fea_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_65fea_row7_col0\" class=\"data row7 col0\" >1.0059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_65fea_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_65fea_row8_col0\" class=\"data row8 col0\" >0.0358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_65fea_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_65fea_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_65fea_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x18fbea2dea0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "from fairmlhealth.__utils import Flagger\n",
    "\n",
    "class MyFlagger(Flagger):\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.flag_color = \"#491ee6\"   \n",
    "        self.flag_type = \"background-color\"\n",
    "\n",
    "styled_knn = MyFlagger().apply_flag(\n",
    "    df=knn_bias,\n",
    "    caption=\"KNN Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e135b8e",
   "metadata": {},
   "source": [
    "### Interpretation of KNN Fairness Metrics by Gender\n",
    "\n",
    "The table summarizes multiple fairness metrics for the KNN model with **gender** as the sensitive attribute.\n",
    "\n",
    "#### Key Observations:\n",
    "\n",
    "- **AUC Difference (0.0064)** and **Balanced Accuracy Difference (0.0095)** are very small.  \n",
    "  → The model performs almost equally well across gender groups in terms of ranking ability and balanced accuracy.\n",
    "\n",
    "- **Balanced Accuracy Ratio (1.0141)** is close to 1, further confirming parity in performance across groups.\n",
    "\n",
    "- **Disparate Impact Ratio (1.0780)** is slightly above 1.  \n",
    "  → This means the positive outcome rate is a bit higher for the unprivileged group, but still within the commonly accepted fairness range (0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (0.0425)** is modest.  \n",
    "  → Suggests a small difference in error rates (TPR and FPR) between genders, though not extreme.\n",
    "\n",
    "- **Equal Odds Ratio (1.0817)** also indicates near parity, with only minor deviation between groups.\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0040)** and **Ratio (1.0059)** are almost negligible.  \n",
    "  → The precision (likelihood that a predicted positive is correct) is essentially the same for both genders.\n",
    "\n",
    "- **Statistical Parity Difference (0.0358)** is small.  \n",
    "  → The overall probability of receiving a positive prediction differs only slightly between genders.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%)**: The privileged group constitutes 35% of the dataset.  \n",
    "  → This class imbalance may influence fairness metrics but does not appear to create strong disparities.\n",
    "\n",
    "#### Overall Conclusion:\n",
    "The KNN model shows **minor fairness disparities across gender**, but all differences remain relatively small and within acceptable ranges. Metrics like **Equal Odds Difference (0.0425)** and **Disparate Impact Ratio (1.0780)** indicate some variation, yet the overall fairness performance suggests that the model does not exhibit strong gender bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0eeae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - KNN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0095</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>-0.0234</td>\n",
       "      <td>0.9245</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>-0.0358</td>\n",
       "      <td>0.9277</td>\n",
       "      <td>-0.0425</td>\n",
       "      <td>0.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>1.0141</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>1.0817</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.0059</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>1.0780</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>1.0667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0095   \n",
       "1       gender             1                        0.0095   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9861   -0.0234     0.9245    -0.004     0.9942   \n",
       "1                   1.0141    0.0234     1.0817     0.004     1.0059   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0         -0.0358           0.9277   -0.0425     0.9375  \n",
       "1          0.0358           1.0780    0.0425     1.0667  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - KNN\")\n",
    "measure.bias(X_test, y_test, y_pred_knn, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b169e",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – KNN (by Gender)\n",
    "\n",
    "The table reports **bias metrics stratified by gender groups (0 = unprivileged, 1 = privileged)** for the KNN model.\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Gender 0 (unprivileged):** Balanced Accuracy Difference = **-0.0095**, Ratio = **0.9861**  \n",
    "- **Gender 1 (privileged):** Balanced Accuracy Difference = **0.0095**, Ratio = **1.0141**  \n",
    "➡️ Performance is very similar across genders, with only a **~1% difference**, showing near parity.\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Unprivileged (0):** FPR Diff = **-0.0234**, Ratio = **0.9245**  \n",
    "- **Privileged (1):** FPR Diff = **0.0234**, Ratio = **1.0817**  \n",
    "➡️ The privileged group has a **slightly higher false positive rate** compared to the unprivileged group.\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, i.e., precision)\n",
    "- **Unprivileged (0):** PPV Diff = **-0.0040**, Ratio = **0.9942**  \n",
    "- **Privileged (1):** PPV Diff = **0.0040**, Ratio = **1.0059**  \n",
    "➡️ Precision is nearly identical across groups, with differences negligible (<0.5%).\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Unprivileged (0):** Selection Diff = **-0.0358**, Ratio = **0.9277**  \n",
    "- **Privileged (1):** Selection Diff = **0.0358**, Ratio = **1.0780**  \n",
    "➡️ Privileged individuals are **slightly more likely** to receive positive predictions.\n",
    "\n",
    "#### 5. True Positive Rate (TPR, recall/sensitivity)\n",
    "- **Unprivileged (0):** TPR Diff = **-0.0425**, Ratio = **0.9375**  \n",
    "- **Privileged (1):** TPR Diff = **0.0425**, Ratio = **1.0667**  \n",
    "➡️ Privileged group has a **slightly higher recall**, meaning they benefit from fewer false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "The stratified analysis shows that:\n",
    "- **Performance differences are small** (balanced accuracy and precision are nearly equal).  \n",
    "- **Privileged group (1)** has **slightly higher recall and selection rate**, meaning they are more often predicted positive and less often missed.  \n",
    "- **Unprivileged group (0)** faces slightly fewer false positives but also slightly lower recall.  \n",
    "\n",
    "All differences remain relatively minor, suggesting that the KNN model is **fairly balanced across gender groups**, though it leans slightly in favor of the privileged group in terms of recall and positive prediction likelihood.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fdeff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.4826</td>\n",
       "      <td>0.6815</td>\n",
       "      <td>0.6751</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6856</td>\n",
       "      <td>0.7218</td>\n",
       "      <td>0.6649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>0.6846</td>\n",
       "      <td>0.6831</td>\n",
       "      <td>0.3102</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.7248</td>\n",
       "      <td>0.6794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.4592</td>\n",
       "      <td>0.6757</td>\n",
       "      <td>0.6591</td>\n",
       "      <td>0.2868</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6829</td>\n",
       "      <td>0.7184</td>\n",
       "      <td>0.6369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.4826   \n",
       "1        gender             0   7362.0       0.5004           0.4950   \n",
       "2        gender             1   3894.0       0.4923           0.4592   \n",
       "\n",
       "   Accuracy  F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.6815    0.6751  0.3020      —     0.6856   0.7218  0.6649  \n",
       "1    0.6846    0.6831  0.3102      —     0.6869   0.7248  0.6794  \n",
       "2    0.6757    0.6591  0.2868      —     0.6829   0.7184  0.6369  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fairmlhealth import measure\n",
    "import pandas as pd\n",
    "from IPython.display import display  \n",
    "\n",
    "# convert gender into DataFrame with a clear column name to get a nice table as output\n",
    "gender_df = pd.DataFrame({\"gender\": X_test[\"gender\"].astype(int)})\n",
    "\n",
    "\n",
    "# Get the stratified table\n",
    "perf_table_knn = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_knn,\n",
    "    y_prob=y_prob_knn\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_knn = perf_table_knn.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ea758",
   "metadata": {},
   "source": [
    "### Interpretation of Performance Metrics – KNN by Gender\n",
    "\n",
    "The table shows performance metrics of the KNN model, stratified by gender groups (0 = unprivileged, 1 = privileged).\n",
    "\n",
    "#### Overall Performance (All Features)\n",
    "- **Accuracy:** 0.6815  \n",
    "- **F1-Score:** 0.6751  \n",
    "- **Precision:** 0.6856  \n",
    "- **ROC AUC:** 0.7218  \n",
    "- **TPR (Recall):** 0.6649  \n",
    "- → The KNN model shows **moderate predictive performance**, with balanced precision and recall.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Gender = 0 (Unprivileged Group)**\n",
    "- **Accuracy:** 0.6846 (slightly higher than privileged group)  \n",
    "- **F1-Score:** 0.6831 (better balance of precision and recall)  \n",
    "- **FPR:** 0.3102 (higher false positive rate than privileged group)  \n",
    "- **Precision:** 0.6869 (slightly higher than privileged group)  \n",
    "- **ROC AUC:** 0.7248 (slightly better discrimination ability)  \n",
    "- **TPR (Recall):** 0.6794 (higher sensitivity than privileged group)  \n",
    "\n",
    "➡️ The unprivileged group benefits from **better recall and F1-score**, but also experiences a **higher false positive rate**.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Gender = 1 (Privileged Group)**\n",
    "- **Accuracy:** 0.6757 (slightly lower than unprivileged group)  \n",
    "- **F1-Score:** 0.6591 (lower balance of precision and recall)  \n",
    "- **FPR:** 0.2868 (lower false positive rate than unprivileged group)  \n",
    "- **Precision:** 0.6829 (comparable to unprivileged group)  \n",
    "- **ROC AUC:** 0.7184 (slightly lower than unprivileged group)  \n",
    "- **TPR (Recall):** 0.6369 (lower sensitivity, more false negatives)  \n",
    "\n",
    "➡️ The privileged group experiences **fewer false positives**, but at the cost of **lower recall and F1-score**.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Unprivileged group (0):** Higher recall and F1-score → better at identifying true positives, but with more false positives.  \n",
    "- **Privileged group (1):** Lower false positives but worse recall and F1-score, meaning more missed positives.  \n",
    "- **Fairness perspective:** The performance differences are small (within a few percentage points), suggesting **no severe bias**, but the model leans slightly toward favoring the unprivileged group in terms of predictive sensitivity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5db1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6794\n",
      "  False Positive Rate (FPR): 0.3102\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.6369\n",
      "  False Positive Rate (FPR): 0.2868\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#group specific error analysis\n",
    "\n",
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_knn == 0)  # unprivileged group (female)\n",
    "male_mask   = (protected_attr_knn == 1)  # privileged group (male)\n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_knn[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74986e6d",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – KNN Model\n",
    "\n",
    "This section evaluates the classification performance of the KNN model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.6794  | 0.3102  |\n",
    "| Male (Privileged = 1)        | 0.6369  | 0.2868  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Achieve a **higher TPR (67.94%)**, meaning the model correctly identifies more true positives compared to males.  \n",
    "  - However, they also have a **higher FPR (31.02%)**, meaning more false positives occur.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Have a **lower TPR (63.69%)**, so the model misses more true positives in this group.  \n",
    "  - At the same time, they experience a **lower FPR (28.68%)**, resulting in fewer false positives.\n",
    "\n",
    "#### Conclusion\n",
    "The KNN model shows a **trade-off between sensitivity and false positives** across genders:  \n",
    "- It is **more sensitive for females** (higher recall), but at the cost of more false alarms.  \n",
    "- For **males**, the model makes **fewer false alarms**, but also fails to detect more true cases.  \n",
    "Overall, the differences are moderate and suggest a slight imbalance rather than a strong systematic bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb317ca",
   "metadata": {},
   "source": [
    "### Decision Tree - DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adbaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_dt    y_prob\n",
      "0       0       0          0  0.229598\n",
      "1       0       0          1  0.872696\n",
      "2       1       0          0  0.386614\n",
      "3       0       0          0  0.386614\n",
      "4       0       0          0  0.326141\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load KNN results\n",
    "dt_df = pd.read_csv(\"CVDKaggleData_75M25F_DT_tuned_predictions.csv\")\n",
    "\n",
    "print(dt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06530601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract common columns\n",
    "y_true_dt = dt_df[\"y_true\"].values\n",
    "y_prob_dt = dt_df[\"y_prob\"].values\n",
    "y_pred_dt = dt_df[\"y_pred_dt\"].values\n",
    "gender_dt = dt_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_dt = gender_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b83e2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Decision Tree Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0123\n",
      "               Balanced Accuracy Difference            0.0008\n",
      "               Balanced Accuracy Ratio                 1.0011\n",
      "               Disparate Impact Ratio                  0.9782\n",
      "               Equal Odds Difference                  -0.0155\n",
      "               Equal Odds Ratio                        0.9508\n",
      "               Positive Predictive Parity Difference   0.0134\n",
      "               Positive Predictive Parity Ratio        1.0193\n",
      "               Statistical Parity Difference          -0.0114\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Gender Bias Report\n",
    "print(\"\\n--- Decision Tree Gender Bias Report ---\")\n",
    "\n",
    "dt_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt,\n",
    "    prtc_attr=protected_attr_dt,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,  \n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(dt_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc2d88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_03819\">\n",
       "  <caption>Decision Tree Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_03819_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_03819_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_03819_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_03819_row0_col0\" class=\"data row0 col0\" >0.0123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03819_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_03819_row1_col0\" class=\"data row1 col0\" >0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03819_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_03819_row2_col0\" class=\"data row2 col0\" >1.0011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03819_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_03819_row3_col0\" class=\"data row3 col0\" >0.9782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03819_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_03819_row4_col0\" class=\"data row4 col0\" >-0.0155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03819_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_03819_row5_col0\" class=\"data row5 col0\" >0.9508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03819_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_03819_row6_col0\" class=\"data row6 col0\" >0.0134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03819_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_03819_row7_col0\" class=\"data row7 col0\" >1.0193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03819_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_03819_row8_col0\" class=\"data row8 col0\" >-0.0114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_03819_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_03819_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_03819_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x18fbec073a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flag metrics outside acceptable fairness bounds in current table \n",
    "\n",
    "styled_dt = MyFlagger().apply_flag(\n",
    "    df=dt_bias,\n",
    "    caption=\"Decision Tree Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2830b1e",
   "metadata": {},
   "source": [
    "### Interpretation of Decision Tree Fairness Metrics by Gender\n",
    "\n",
    "The table summarizes fairness metrics for the **Decision Tree model**, using gender as the sensitive attribute.\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0123):** Very small.  \n",
    "  → The ability to rank positives and negatives is nearly identical between gender groups.\n",
    "\n",
    "- **Balanced Accuracy Difference (0.0008)** and **Ratio (1.0011):**  \n",
    "  → Practically no disparity; the model achieves nearly equal accuracy across genders.\n",
    "\n",
    "- **Disparate Impact Ratio (0.9782):**  \n",
    "  → Close to 1, showing that the probability of receiving a positive prediction is similar across genders (well within the acceptable fairness range 0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (-0.0155)** and **Ratio (0.9508):**  \n",
    "  → Small deviation in terms of error rates (TPR/FPR). The slightly negative difference suggests marginally better treatment of the unprivileged group.\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0134)** and **Ratio (1.0193):**  \n",
    "  → Precision is nearly the same for both genders; differences are negligible.\n",
    "\n",
    "- **Statistical Parity Difference (-0.0114):**  \n",
    "  → Indicates a very small disparity in positive prediction rates, slightly favoring the unprivileged group.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → The privileged group makes up 35% of the dataset, but the metrics show that this imbalance does not lead to major fairness issues.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The Decision Tree model exhibits **very balanced fairness performance across gender groups**.  \n",
    "- Disparities in AUC, balanced accuracy, predictive parity, and statistical parity are minimal.  \n",
    "- All fairness metrics remain well within accepted thresholds, suggesting that the model does **not introduce meaningful gender bias**.  \n",
    "If anything, the results show a **slight advantage for the unprivileged group**, but the differences are too small to be concerning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "631b7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - DT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>1.0517</td>\n",
       "      <td>-0.0134</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>1.0223</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>1.0195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1.0011</td>\n",
       "      <td>-0.0155</td>\n",
       "      <td>0.9508</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>1.0193</td>\n",
       "      <td>-0.0114</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>-0.0139</td>\n",
       "      <td>0.9809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0008   \n",
       "1       gender             1                        0.0008   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9989    0.0155     1.0517   -0.0134     0.9810   \n",
       "1                   1.0011   -0.0155     0.9508    0.0134     1.0193   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0114           1.0223    0.0139     1.0195  \n",
       "1         -0.0114           0.9782   -0.0139     0.9809  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - DT\")\n",
    "measure.bias(X_test, y_test, y_pred_dt, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b884a10",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – Decision Tree (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **Decision Tree model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **-0.0008**, Ratio = **0.9989**  \n",
    "- **Male (1):** Difference = **0.0008**, Ratio = **1.0011**  \n",
    "➡️ Practically no disparity in balanced accuracy; both genders are treated equally well.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **0.0155**, Ratio = **1.0517**  \n",
    "- **Male (1):** FPR Diff = **-0.0155**, Ratio = **0.9508**  \n",
    "➡️ Females experience a **slightly higher false positive rate**, while males benefit from fewer false positives. The difference is small.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0134**, Ratio = **0.9810**  \n",
    "- **Male (1):** PPV Diff = **0.0134**, Ratio = **1.0193**  \n",
    "➡️ Precision is nearly identical across genders, with only a slight advantage for males.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **0.0114**, Ratio = **1.0223**  \n",
    "- **Male (1):** Selection Diff = **-0.0114**, Ratio = **0.9782**  \n",
    "➡️ Females are slightly **more likely to be predicted positive** compared to males.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **0.0139**, Ratio = **1.0195**  \n",
    "- **Male (1):** TPR Diff = **-0.0139**, Ratio = **0.9809**  \n",
    "➡️ Females benefit from a **slightly higher recall**, while males experience more false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Differences across all fairness metrics are minimal** (all differences ≤ 0.015, ratios very close to 1).  \n",
    "- **Females (0)** have a slight advantage in **recall and selection rate**, but at the cost of a somewhat **higher false positive rate**.  \n",
    "- **Males (1)** benefit from fewer false positives but at the cost of slightly lower recall and selection rate.  \n",
    "\n",
    "Overall, the Decision Tree model demonstrates **very balanced fairness across gender groups**, with only negligible disparities that do not indicate systematic bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1acccc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.5123</td>\n",
       "      <td>0.7082</td>\n",
       "      <td>0.7111</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>0.7593</td>\n",
       "      <td>0.7217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.5084</td>\n",
       "      <td>0.7086</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>0.2996</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7056</td>\n",
       "      <td>0.7637</td>\n",
       "      <td>0.7169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>0.7075</td>\n",
       "      <td>0.7110</td>\n",
       "      <td>0.3151</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6922</td>\n",
       "      <td>0.7514</td>\n",
       "      <td>0.7308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.5123   \n",
       "1        gender             0   7362.0       0.5004           0.5084   \n",
       "2        gender             1   3894.0       0.4923           0.5198   \n",
       "\n",
       "   Accuracy  F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7082    0.7111  0.3050      —     0.7009   0.7593  0.7217  \n",
       "1    0.7086    0.7112  0.2996      —     0.7056   0.7637  0.7169  \n",
       "2    0.7075    0.7110  0.3151      —     0.6922   0.7514  0.7308  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_dt = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_dt,\n",
    "    y_prob=y_prob_dt\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_dt = perf_table_dt.fillna(\"—\")\n",
    "\n",
    "# Display pretty table\n",
    "display(perf_table_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a06dc",
   "metadata": {},
   "source": [
    "### Interpretation of Performance Metrics – Decision Tree by Gender\n",
    "\n",
    "The table shows performance metrics for the **Decision Tree model**, stratified by gender  \n",
    "(0 = Female / Unprivileged, 1 = Male / Privileged).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7082  \n",
    "- **F1-Score:** 0.7111  \n",
    "- **Precision:** 0.7009  \n",
    "- **ROC AUC:** 0.7593  \n",
    "- **TPR (Recall):** 0.7217  \n",
    "→ The Decision Tree achieves **moderate predictive performance**, with a balanced trade-off between precision and recall.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (Unprivileged = 0)**  \n",
    "- **Accuracy:** 0.7086 (slightly higher than males)  \n",
    "- **F1-Score:** 0.7112 (virtually identical to males)  \n",
    "- **FPR:** 0.2996 (lower false positive rate than males)  \n",
    "- **Precision:** 0.7056 (slightly higher precision than males)  \n",
    "- **ROC AUC:** 0.7637 (better discrimination ability than males)  \n",
    "- **TPR (Recall):** 0.7169 (slightly lower recall than males)  \n",
    "\n",
    "➡️ Female group shows **better precision and ROC AUC**, meaning predictions are more reliable and the model separates classes slightly better for females. However, recall is marginally lower.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (Privileged = 1)**  \n",
    "- **Accuracy:** 0.7075 (slightly lower than females)  \n",
    "- **F1-Score:** 0.7110 (nearly identical to females)  \n",
    "- **FPR:** 0.3151 (higher false positive rate than females)  \n",
    "- **Precision:** 0.6922 (lower than females)  \n",
    "- **ROC AUC:** 0.7514 (slightly worse than females)  \n",
    "- **TPR (Recall):** 0.7308 (higher recall than females)  \n",
    "\n",
    "➡️ Male group benefits from a **higher recall (73.08%)**, meaning fewer missed positives, but this comes at the cost of **lower precision and a higher false positive rate**.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (Unprivileged):** Stronger in **precision and ROC AUC**, meaning more reliable and well-calibrated predictions, with fewer false positives.  \n",
    "- **Males (Privileged):** Stronger in **recall**, meaning more positives are detected, but at the cost of more false alarms and slightly lower precision.  \n",
    "\n",
    "The model demonstrates **balanced performance across genders**, with only **minor trade-offs**: females get fewer false positives, while males get more true positives. This indicates **no substantial gender bias** but rather a small sensitivity–specificity trade-off between groups.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53556439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.7169\n",
      "  False Positive Rate (FPR): 0.2996\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.7308\n",
      "  False Positive Rate (FPR): 0.3151\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_dt == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_dt == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_dt[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e7685",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – Decision Tree Model\n",
    "\n",
    "This section reports the classification performance of the Decision Tree model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.7169  | 0.2996  |\n",
    "| Male (Privileged = 1)        | 0.7308  | 0.3151  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Achieve a **slightly lower TPR (71.69%)**, meaning the model detects fewer true positives compared to males.  \n",
    "  - Benefit from a **lower FPR (29.96%)**, meaning they receive fewer false positives than males.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Achieve a **slightly higher TPR (73.08%)**, meaning the model detects more true positives in this group.  \n",
    "  - However, they also experience a **higher FPR (31.51%)**, meaning more false positives occur.\n",
    "\n",
    "#### Conclusion\n",
    "The Decision Tree model shows a **small trade-off between sensitivity and specificity** across genders:  \n",
    "- **Males** benefit from higher sensitivity (recall) but at the cost of more false alarms.  \n",
    "- **Females** have fewer false alarms but slightly lower sensitivity.  \n",
    "\n",
    "The differences are minor, suggesting that the model maintains **relatively balanced fairness across gender groups**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5168d",
   "metadata": {},
   "source": [
    "### Ensemble Model - Random Forest - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaa2a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred_rf_tuned    y_prob\n",
      "0       0       0                0  0.235104\n",
      "1       0       0                1  0.780096\n",
      "2       1       0                0  0.222839\n",
      "3       0       0                0  0.282291\n",
      "4       0       0                1  0.574989\n"
     ]
    }
   ],
   "source": [
    "rf_df = pd.read_csv(\"CVDKaggleData_75M25F_RF_tuned_predictions.csv\")\n",
    "print(rf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "278af2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns\n",
    "y_true_rf = rf_df[\"y_true\"].values\n",
    "y_pred_rf = rf_df[\"y_pred_rf_tuned\"].values\n",
    "y_prob_rf = rf_df[\"y_prob\"].values\n",
    "gender_rf = rf_df[\"gender\"].values\n",
    "\n",
    "\n",
    "# Use gender_knn as the protected attribute (0/1 as in your CSV)\n",
    "protected_attr_rf = gender_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "689f0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest Gender Bias Report ---\n",
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0188\n",
      "               Balanced Accuracy Difference            0.0198\n",
      "               Balanced Accuracy Ratio                 1.0287\n",
      "               Disparate Impact Ratio                  1.0216\n",
      "               Equal Odds Difference                   0.0267\n",
      "               Equal Odds Ratio                        0.9543\n",
      "               Positive Predictive Parity Difference   0.0246\n",
      "               Positive Predictive Parity Ratio        1.0354\n",
      "               Statistical Parity Difference           0.0101\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Gender Bias Report\n",
    "print(\"\\n--- Random Forest Gender Bias Report ---\")\n",
    "\n",
    "rf_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf,\n",
    "    prtc_attr=protected_attr_rf,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,  # 1 = Male = Privileged\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(rf_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7a8e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b7144\">\n",
       "  <caption>Random Forest Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b7144_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b7144_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_b7144_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_b7144_row0_col0\" class=\"data row0 col0\" >0.0188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b7144_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_b7144_row1_col0\" class=\"data row1 col0\" >0.0198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b7144_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_b7144_row2_col0\" class=\"data row2 col0\" >1.0287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b7144_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_b7144_row3_col0\" class=\"data row3 col0\" >1.0216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b7144_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_b7144_row4_col0\" class=\"data row4 col0\" >0.0267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b7144_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_b7144_row5_col0\" class=\"data row5 col0\" >0.9543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b7144_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_b7144_row6_col0\" class=\"data row6 col0\" >0.0246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b7144_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_b7144_row7_col0\" class=\"data row7 col0\" >1.0354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b7144_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_b7144_row8_col0\" class=\"data row8 col0\" >0.0101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b7144_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_b7144_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_b7144_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x18fbea2e7a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for Random Forest\n",
    "styled_rf = MyFlagger().apply_flag(\n",
    "    df=rf_bias,\n",
    "    caption=\"Random Forest Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c96281",
   "metadata": {},
   "source": [
    "### Interpretation of Random Forest Fairness Metrics by Gender\n",
    "\n",
    "The table reports fairness metrics for the **Random Forest model**, with gender as the sensitive attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0188):**  \n",
    "  → Very small difference in ranking performance across genders; the model distinguishes positives and negatives equally well for both groups.\n",
    "\n",
    "- **Balanced Accuracy Difference (0.0198)** and **Balanced Accuracy Ratio (1.0287):**  \n",
    "  → A slight disparity in accuracy across genders, with one group performing marginally better. However, the ratio is still close to 1, indicating only a small imbalance.\n",
    "\n",
    "- **Disparate Impact Ratio (1.0216):**  \n",
    "  → Positive prediction rates between genders are very similar. This value lies comfortably within the commonly accepted fairness range (0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (0.0267)** and **Equal Odds Ratio (0.9543):**  \n",
    "  → Small differences in error rates (true positive and false positive rates) between genders. Indicates that predictions are fairly balanced, though not perfectly aligned.\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0246)** and **Ratio (1.0354):**  \n",
    "  → Precision (likelihood that a predicted positive is correct) is very similar between genders, with only a minor advantage for one group.\n",
    "\n",
    "- **Statistical Parity Difference (0.0101):**  \n",
    "  → Practically negligible difference in overall positive prediction rates across genders.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → Males (privileged group) account for 35% of the dataset, which could contribute to minor metric fluctuations but does not result in strong disparities.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The Random Forest model demonstrates **balanced fairness performance across gender groups**:  \n",
    "- Differences in **AUC, accuracy, and error rates** are very small.  \n",
    "- Ratios remain well within accepted fairness thresholds (0.8–1.25).  \n",
    "- Slight variations exist (e.g., recall and precision slightly higher for one group), but they are **not substantial enough to indicate systematic bias**.  \n",
    "\n",
    "Overall, Random Forest shows **slightly larger disparities than Decision Tree**, but the fairness performance remains strong and indicates **no major gender bias**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47479386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - RF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0198</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>1.0479</td>\n",
       "      <td>-0.0246</td>\n",
       "      <td>0.9658</td>\n",
       "      <td>-0.0101</td>\n",
       "      <td>0.9788</td>\n",
       "      <td>-0.0267</td>\n",
       "      <td>0.9609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>1.0287</td>\n",
       "      <td>-0.0128</td>\n",
       "      <td>0.9543</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>1.0354</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>1.0216</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>1.0407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0198   \n",
       "1       gender             1                        0.0198   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9721    0.0128     1.0479   -0.0246     0.9658   \n",
       "1                   1.0287   -0.0128     0.9543    0.0246     1.0354   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0         -0.0101           0.9788   -0.0267     0.9609  \n",
       "1          0.0101           1.0216    0.0267     1.0407  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - RF\")\n",
    "measure.bias(X_test, y_test, y_pred_rf, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da34894",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – Random Forest (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **Random Forest model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **-0.0198**, Ratio = **0.9721**  \n",
    "- **Male (1):** Difference = **0.0198**, Ratio = **1.0287**  \n",
    "➡️ Balanced accuracy is slightly higher for males, while females show a small disadvantage (~2% lower). The difference is minor.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **0.0128**, Ratio = **1.0479**  \n",
    "- **Male (1):** FPR Diff = **-0.0128**, Ratio = **0.9543**  \n",
    "➡️ Females experience a **slightly higher false positive rate**, while males benefit from fewer false positives.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0246**, Ratio = **0.9658**  \n",
    "- **Male (1):** PPV Diff = **0.0246**, Ratio = **1.0354**  \n",
    "➡️ Precision is slightly higher for males, meaning male positive predictions are a bit more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **-0.0101**, Ratio = **0.9788**  \n",
    "- **Male (1):** Selection Diff = **0.0101**, Ratio = **1.0216**  \n",
    "➡️ Males are slightly **more likely to be predicted positive** than females.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **-0.0267**, Ratio = **0.9609**  \n",
    "- **Male (1):** TPR Diff = **0.0267**, Ratio = **1.0407**  \n",
    "➡️ Males achieve a **slightly higher recall**, meaning fewer false negatives compared to females.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Slight disadvantage in **balanced accuracy, recall, and precision**, while also having a **slightly higher false positive rate**.  \n",
    "- **Males (1):** Slight advantage in **precision, recall, and selection rate**, while benefiting from fewer false positives.  \n",
    "\n",
    "The disparities are small (all differences ≤ 0.027), but they suggest that the **Random Forest model leans marginally in favor of males** in terms of predictive performance and error balance. Nonetheless, the results remain **within acceptable fairness thresholds**, indicating no severe gender bias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6573254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.4717</td>\n",
       "      <td>0.7018</td>\n",
       "      <td>0.6924</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7114</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.6743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.4751</td>\n",
       "      <td>0.7085</td>\n",
       "      <td>0.7012</td>\n",
       "      <td>0.2664</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7198</td>\n",
       "      <td>0.7638</td>\n",
       "      <td>0.6835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.4651</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.6754</td>\n",
       "      <td>0.2792</td>\n",
       "      <td>—</td>\n",
       "      <td>0.6952</td>\n",
       "      <td>0.7449</td>\n",
       "      <td>0.6568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.4717   \n",
       "1        gender             0   7362.0       0.5004           0.4751   \n",
       "2        gender             1   3894.0       0.4923           0.4651   \n",
       "\n",
       "   Accuracy  F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7018    0.6924  0.2709      —     0.7114   0.7572  0.6743  \n",
       "1    0.7085    0.7012  0.2664      —     0.7198   0.7638  0.6835  \n",
       "2    0.6893    0.6754  0.2792      —     0.6952   0.7449  0.6568  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_rf = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_rf,\n",
    "    y_prob=y_prob_rf\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_rf = perf_table_rf.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46300b07",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – Random Forest (by Gender)\n",
    "\n",
    "The table shows performance results for the **Random Forest model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7018  \n",
    "- **F1-Score:** 0.6924  \n",
    "- **Precision:** 0.7114  \n",
    "- **ROC AUC:** 0.7572  \n",
    "- **TPR (Recall):** 0.6743  \n",
    "→ The model performs at a **moderate level**, with a good balance of precision and recall.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.7085 (slightly higher than males)  \n",
    "- **F1-Score:** 0.7012 (higher than males)  \n",
    "- **FPR:** 0.2664 (lower than males → fewer false positives)  \n",
    "- **Precision:** 0.7198 (higher than males)  \n",
    "- **ROC AUC:** 0.7638 (better discrimination ability)  \n",
    "- **TPR (Recall):** 0.6835 (higher sensitivity than males)  \n",
    "\n",
    "➡️ Females perform **better overall**: higher accuracy, F1-score, precision, recall, and ROC AUC, while also benefiting from fewer false positives.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.6893 (lower than females)  \n",
    "- **F1-Score:** 0.6754 (lower than females)  \n",
    "- **FPR:** 0.2792 (higher than females → more false positives)  \n",
    "- **Precision:** 0.6952 (lower than females)  \n",
    "- **ROC AUC:** 0.7449 (lower discrimination ability)  \n",
    "- **TPR (Recall):** 0.6568 (lower sensitivity → more missed positives)  \n",
    "\n",
    "➡️ Males perform **slightly worse overall**, showing weaker accuracy, recall, and precision, with higher false positive rates.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0)** achieve consistently stronger performance across almost all metrics (accuracy, F1, precision, recall, ROC AUC).  \n",
    "- **Males (1)** lag behind in all measures and face both more false positives and more false negatives.  \n",
    "\n",
    "The Random Forest model therefore shows a **slight performance advantage for females**, but the differences are not extreme. This suggests that while the model is reasonably fair overall, it **leans in favor of females** in predictive performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a52946a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6835\n",
      "  False Positive Rate (FPR): 0.2664\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.6568\n",
      "  False Positive Rate (FPR): 0.2792\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_rf == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_rf == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_rf[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1818aaf",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – Random Forest (by Gender)\n",
    "\n",
    "The table shows performance results for the **Random Forest model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7018  \n",
    "- **F1-Score:** 0.6924  \n",
    "- **Precision:** 0.7114  \n",
    "- **ROC AUC:** 0.7572  \n",
    "- **TPR (Recall):** 0.6743  \n",
    "→ The model performs at a **moderate level**, with a good balance of precision and recall.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.7085 (slightly higher than males)  \n",
    "- **F1-Score:** 0.7012 (higher than males)  \n",
    "- **FPR:** 0.2664 (lower than males → fewer false positives)  \n",
    "- **Precision:** 0.7198 (higher than males)  \n",
    "- **ROC AUC:** 0.7638 (better discrimination ability)  \n",
    "- **TPR (Recall):** 0.6835 (higher sensitivity than males)  \n",
    "\n",
    "➡️ Females perform **better overall**: higher accuracy, F1-score, precision, recall, and ROC AUC, while also benefiting from fewer false positives.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.6893 (lower than females)  \n",
    "- **F1-Score:** 0.6754 (lower than females)  \n",
    "- **FPR:** 0.2792 (higher than females → more false positives)  \n",
    "- **Precision:** 0.6952 (lower than females)  \n",
    "- **ROC AUC:** 0.7449 (lower discrimination ability)  \n",
    "- **TPR (Recall):** 0.6568 (lower sensitivity → more missed positives)  \n",
    "\n",
    "➡️ Males perform **slightly worse overall**, showing weaker accuracy, recall, and precision, with higher false positive rates.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0)** achieve consistently stronger performance across almost all metrics (accuracy, F1, precision, recall, ROC AUC).  \n",
    "- **Males (1)** lag behind in all measures and face both more false positives and more false negatives.  \n",
    "\n",
    "The Random Forest model therefore shows a **slight performance advantage for females**, but the differences are not extreme. This suggests that while the model is reasonably fair overall, it **leans in favor of females** in predictive performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136de6",
   "metadata": {},
   "source": [
    "### Deep Learning Model - Feed Forward Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c69d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  y_true  y_pred    y_prob\n",
      "0       0       0       0  0.258611\n",
      "1       0       0       1  0.848005\n",
      "2       1       0       0  0.452818\n",
      "3       0       0       0  0.325508\n",
      "4       0       0       0  0.255966\n"
     ]
    }
   ],
   "source": [
    "mlp_df = pd.read_csv(\"CVDKaggleData_75M25F_MLP_adamtuned_predictions.csv\")\n",
    "print(mlp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f65a498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common columns \n",
    "y_true_mlp = mlp_df[\"y_true\"].values \n",
    "y_prob_mlp = mlp_df[\"y_prob\"].values\n",
    "y_pred_mlp = mlp_df[\"y_pred\"].values\n",
    "gender_mlp = mlp_df[\"gender\"].values \n",
    "\n",
    "# Use gender_mlp as the protected attribute\n",
    "protected_attr_mlp = gender_mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6969bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Value\n",
      "Metric         Measure                                       \n",
      "Group Fairness AUC Difference                          0.0084\n",
      "               Balanced Accuracy Difference            0.0063\n",
      "               Balanced Accuracy Ratio                 1.0089\n",
      "               Disparate Impact Ratio                  0.9955\n",
      "               Equal Odds Difference                  -0.0119\n",
      "               Equal Odds Ratio                        0.9553\n",
      "               Positive Predictive Parity Difference   0.0159\n",
      "               Positive Predictive Parity Ratio        1.0223\n",
      "               Statistical Parity Difference          -0.0021\n",
      "Data Metrics   Prevalence of Privileged Class (%)     35.0000\n"
     ]
    }
   ],
   "source": [
    "#Run fairmlhealth bias detection for MLP \n",
    "\n",
    "mlp_bias = measure.summary(\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp,\n",
    "    prtc_attr=protected_attr_mlp,\n",
    "    pred_type=\"classification\",\n",
    "    priv_grp=1,\n",
    "    sig_fig=4,\n",
    "    skip_if=True,\n",
    "    skip_performance = True\n",
    ")\n",
    "\n",
    "print(mlp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbf7eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f7ea6\">\n",
       "  <caption>MLP Fairness (Gender)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f7ea6_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"index_name level1\" >Measure</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f7ea6_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"9\">Group Fairness</th>\n",
       "      <th id=\"T_f7ea6_level1_row0\" class=\"row_heading level1 row0\" >AUC Difference</th>\n",
       "      <td id=\"T_f7ea6_row0_col0\" class=\"data row0 col0\" >0.0084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7ea6_level1_row1\" class=\"row_heading level1 row1\" >Balanced Accuracy Difference</th>\n",
       "      <td id=\"T_f7ea6_row1_col0\" class=\"data row1 col0\" >0.0063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7ea6_level1_row2\" class=\"row_heading level1 row2\" >Balanced Accuracy Ratio</th>\n",
       "      <td id=\"T_f7ea6_row2_col0\" class=\"data row2 col0\" >1.0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7ea6_level1_row3\" class=\"row_heading level1 row3\" >Disparate Impact Ratio</th>\n",
       "      <td id=\"T_f7ea6_row3_col0\" class=\"data row3 col0\" >0.9955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7ea6_level1_row4\" class=\"row_heading level1 row4\" >Equal Odds Difference</th>\n",
       "      <td id=\"T_f7ea6_row4_col0\" class=\"data row4 col0\" >-0.0119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7ea6_level1_row5\" class=\"row_heading level1 row5\" >Equal Odds Ratio</th>\n",
       "      <td id=\"T_f7ea6_row5_col0\" class=\"data row5 col0\" >0.9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7ea6_level1_row6\" class=\"row_heading level1 row6\" >Positive Predictive Parity Difference</th>\n",
       "      <td id=\"T_f7ea6_row6_col0\" class=\"data row6 col0\" >0.0159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7ea6_level1_row7\" class=\"row_heading level1 row7\" >Positive Predictive Parity Ratio</th>\n",
       "      <td id=\"T_f7ea6_row7_col0\" class=\"data row7 col0\" >1.0223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7ea6_level1_row8\" class=\"row_heading level1 row8\" >Statistical Parity Difference</th>\n",
       "      <td id=\"T_f7ea6_row8_col0\" class=\"data row8 col0\" >-0.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7ea6_level0_row9\" class=\"row_heading level0 row9\" >Data Metrics</th>\n",
       "      <th id=\"T_f7ea6_level1_row9\" class=\"row_heading level1 row9\" >Prevalence of Privileged Class (%)</th>\n",
       "      <td id=\"T_f7ea6_row9_col0\" class=\"data row9 col0\" >35.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x18fbec21b40>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flagged fairness table for MLP\n",
    "styled_mlp = MyFlagger().apply_flag(\n",
    "    df=mlp_bias,\n",
    "    caption=\"MLP Fairness (Gender)\",\n",
    "    boundaries=bounds,\n",
    "    sig_fig=4,\n",
    "    as_styler=True\n",
    ")\n",
    "styled_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8a2e5",
   "metadata": {},
   "source": [
    "### Interpretation of MLP Fairness Metrics by Gender\n",
    "\n",
    "The table reports fairness metrics for the **Multilayer Perceptron (MLP) model**, using gender as the sensitive attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "- **AUC Difference (0.0084):**  \n",
    "  → Very small; the model’s ability to rank positive vs. negative cases is nearly identical across genders.\n",
    "\n",
    "- **Balanced Accuracy Difference (0.0063)** and **Ratio (1.0089):**  \n",
    "  → Balanced accuracy is almost the same for both genders, showing very minimal disparity.\n",
    "\n",
    "- **Disparate Impact Ratio (0.9955):**  \n",
    "  → Very close to 1, suggesting that the likelihood of receiving a positive prediction is nearly equal across genders (well within the fairness range 0.8–1.25).\n",
    "\n",
    "- **Equal Odds Difference (-0.0119)** and **Equal Odds Ratio (0.9553):**  \n",
    "  → Error rates (TPR and FPR) differ only slightly, with a very small bias in favor of one group, but the magnitude is negligible.\n",
    "\n",
    "- **Positive Predictive Parity Difference (0.0159)** and **Ratio (1.0223):**  \n",
    "  → Precision is nearly equal across genders, with just a tiny advantage for one group.\n",
    "\n",
    "- **Statistical Parity Difference (-0.0021):**  \n",
    "  → Essentially zero, indicating almost no difference in positive prediction rates.\n",
    "\n",
    "- **Prevalence of Privileged Class (35%):**  \n",
    "  → The privileged group (males) makes up 35% of the dataset, but this imbalance does not result in fairness violations.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Conclusion\n",
    "The MLP model demonstrates **very strong fairness across gender groups**:  \n",
    "- All disparities are **extremely small** (well below common fairness thresholds, e.g., ≤0.1 for differences, 0.8–1.25 for ratios).  \n",
    "- Both performance parity (AUC, balanced accuracy) and fairness parity (disparate impact, statistical parity, equal odds) are well maintained.  \n",
    "\n",
    "In summary, the MLP model shows **no meaningful gender bias** and achieves one of the most balanced fairness performances among the tested models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aa66097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FairMLHealth Stratified Bias Table - MLP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Balanced Accuracy Difference</th>\n",
       "      <th>Balanced Accuracy Ratio</th>\n",
       "      <th>FPR Diff</th>\n",
       "      <th>FPR Ratio</th>\n",
       "      <th>PPV Diff</th>\n",
       "      <th>PPV Ratio</th>\n",
       "      <th>Selection Diff</th>\n",
       "      <th>Selection Ratio</th>\n",
       "      <th>TPR Diff</th>\n",
       "      <th>TPR Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0063</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>1.0468</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>1.0045</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>0.9989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>1.0089</td>\n",
       "      <td>-0.0119</td>\n",
       "      <td>0.9553</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>1.0223</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1.0011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature Name Feature Value  Balanced Accuracy Difference  \\\n",
       "0       gender             0                       -0.0063   \n",
       "1       gender             1                        0.0063   \n",
       "\n",
       "   Balanced Accuracy Ratio  FPR Diff  FPR Ratio  PPV Diff  PPV Ratio  \\\n",
       "0                   0.9912    0.0119     1.0468   -0.0159     0.9782   \n",
       "1                   1.0089   -0.0119     0.9553    0.0159     1.0223   \n",
       "\n",
       "   Selection Diff  Selection Ratio  TPR Diff  TPR Ratio  \n",
       "0          0.0021           1.0045   -0.0008     0.9989  \n",
       "1         -0.0021           0.9955    0.0008     1.0011  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FairMLHealth Stratified Bias Table - MLP\")\n",
    "measure.bias(X_test, y_test, y_pred_mlp, features=['gender'], flag_oor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efef5a5",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Bias Analysis – MLP (by Gender)\n",
    "\n",
    "The table shows group-specific fairness metrics for the **Multilayer Perceptron (MLP) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Balanced Accuracy\n",
    "- **Female (0):** Difference = **-0.0063**, Ratio = **0.9912**  \n",
    "- **Male (1):** Difference = **0.0063**, Ratio = **1.0089**  \n",
    "➡️ Balanced accuracy is almost identical across genders, with males showing a very small advantage.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "- **Female (0):** FPR Diff = **0.0119**, Ratio = **1.0468**  \n",
    "- **Male (1):** FPR Diff = **-0.0119**, Ratio = **0.9553**  \n",
    "➡️ Females experience a **slightly higher false positive rate**, while males benefit from slightly fewer false positives. The difference is minimal.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Positive Predictive Value (PPV, Precision)\n",
    "- **Female (0):** PPV Diff = **-0.0159**, Ratio = **0.9782**  \n",
    "- **Male (1):** PPV Diff = **0.0159**, Ratio = **1.0223**  \n",
    "➡️ Males have a **slight advantage in precision**, meaning their positive predictions are marginally more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Selection Rate (likelihood of being predicted positive)\n",
    "- **Female (0):** Selection Diff = **0.0021**, Ratio = **1.0045**  \n",
    "- **Male (1):** Selection Diff = **-0.0021**, Ratio = **0.9955**  \n",
    "➡️ Females are predicted positive at nearly the same rate as males, with only a negligible difference.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. True Positive Rate (TPR, Recall)\n",
    "- **Female (0):** TPR Diff = **-0.0008**, Ratio = **0.9989**  \n",
    "- **Male (1):** TPR Diff = **0.0008**, Ratio = **1.0011**  \n",
    "➡️ Recall is **practically identical** across genders, with no meaningful difference.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0):** Slight disadvantage in **false positive rate** and **precision**, but nearly equal in recall and selection rate.  \n",
    "- **Males (1):** Slight advantage in **precision** and **lower FPR**, but again the differences are very small.  \n",
    "\n",
    "Overall, the MLP model demonstrates **exceptionally balanced fairness across gender groups**.  \n",
    "All disparities are negligible (≤ 0.016 difference, ratios ~1), confirming that the model does **not exhibit systematic gender bias**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbc2caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Roaming\\Python\\Python310\\site-packages\\fairmlhealth\\__utils.py:57: UserWarning: Possible error in column(s) ['gender']. DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n",
      "  warn(f\"Possible error in column(s) {cols}. {wr}\\n\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Feature Value</th>\n",
       "      <th>Obs.</th>\n",
       "      <th>Mean Target</th>\n",
       "      <th>Mean Prediction</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>FPR</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL FEATURES</td>\n",
       "      <td>ALL VALUES</td>\n",
       "      <td>11256.0</td>\n",
       "      <td>0.4976</td>\n",
       "      <td>0.4711</td>\n",
       "      <td>0.7145</td>\n",
       "      <td>0.7052</td>\n",
       "      <td>0.2578</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7251</td>\n",
       "      <td>0.7674</td>\n",
       "      <td>0.6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "      <td>7362.0</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.4704</td>\n",
       "      <td>0.7165</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.2537</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.7706</td>\n",
       "      <td>0.6868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>1</td>\n",
       "      <td>3894.0</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.4725</td>\n",
       "      <td>0.7106</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>0.2656</td>\n",
       "      <td>—</td>\n",
       "      <td>0.7147</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>0.6860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature Name Feature Value     Obs.  Mean Target  Mean Prediction  \\\n",
       "0  ALL FEATURES    ALL VALUES  11256.0       0.4976           0.4711   \n",
       "1        gender             0   7362.0       0.5004           0.4704   \n",
       "2        gender             1   3894.0       0.4923           0.4725   \n",
       "\n",
       "   Accuracy  F1-Score     FPR PR AUC  Precision  ROC AUC     TPR  \n",
       "0    0.7145    0.7052  0.2578      —     0.7251   0.7674  0.6865  \n",
       "1    0.7165    0.7080  0.2537      —     0.7306   0.7706  0.6868  \n",
       "2    0.7106    0.7000  0.2656      —     0.7147   0.7622  0.6860  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the stratified performance table\n",
    "perf_table_mlp = measure.performance(\n",
    "    X=gender_df,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_mlp,\n",
    "    y_prob=y_prob_mlp\n",
    ")\n",
    "\n",
    "# Replace NaN with a dash\n",
    "perf_table_mlp = perf_table_mlp.fillna(\"—\")\n",
    "\n",
    "# display pretty table\n",
    "display(perf_table_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7c382",
   "metadata": {},
   "source": [
    "### Interpretation of Stratified Performance Metrics – MLP (by Gender)\n",
    "\n",
    "The table shows performance results for the **Multilayer Perceptron (MLP) model**, stratified by gender  \n",
    "(0 = Female, 1 = Male).\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Performance (All Data)\n",
    "- **Accuracy:** 0.7145  \n",
    "- **F1-Score:** 0.7052  \n",
    "- **Precision:** 0.7251  \n",
    "- **ROC AUC:** 0.7674  \n",
    "- **TPR (Recall):** 0.6865  \n",
    "→ The MLP achieves **solid predictive performance**, with a good balance between precision and recall.\n",
    "\n",
    "---\n",
    "\n",
    "#### Group-Specific Performance\n",
    "\n",
    "**1. Female (0 – Unprivileged)**  \n",
    "- **Accuracy:** 0.7165 (slightly higher than males)  \n",
    "- **F1-Score:** 0.7080 (slightly higher than males)  \n",
    "- **FPR:** 0.2537 (lower than males → fewer false positives)  \n",
    "- **Precision:** 0.7306 (higher than males → more reliable positive predictions)  \n",
    "- **ROC AUC:** 0.7706 (slightly better discrimination ability)  \n",
    "- **TPR (Recall):** 0.6868 (nearly identical to males)  \n",
    "\n",
    "➡️ Females show **slightly better accuracy, F1-score, precision, and ROC AUC**, while maintaining comparable recall and benefiting from fewer false positives.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Male (1 – Privileged)**  \n",
    "- **Accuracy:** 0.7106 (slightly lower than females)  \n",
    "- **F1-Score:** 0.7000 (slightly lower than females)  \n",
    "- **FPR:** 0.2656 (higher than females → more false positives)  \n",
    "- **Precision:** 0.7147 (lower than females)  \n",
    "- **ROC AUC:** 0.7622 (slightly lower than females)  \n",
    "- **TPR (Recall):** 0.6860 (nearly identical to females)  \n",
    "\n",
    "➡️ Males show **slightly weaker predictive performance overall**, with higher false positive rates and lower precision, though recall remains on par with females.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Conclusion\n",
    "- **Females (0)** achieve **slightly better overall performance**, with higher accuracy, precision, and F1-score, plus fewer false positives.  \n",
    "- **Males (1)** perform slightly worse across most metrics, though recall (TPR) is nearly identical.  \n",
    "\n",
    "The differences are **minor and well within acceptable bounds**, indicating that the MLP model achieves **balanced fairness across genders** while showing a **small performance advantage for females**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379dbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female (Unprivileged) Results:\n",
      "  True Positive Rate (TPR): 0.6868\n",
      "  False Positive Rate (FPR): 0.2537\n",
      "----------------------------------------\n",
      "Male (Privileged) Results:\n",
      "  True Positive Rate (TPR): 0.6860\n",
      "  False Positive Rate (FPR): 0.2656\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fairmlhealth import performance_metrics as pm\n",
    "\n",
    "# Define group masks with clear names\n",
    "female_mask = (protected_attr_mlp == 0)  # female = unprivileged group\n",
    "male_mask   = (protected_attr_mlp == 1)  # male = privileged group \n",
    "\n",
    "# Function to evaluate group-specific metrics\n",
    "def evaluate_group_performance(group_name, mask):\n",
    "    tpr = pm.true_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    fpr = pm.false_positive_rate(y_test[mask], y_pred_mlp[mask])\n",
    "    print(f\"{group_name} Results:\")\n",
    "    print(f\"  True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"  False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate for each group\n",
    "evaluate_group_performance(\"Female (Unprivileged)\", female_mask)\n",
    "evaluate_group_performance(\"Male (Privileged)\", male_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29443ed8",
   "metadata": {},
   "source": [
    "### Group-Specific Error Analysis – MLP Model\n",
    "\n",
    "This section evaluates the classification performance of the MLP model across gender groups using **True Positive Rate (TPR)** and **False Positive Rate (FPR)**.\n",
    "\n",
    "#### Results by Gender Group\n",
    "\n",
    "| Group                        | TPR     | FPR     |\n",
    "|------------------------------|---------|---------|\n",
    "| Female (Unprivileged = 0)    | 0.6868  | 0.2537  |\n",
    "| Male (Privileged = 1)        | 0.6860  | 0.2656  |\n",
    "\n",
    "#### Interpretation\n",
    "- **Females (Unprivileged):**  \n",
    "  - Achieve a **TPR of 68.68%**, which is almost identical to males.  \n",
    "  - Benefit from a **slightly lower FPR (25.37%)**, meaning fewer false positives compared to males.\n",
    "\n",
    "- **Males (Privileged):**  \n",
    "  - Achieve a **TPR of 68.60%**, virtually the same as females.  \n",
    "  - Experience a **slightly higher FPR (26.56%)**, meaning they are marginally more often misclassified as positive.\n",
    "\n",
    "#### Conclusion\n",
    "The MLP model achieves **highly balanced error rates across genders**:  \n",
    "- **Recall (TPR)** is nearly identical for females and males.  \n",
    "- **Females** have a small advantage with fewer false positives.  \n",
    "\n",
    "Overall, the differences are **minimal**, indicating that the model is **fair and unbiased across gender groups** in terms of error distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abb196",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairml310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
